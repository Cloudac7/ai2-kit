{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ai 2 -kit A toolkit featured a rtificial i ntelligence \u00d7 a b i nitio for computational chemistry research. Please be advised that ai2-kit is still under heavy development and you should expect things to change often. We encourage people to play and explore with ai2-kit , and stay tuned with us for more features to come. Feature Highlights Collection of tools to facilitate the development of automated workflows for computational chemistry research. Utilities to execute and manage jobs in local or remote HPC job scheduler. Utilities to simplified automated workflows development with reusable components. Installation You can use the following command to install ai2-kit : pip install ai2-kit ai2-kit --help If you want to run ai2-kit from source, you can run the following commands in the project folder: pip install poetry poetry install ./ai2-kit --help Note that instead of running global ai2-kit command, you should run ./ai2-kit to run the command from source. Manuals Domain Specific Tools Proton Transfer Analysis Toolkit CLL MLP Training Workflow FEP MLP Training Workflow Build-in Functionalities Checkpoint Mechanism ASE Toolkit FAQ","title":"Home"},{"location":"#ai2-kit","text":"A toolkit featured a rtificial i ntelligence \u00d7 a b i nitio for computational chemistry research. Please be advised that ai2-kit is still under heavy development and you should expect things to change often. We encourage people to play and explore with ai2-kit , and stay tuned with us for more features to come.","title":"ai2-kit"},{"location":"#feature-highlights","text":"Collection of tools to facilitate the development of automated workflows for computational chemistry research. Utilities to execute and manage jobs in local or remote HPC job scheduler. Utilities to simplified automated workflows development with reusable components.","title":"Feature Highlights"},{"location":"#installation","text":"You can use the following command to install ai2-kit : pip install ai2-kit ai2-kit --help If you want to run ai2-kit from source, you can run the following commands in the project folder: pip install poetry poetry install ./ai2-kit --help Note that instead of running global ai2-kit command, you should run ./ai2-kit to run the command from source.","title":"Installation"},{"location":"#manuals","text":"","title":"Manuals"},{"location":"#domain-specific-tools","text":"Proton Transfer Analysis Toolkit CLL MLP Training Workflow FEP MLP Training Workflow","title":"Domain Specific Tools"},{"location":"#build-in-functionalities","text":"Checkpoint Mechanism ASE Toolkit FAQ","title":"Build-in Functionalities"},{"location":"CHANGELOG/","text":"Change Logs v0.3.6 feat: implement read tag for yaml parser v0.3.5 feat: support plumed config in cll workflow feat: support artifacts attributes inheritance in workflow feat: implement join tag for yaml parser v0.3.4 feat: apply checkpoint to all workflows and job submission steps feat: add checkpoint command line interface v0.3.3 refactor: using async.io to implement tasks and workflows v0.3.2 improvement: support relative work_dir v0.3.1 feat: async job polling v0.3.0 feat: ase toolkit fix: fep training workflow improvement: speed up command line interface with lazy import v0.2.0 feat: proton transfer analysis toolkit v0.1.0 feat: cll-mlp-training workflow feat: fep-mlp-training workflow v0.0.2 feat: support providing initial structures for label task at first iteration. feat: support dynamic configuration update for each iterations. improvement: reduce size of remote python script by compressing it with bz2. fix: ssh connection closed when running large remote python script. refactor: design common interface for CLL workflow. refactor: use ase.Atoms as the universal in-memory data structure. refactor: use ResourceManager to manage executors and artifacts. v0.0.1 feat: utilities to execute and manage jobs in local or remote HPC job scheduler. feat: utilities to run python functions in remote machines directly. feat: utilities to simplified automated workflows development with reusable components. feat: fundamental CLL & FEP workflows.","title":"Change Logs"},{"location":"CHANGELOG/#change-logs","text":"","title":"Change Logs"},{"location":"CHANGELOG/#v036","text":"feat: implement read tag for yaml parser","title":"v0.3.6"},{"location":"CHANGELOG/#v035","text":"feat: support plumed config in cll workflow feat: support artifacts attributes inheritance in workflow feat: implement join tag for yaml parser","title":"v0.3.5"},{"location":"CHANGELOG/#v034","text":"feat: apply checkpoint to all workflows and job submission steps feat: add checkpoint command line interface","title":"v0.3.4"},{"location":"CHANGELOG/#v033","text":"refactor: using async.io to implement tasks and workflows","title":"v0.3.3"},{"location":"CHANGELOG/#v032","text":"improvement: support relative work_dir","title":"v0.3.2"},{"location":"CHANGELOG/#v031","text":"feat: async job polling","title":"v0.3.1"},{"location":"CHANGELOG/#v030","text":"feat: ase toolkit fix: fep training workflow improvement: speed up command line interface with lazy import","title":"v0.3.0"},{"location":"CHANGELOG/#v020","text":"feat: proton transfer analysis toolkit","title":"v0.2.0"},{"location":"CHANGELOG/#v010","text":"feat: cll-mlp-training workflow feat: fep-mlp-training workflow","title":"v0.1.0"},{"location":"CHANGELOG/#v002","text":"feat: support providing initial structures for label task at first iteration. feat: support dynamic configuration update for each iterations. improvement: reduce size of remote python script by compressing it with bz2. fix: ssh connection closed when running large remote python script. refactor: design common interface for CLL workflow. refactor: use ase.Atoms as the universal in-memory data structure. refactor: use ResourceManager to manage executors and artifacts.","title":"v0.0.2"},{"location":"CHANGELOG/#v001","text":"feat: utilities to execute and manage jobs in local or remote HPC job scheduler. feat: utilities to run python functions in remote machines directly. feat: utilities to simplified automated workflows development with reusable components. feat: fundamental CLL & FEP workflows.","title":"v0.0.1"},{"location":"reference/ai2_kit/","text":"Module ai2_kit Sub-modules ai2_kit.algorithm ai2_kit.core ai2_kit.dflow ai2_kit.domain ai2_kit.main ai2_kit.tool ai2_kit.workflow","title":"Index"},{"location":"reference/ai2_kit/#module-ai2_kit","text":"","title":"Module ai2_kit"},{"location":"reference/ai2_kit/#sub-modules","text":"ai2_kit.algorithm ai2_kit.core ai2_kit.dflow ai2_kit.domain ai2_kit.main ai2_kit.tool ai2_kit.workflow","title":"Sub-modules"},{"location":"reference/ai2_kit/dflow/","text":"Module ai2_kit.dflow","title":"Dflow"},{"location":"reference/ai2_kit/dflow/#module-ai2_kitdflow","text":"","title":"Module ai2_kit.dflow"},{"location":"reference/ai2_kit/main/","text":"Module ai2_kit.main View Source from fire import Fire class Group : def __init__ ( self , items : dict , doc : str = '' ) -> None : self . __doc__ = doc self . __dict__ . update ( items ) class ProtonTransferGroup : @property def analyze ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . proton_transfer_detection @property def visualize ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . visualize_transfer @property def show_transfer_paths ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . analysis_transfer_paths @property def show_type_change ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . detect_type_change class WorkflowGroup : @property def cll_mlp_training ( self ): from ai2_kit.workflow.cll_mlp import run_workflow return run_workflow @property def fep_mlp_training ( self ): from ai2_kit.workflow.fep_mlp import run_workflow return run_workflow class ToolGroup : @property def ase ( self ): from ai2_kit.tool.ase import AseHelper return AseHelper @property def checkpoint ( self ): from ai2_kit.core.checkpoint import CheckpointCmd return CheckpointCmd kit = Group ({ 'workflow' : WorkflowGroup (), 'algorithm' : Group ({ 'proton-transfer' : ProtonTransferGroup (), }), 'tool' : ToolGroup (), }, doc = \"Welcome to use ai2-kit!\" ) def main (): Fire ( kit ) if __name__ == '__main__' : main () Variables kit Functions main def main ( ) View Source def main(): Fire(kit) Classes Group class Group ( items : dict , doc : str = '' ) View Source class Group: def __init__ ( self , items: dict , doc: str = '' ) -> None: self . __doc__ = doc self . __dict__ . update ( items ) ProtonTransferGroup class ProtonTransferGroup ( / , * args , ** kwargs ) View Source class ProtonTransferGroup : @property def analyze ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . proton_transfer_detection @property def visualize ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . visualize_transfer @property def show_transfer_paths ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . analysis_transfer_paths @property def show_type_change ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . detect_type_change Instance variables analyze show_transfer_paths show_type_change visualize ToolGroup class ToolGroup ( / , * args , ** kwargs ) View Source class ToolGroup : @property def ase ( self ): from ai2_kit.tool.ase import AseHelper return AseHelper @property def checkpoint ( self ): from ai2_kit.core.checkpoint import CheckpointCmd return CheckpointCmd Instance variables ase checkpoint WorkflowGroup class WorkflowGroup ( / , * args , ** kwargs ) View Source class WorkflowGroup : @property def cll_mlp_training ( self ): from ai2_kit.workflow.cll_mlp import run_workflow return run_workflow @property def fep_mlp_training ( self ): from ai2_kit.workflow.fep_mlp import run_workflow return run_workflow Instance variables cll_mlp_training fep_mlp_training","title":"Main"},{"location":"reference/ai2_kit/main/#module-ai2_kitmain","text":"View Source from fire import Fire class Group : def __init__ ( self , items : dict , doc : str = '' ) -> None : self . __doc__ = doc self . __dict__ . update ( items ) class ProtonTransferGroup : @property def analyze ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . proton_transfer_detection @property def visualize ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . visualize_transfer @property def show_transfer_paths ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . analysis_transfer_paths @property def show_type_change ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . detect_type_change class WorkflowGroup : @property def cll_mlp_training ( self ): from ai2_kit.workflow.cll_mlp import run_workflow return run_workflow @property def fep_mlp_training ( self ): from ai2_kit.workflow.fep_mlp import run_workflow return run_workflow class ToolGroup : @property def ase ( self ): from ai2_kit.tool.ase import AseHelper return AseHelper @property def checkpoint ( self ): from ai2_kit.core.checkpoint import CheckpointCmd return CheckpointCmd kit = Group ({ 'workflow' : WorkflowGroup (), 'algorithm' : Group ({ 'proton-transfer' : ProtonTransferGroup (), }), 'tool' : ToolGroup (), }, doc = \"Welcome to use ai2-kit!\" ) def main (): Fire ( kit ) if __name__ == '__main__' : main ()","title":"Module ai2_kit.main"},{"location":"reference/ai2_kit/main/#variables","text":"kit","title":"Variables"},{"location":"reference/ai2_kit/main/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/main/#main","text":"def main ( ) View Source def main(): Fire(kit)","title":"main"},{"location":"reference/ai2_kit/main/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/main/#group","text":"class Group ( items : dict , doc : str = '' ) View Source class Group: def __init__ ( self , items: dict , doc: str = '' ) -> None: self . __doc__ = doc self . __dict__ . update ( items )","title":"Group"},{"location":"reference/ai2_kit/main/#protontransfergroup","text":"class ProtonTransferGroup ( / , * args , ** kwargs ) View Source class ProtonTransferGroup : @property def analyze ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . proton_transfer_detection @property def visualize ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . visualize_transfer @property def show_transfer_paths ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . analysis_transfer_paths @property def show_type_change ( self ): from ai2_kit.algorithm import proton_transfer return proton_transfer . detect_type_change","title":"ProtonTransferGroup"},{"location":"reference/ai2_kit/main/#instance-variables","text":"analyze show_transfer_paths show_type_change visualize","title":"Instance variables"},{"location":"reference/ai2_kit/main/#toolgroup","text":"class ToolGroup ( / , * args , ** kwargs ) View Source class ToolGroup : @property def ase ( self ): from ai2_kit.tool.ase import AseHelper return AseHelper @property def checkpoint ( self ): from ai2_kit.core.checkpoint import CheckpointCmd return CheckpointCmd","title":"ToolGroup"},{"location":"reference/ai2_kit/main/#instance-variables_1","text":"ase checkpoint","title":"Instance variables"},{"location":"reference/ai2_kit/main/#workflowgroup","text":"class WorkflowGroup ( / , * args , ** kwargs ) View Source class WorkflowGroup : @property def cll_mlp_training ( self ): from ai2_kit.workflow.cll_mlp import run_workflow return run_workflow @property def fep_mlp_training ( self ): from ai2_kit.workflow.fep_mlp import run_workflow return run_workflow","title":"WorkflowGroup"},{"location":"reference/ai2_kit/main/#instance-variables_2","text":"cll_mlp_training fep_mlp_training","title":"Instance variables"},{"location":"reference/ai2_kit/algorithm/","text":"Module ai2_kit.algorithm Sub-modules ai2_kit.algorithm.proton_transfer","title":"Index"},{"location":"reference/ai2_kit/algorithm/#module-ai2_kitalgorithm","text":"","title":"Module ai2_kit.algorithm"},{"location":"reference/ai2_kit/algorithm/#sub-modules","text":"ai2_kit.algorithm.proton_transfer","title":"Sub-modules"},{"location":"reference/ai2_kit/algorithm/proton_transfer/","text":"Module ai2_kit.algorithm.proton_transfer View Source from MDAnalysis import Universe from MDAnalysis.lib.distances import minimize_vectors from multiprocessing.pool import Pool import ase.io as ai from ase import Atom from functools import partial from dataclasses import dataclass from typing import List , Tuple , NamedTuple import fire import numpy as np import json import os import io # TODO: use array instead of list for better performance when possible # TODO: use numba to speed up the calculation class AnalysisResult ( NamedTuple ): indicator_position : Tuple [ float , float , float ] transfers : List [ Tuple [ int , int ]] @dataclass class SystemInfo : # Information about the system initial_donor : int u : Universe cell : List [ float ] acceptor_elements : List [ str ] # The relevant parameters of the algorithm class AlgorithmParameter ( NamedTuple ): r_a : float # The radius used to search for acceptor r_h : float # The radius used to search for H rho_0 : float # Control the rate of the weights change rho_max : float # The critical value of proton transfer max_depth : int # The maximum length of the path g_threshold : float # The threshold for whether to join the node to the path class System ( object ): def __init__ ( self , sys_info : SystemInfo , parameter : AlgorithmParameter ): self . u = sys_info . u self . cell = sys_info . cell self . r_a = parameter . r_a self . r_h = parameter . r_h self . g_threshold = parameter . g_threshold self . max_depth = parameter . max_depth self . rho_0 = parameter . rho_0 self . rho_max = parameter . rho_max self . acceptor_elements = sys_info . acceptor_elements def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ): self . u . trajectory [ time ] donor = prev_donor transfers = [] list_of_paths = [[ prev_donor ]] list_of_weights = [[ 1 ]] for depth in range ( self . max_depth ): for j , path in enumerate ( list_of_paths ): found = False if depth == len ( path ) - 1 : acceptors = self . u . select_atoms ( f \"(around { self . r_a } index { path [ - 1 ] } ) and ( { acceptor_query } )\" ) protons = self . u . select_atoms ( f \"(around { self . r_h } index { path [ - 1 ] } ) and (name H)\" ) for i , acceptor in enumerate ( acceptors . ix ): g , proton = self . calculate_g ( path [ - 1 ], acceptor , protons . ix ) if ( g >= self . g_threshold ) and ( acceptor not in path ): found = True list_of_weights . append ( list_of_weights [ j ] + [ g * list_of_weights [ j ][ - 1 ]]) list_of_paths . append ( path + [ acceptor ]) if proton > 0 and all ( w >= 0.9 for w in list_of_weights [ j ]): donor = acceptor transfers . append (( int ( acceptor ), int ( proton ))) if found : list_of_paths . pop ( j ) list_of_weights . pop ( j ) indicator_position = self . calculate_position ( list_of_paths , list_of_weights ) result = AnalysisResult ( indicator_position = tuple ( indicator_position [ 0 ]), transfers = transfers ) return donor , result def calculate_g ( self , donor : int , acceptor : int , protons : list ): donor_pos = self . u . atoms [ donor ] . position acceptor_pos = self . u . atoms [ acceptor ] . position g_value = 0 proton_index = - 1 for i , proton in enumerate ( protons ): proton_pos = self . u . atoms [ proton ] . position r_da = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) r_dh = minimize_vectors ( proton_pos - donor_pos , self . cell ) z1 = np . dot ( r_dh , r_da ) z2 = np . dot ( r_da , r_da ) z = ( z1 / z2 ) p = (( self . rho_max - z ) / ( self . rho_max - self . rho_0 )) if p >= 1 : g = 0 elif p <= 0 : g = 1 proton_index = protons [ i ] else : g = - 6 * ( p ** 5 ) + 15 * ( p ** 4 ) - 10 * ( p ** 3 ) + 1 g_value = g_value + g return g_value , proton_index def calculate_position ( self , paths : list , weights : list ): positions_all = [] nodes_all = [] weights_all = [] for i , path in enumerate ( paths ): for j , node in enumerate ( path ): if node not in nodes_all : donor_pos = self . u . atoms [ path [ 0 ]] . position if j == 0 : positions_all . append ( donor_pos ) else : acceptor_pos = self . u . atoms [ node ] . position min_vector = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) real_acceptor_pos = min_vector + donor_pos positions_all . append ( real_acceptor_pos ) nodes_all . append ( node ) weights_all . append ( weights [ i ][ j ]) else : index = nodes_all . index ( node ) weights_all [ index ] = max ( weights [ i ][ j ], weights_all [ index ]) p = np . array ( positions_all ) . reshape ( - 1 , 3 ) w = np . array ( weights_all ) . reshape ( 1 , - 1 ) z = w @ p pos_ind = z / w . sum () return pos_ind def analysis ( self , initial_donor : int , out_dir : str ): donor = initial_donor acceptor_query = ' or ' . join ( [ f '(name { el } )' for el in self . acceptor_elements ]) rand_file = io . FileIO ( os . path . join ( out_dir , f ' { initial_donor } .jsonl' ), 'w' ) writer = io . BufferedWriter ( rand_file ) line = ( tuple ( self . u . atoms [ initial_donor ] . position . astype ( float )), []) writer . write (( json . dumps ( line ) + ' \\n ' ) . encode ( 'utf-8' )) for time in range ( self . u . trajectory . n_frames - 1 ): donor , result = self . frame_analysis ( donor , acceptor_query , time + 1 ) line = ( result . indicator_position , result . transfers ) writer . write (( json . dumps ( line ) + ' \\n ' ) . encode ( 'utf-8' )) writer . flush () def proton_transfer_detection ( input_traj : str , out_dir : str , cell : List [ float ], acceptor_elements : List [ str ], initial_donors : List [ int ], core_num : int = 4 , dt : float = 0.0005 , r_a : float = 4.0 , r_h : float = 1.3 , rho_0 : float = 1 / 2.2 , rho_max : float = 0.5 , max_depth : int = 4 , g_threshold : float = 0.0001 , ): os . makedirs ( out_dir , exist_ok = True ) u = Universe ( input_traj ) u . trajectory . ts . dt = dt u . dimensions = np . array ( cell ) sys_info = SystemInfo ( initial_donor =- 1 , u = u , cell = cell , acceptor_elements = acceptor_elements ) parameter = AlgorithmParameter ( r_a = r_a , r_h = r_h , rho_0 = rho_0 , rho_max = rho_max , max_depth = max_depth , g_threshold = g_threshold ) system = System ( sys_info , parameter , ) with Pool ( processes = core_num ) as pool : pool . map ( partial ( system . analysis , out_dir = out_dir ), initial_donors ) def visualize_transfer ( analysis_result : str , input_traj : str , output_traj : str , initial_donor : int , cell : list ): stc_list = ai . read ( input_traj , index = \":\" ) donor = initial_donor with open ( os . path . join ( analysis_result , f ' { initial_donor } .jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) stc_list [ i ][ donor ] . symbol = 'N' stc_list [ i ] . set_cell ( cell ) stc_list [ i ] . set_pbc ( True ) if line [ 1 ]: donor = line [ 1 ][ - 1 ][ 0 ] pos = line [ 0 ] ind = Atom ( 'F' , pos ) stc_list [ i ] . append ( ind ) ai . write ( output_traj , stc_list ) def analysis_transfer_paths ( analysis_result : str , initial_donor : int ): donor = initial_donor print ( \"transfer_paths\" ) fmt = \" {:^40} \\t {:^8} \" content = fmt . format ( \"transfer_path_index\" , \"Snapshot\" ) print ( f \" { content } \" ) with open ( os . path . join ( analysis_result , f ' { initial_donor } .jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): acceptor = event [ 0 ] proton = event [ 1 ] content = f \" { donor } ( { proton } )->\" donor = acceptor content = content + f \" { acceptor } \" content = fmt . format ( f \" { content } \" , f \" { i } \" ) print ( content ) def detect_type_change ( analysis_result : str , atom_types : dict , donors : list ): type_change_event = [] type_list = [] type_change_name = [] for i in range ( len ( atom_types )): for j in range ( i + 1 ): type_change_event . append (([], [])) type_list . append ([ j , i ]) type_change_name . append ( f \" { list ( atom_types . keys ())[ j ] } <-> { list ( atom_types . keys ())[ i ] } \" ) for donor in donors : with open ( os . path . join ( analysis_result , f ' { donor } .jsonl' ), mode = 'r' ) as reader : change_times = 0 change_info = [] change = False for k in range ( len ( atom_types )): if donor in list ( atom_types . values ())[ k ]: change_info . append (( donor , k , 0 )) change_times = 1 for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): for k in range ( len ( atom_types )): if event [ 0 ] in list ( atom_types . values ())[ k ]: change_info . append (( event [ 0 ], k , i )) change_times = change_times + 1 change = True if change == False and change_times > 0 : change_info . append (( event [ 0 ], - 1 , i )) if change_times == 2 : real_type = change_info [ - 1 ][ 1 ] type = [ min ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ]), max ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ])] index = [ x [ 0 ] for x in change_info ] time = [ x [ 2 ] for x in change_info ] if index not in type_change_event [ type_list . index ( type )][ 0 ]: type_change_event [ type_list . index ( type )][ 0 ] . append ( index ) type_change_event [ type_list . index ( type )][ 1 ] . append ( time ) change_info = [( index [ - 1 ], real_type , time [ - 1 ])] change_times = 1 change = False for j , type_change in enumerate ( type_change_event ): type_change [ 0 ] . sort ( key = lambda x : len ( x )) type_change [ 1 ] . sort ( key = lambda x : len ( x )) print ( \"proton transfer type change\" ) print ( \"-------------------------------------\" ) fmt = \" {:^25} \\t {:^15} \\t {:^15} \" content = fmt . format ( \"Path_index\" , \"start_Snapshot\" , \"end_Snapshot\" ) print ( f \" { content } \" ) for i in range ( len ( type_change_name )): print ( type_change_name [ i ]) for j in range ( len ( type_change_event [ i ][ 0 ])): content = ' -> ' . join ([ f ' { el } ' for el in type_change_event [ i ][ 0 ][ j ]]) content = fmt . format ( f \" { content } \" , f \" { type_change_event [ i ][ 1 ][ j ][ 0 ] } \" , f \" { type_change_event [ i ][ 1 ][ j ][ - 1 ] } \" ) print ( f \" { content } \" ) if __name__ == '__main__' : fire . Fire ({ \"analyze\" : proton_transfer_detection , \"visualize\" : visualize_transfer , \"show-transfer-paths\" : analysis_transfer_paths , \"show-type-change\" : detect_type_change }) Functions analysis_transfer_paths def analysis_transfer_paths ( analysis_result : str , initial_donor : int ) View Source def analysis_transfer_paths ( analysis_result : str , initial_donor : int ): donor = initial_donor print ( \"transfer_paths\" ) fmt = \"{:^40} \\t {:^8}\" content = fmt . format ( \"transfer_path_index\" , \"Snapshot\" ) print ( f \"{content}\" ) with open ( os . path . join ( analysis_result , f '{initial_donor}.jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): acceptor = event [ 0 ] proton = event [ 1 ] content = f \"{donor}({proton})->\" donor = acceptor content = content + f \"{acceptor}\" content = fmt . format ( f \"{content}\" , f \"{i}\" ) print ( content ) detect_type_change def detect_type_change ( analysis_result : str , atom_types : dict , donors : list ) View Source def detect_type_change ( analysis_result : str , atom_types : dict , donors : list ): type_change_event = [] type_list = [] type_change_name = [] for i in range ( len ( atom_types )): for j in range ( i + 1 ): type_change_event . append (([], [])) type_list . append ([ j , i ]) type_change_name . append ( f \"{list(atom_types.keys())[j]}<->{list(atom_types.keys())[i]}\" ) for donor in donors : with open ( os . path . join ( analysis_result , f '{donor}.jsonl' ), mode = 'r' ) as reader : change_times = 0 change_info = [] change = False for k in range ( len ( atom_types )): if donor in list ( atom_types . values ())[ k ]: change_info . append (( donor , k , 0 )) change_times = 1 for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): for k in range ( len ( atom_types )): if event [ 0 ] in list ( atom_types . values ())[ k ]: change_info . append (( event [ 0 ], k , i )) change_times = change_times + 1 change = True if change == False and change_times > 0 : change_info . append (( event [ 0 ], - 1 , i )) if change_times == 2 : real_type = change_info [ - 1 ][ 1 ] type = [ min ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ]), max ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ])] index = [ x [ 0 ] for x in change_info ] time = [ x [ 2 ] for x in change_info ] if index not in type_change_event [ type_list . index ( type )][ 0 ]: type_change_event [ type_list . index ( type )][ 0 ] . append ( index ) type_change_event [ type_list . index ( type )][ 1 ] . append ( time ) change_info = [( index [ - 1 ], real_type , time [ - 1 ])] change_times = 1 change = False for j , type_change in enumerate ( type_change_event ): type_change [ 0 ] . sort ( key = lambda x : len ( x )) type_change [ 1 ] . sort ( key = lambda x : len ( x )) print ( \"proton transfer type change\" ) print ( \"-------------------------------------\" ) fmt = \"{:^25} \\t {:^15} \\t {:^15}\" content = fmt . format ( \"Path_index\" , \"start_Snapshot\" , \"end_Snapshot\" ) print ( f \"{content}\" ) for i in range ( len ( type_change_name )): print ( type_change_name [ i ]) for j in range ( len ( type_change_event [ i ][ 0 ])): content = ' -> ' . join ([ f '{el}' for el in type_change_event [ i ][ 0 ][ j ]]) content = fmt . format ( f \"{content}\" , f \"{type_change_event[i][1][j][0]}\" , f \"{type_change_event[i][1][j][-1]}\" ) print ( f \"{content}\" ) proton_transfer_detection def proton_transfer_detection ( input_traj : str , out_dir : str , cell : List [ float ], acceptor_elements : List [ str ], initial_donors : List [ int ], core_num : int = 4 , dt : float = 0.0005 , r_a : float = 4.0 , r_h : float = 1.3 , rho_0 : float = 0.45454545454545453 , rho_max : float = 0.5 , max_depth : int = 4 , g_threshold : float = 0.0001 ) View Source def proton_transfer_detection ( input_traj : str , out_dir : str , cell : List [ float ] , acceptor_elements : List [ str ] , initial_donors : List [ int ] , core_num : int = 4 , dt : float = 0.0005 , r_a : float = 4.0 , r_h : float = 1.3 , rho_0 : float = 1 / 2.2 , rho_max : float = 0.5 , max_depth : int = 4 , g_threshold : float = 0.0001 , ) : os . makedirs ( out_dir , exist_ok = True ) u = Universe ( input_traj ) u . trajectory . ts . dt = dt u . dimensions = np . array ( cell ) sys_info = SystemInfo ( initial_donor =- 1 , u = u , cell = cell , acceptor_elements = acceptor_elements ) parameter = AlgorithmParameter ( r_a = r_a , r_h = r_h , rho_0 = rho_0 , rho_max = rho_max , max_depth = max_depth , g_threshold = g_threshold ) system = System ( sys_info , parameter , ) with Pool ( processes = core_num ) as pool : pool . map ( partial ( system . analysis , out_dir = out_dir ), initial_donors ) visualize_transfer def visualize_transfer ( analysis_result : str , input_traj : str , output_traj : str , initial_donor : int , cell : list ) View Source def visualize_transfer ( analysis_result : str , input_traj : str , output_traj : str , initial_donor : int , cell : list ): stc_list = ai . read ( input_traj , index = \":\" ) donor = initial_donor with open ( os . path . join ( analysis_result , f '{initial_donor}.jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) stc_list [ i ][ donor ] . symbol = 'N' stc_list [ i ] . set_cell ( cell ) stc_list [ i ] . set_pbc ( True ) if line [ 1 ]: donor = line [ 1 ][ - 1 ][ 0 ] pos = line [ 0 ] ind = Atom ( 'F' , pos ) stc_list [ i ] . append ( ind ) ai . write ( output_traj , stc_list ) Classes AlgorithmParameter class AlgorithmParameter ( / , * args , ** kwargs ) AlgorithmParameter(r_a, r_h, rho_0, rho_max, max_depth, g_threshold) View Source class AlgorithmParameter ( NamedTuple ): r_a: float # The radius used to search for acceptor r_h: float # The radius used to search for H rho_0: float # Control the rate of the weights change rho_max: float # The critical value of proton transfer max_depth: int # The maximum length of the path g_threshold: float # The threshold for whether to join the node to the path Ancestors (in MRO) builtins.tuple Class variables g_threshold max_depth r_a r_h rho_0 rho_max Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. AnalysisResult class AnalysisResult ( / , * args , ** kwargs ) AnalysisResult(indicator_position, transfers) View Source class AnalysisResult ( NamedTuple ): indicator_position: Tuple [ float , float , float ] transfers: List [ Tuple [ int , int ]] Ancestors (in MRO) builtins.tuple Class variables indicator_position transfers Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. System class System ( sys_info : ai2_kit . algorithm . proton_transfer . SystemInfo , parameter : ai2_kit . algorithm . proton_transfer . AlgorithmParameter ) View Source class System ( object ) : def __init__ ( self , sys_info : SystemInfo , parameter : AlgorithmParameter ) : self . u = sys_info . u self . cell = sys_info . cell self . r_a = parameter . r_a self . r_h = parameter . r_h self . g_threshold = parameter . g_threshold self . max_depth = parameter . max_depth self . rho_0 = parameter . rho_0 self . rho_max = parameter . rho_max self . acceptor_elements = sys_info . acceptor_elements def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ) : self . u . trajectory [ time ] donor = prev_donor transfers = [] list_of_paths = [ [prev_donor ] ] list_of_weights = [ [1 ] ] for depth in range ( self . max_depth ) : for j , path in enumerate ( list_of_paths ) : found = False if depth == len ( path ) - 1 : acceptors = self . u . select_atoms ( f \"(around {self.r_a} index {path[-1]}) and ({acceptor_query})\" ) protons = self . u . select_atoms ( f \"(around {self.r_h} index {path[-1]}) and (name H)\" ) for i , acceptor in enumerate ( acceptors . ix ) : g , proton = self . calculate_g ( path [ -1 ] , acceptor , protons . ix ) if ( g >= self . g_threshold ) and ( acceptor not in path ) : found = True list_of_weights . append ( list_of_weights [ j ] + [ g * list_of_weights[j ][ -1 ] ] ) list_of_paths . append ( path + [ acceptor ] ) if proton > 0 and all ( w >= 0.9 for w in list_of_weights [ j ] ) : donor = acceptor transfers . append (( int ( acceptor ), int ( proton ))) if found : list_of_paths . pop ( j ) list_of_weights . pop ( j ) indicator_position = self . calculate_position ( list_of_paths , list_of_weights ) result = AnalysisResult ( indicator_position = tuple ( indicator_position [ 0 ] ), transfers = transfers ) return donor , result def calculate_g ( self , donor : int , acceptor : int , protons : list ) : donor_pos = self . u . atoms [ donor ] . position acceptor_pos = self . u . atoms [ acceptor ] . position g_value = 0 proton_index = - 1 for i , proton in enumerate ( protons ) : proton_pos = self . u . atoms [ proton ] . position r_da = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) r_dh = minimize_vectors ( proton_pos - donor_pos , self . cell ) z1 = np . dot ( r_dh , r_da ) z2 = np . dot ( r_da , r_da ) z = ( z1 / z2 ) p = (( self . rho_max - z ) / ( self . rho_max - self . rho_0 )) if p >= 1 : g = 0 elif p <= 0 : g = 1 proton_index = protons [ i ] else : g = - 6 * ( p ** 5 ) + 15 * ( p ** 4 ) - 10 * ( p ** 3 ) + 1 g_value = g_value + g return g_value , proton_index def calculate_position ( self , paths : list , weights : list ) : positions_all = [] nodes_all = [] weights_all = [] for i , path in enumerate ( paths ) : for j , node in enumerate ( path ) : if node not in nodes_all : donor_pos = self . u . atoms [ path[0 ] ] . position if j == 0 : positions_all . append ( donor_pos ) else : acceptor_pos = self . u . atoms [ node ] . position min_vector = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) real_acceptor_pos = min_vector + donor_pos positions_all . append ( real_acceptor_pos ) nodes_all . append ( node ) weights_all . append ( weights [ i ][ j ] ) else : index = nodes_all . index ( node ) weights_all [ index ] = max ( weights [ i ][ j ] , weights_all [ index ] ) p = np . array ( positions_all ). reshape ( - 1 , 3 ) w = np . array ( weights_all ). reshape ( 1 , - 1 ) z = w @ p pos_ind = z / w . sum () return pos_ind def analysis ( self , initial_donor : int , out_dir : str ) : donor = initial_donor acceptor_query = ' or ' . join ( [ f'(name {el})' for el in self.acceptor_elements ] ) rand_file = io . FileIO ( os . path . join ( out_dir , f '{initial_donor}.jsonl' ), 'w' ) writer = io . BufferedWriter ( rand_file ) line = ( tuple ( self . u . atoms [ initial_donor ] . position . astype ( float )), [] ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) for time in range ( self . u . trajectory . n_frames - 1 ) : donor , result = self . frame_analysis ( donor , acceptor_query , time + 1 ) line = ( result . indicator_position , result . transfers ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) writer . flush () Methods analysis def analysis ( self , initial_donor : int , out_dir : str ) View Source def analysis ( self , initial_donor : int , out_dir : str ) : donor = initial_donor acceptor_query = ' or ' . join ( [ f'(name {el})' for el in self.acceptor_elements ] ) rand_file = io . FileIO ( os . path . join ( out_dir , f '{initial_donor}.jsonl' ), 'w' ) writer = io . BufferedWriter ( rand_file ) line = ( tuple ( self . u . atoms [ initial_donor ] . position . astype ( float )), [] ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) for time in range ( self . u . trajectory . n_frames - 1 ) : donor , result = self . frame_analysis ( donor , acceptor_query , time + 1 ) line = ( result . indicator_position , result . transfers ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) writer . flush () calculate_g def calculate_g ( self , donor : int , acceptor : int , protons : list ) View Source def calculate_g ( self , donor : int , acceptor : int , protons : list ) : donor_pos = self . u . atoms [ donor ] . position acceptor_pos = self . u . atoms [ acceptor ] . position g_value = 0 proton_index = - 1 for i , proton in enumerate ( protons ) : proton_pos = self . u . atoms [ proton ] . position r_da = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) r_dh = minimize_vectors ( proton_pos - donor_pos , self . cell ) z1 = np . dot ( r_dh , r_da ) z2 = np . dot ( r_da , r_da ) z = ( z1 / z2 ) p = (( self . rho_max - z ) / ( self . rho_max - self . rho_0 )) if p >= 1 : g = 0 elif p <= 0 : g = 1 proton_index = protons [ i ] else : g = - 6 * ( p ** 5 ) + 15 * ( p ** 4 ) - 10 * ( p ** 3 ) + 1 g_value = g_value + g return g_value , proton_index calculate_position def calculate_position ( self , paths : list , weights : list ) View Source def calculate_position ( self , paths : list , weights : list ) : positions_all = [] nodes_all = [] weights_all = [] for i , path in enumerate ( paths ) : for j , node in enumerate ( path ) : if node not in nodes_all : donor_pos = self . u . atoms [ path[0 ] ] . position if j == 0 : positions_all . append ( donor_pos ) else : acceptor_pos = self . u . atoms [ node ] . position min_vector = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) real_acceptor_pos = min_vector + donor_pos positions_all . append ( real_acceptor_pos ) nodes_all . append ( node ) weights_all . append ( weights [ i ][ j ] ) else : index = nodes_all . index ( node ) weights_all [ index ] = max ( weights [ i ][ j ] , weights_all [ index ] ) p = np . array ( positions_all ). reshape ( - 1 , 3 ) w = np . array ( weights_all ). reshape ( 1 , - 1 ) z = w @ p pos_ind = z / w . sum () return pos_ind frame_analysis def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ) View Source def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ) : self . u . trajectory [ time ] donor = prev_donor transfers = [] list_of_paths = [ [prev_donor ] ] list_of_weights = [ [1 ] ] for depth in range ( self . max_depth ) : for j , path in enumerate ( list_of_paths ) : found = False if depth == len ( path ) - 1 : acceptors = self . u . select_atoms ( f \"(around {self.r_a} index {path[-1]}) and ({acceptor_query})\" ) protons = self . u . select_atoms ( f \"(around {self.r_h} index {path[-1]}) and (name H)\" ) for i , acceptor in enumerate ( acceptors . ix ) : g , proton = self . calculate_g ( path [ -1 ] , acceptor , protons . ix ) if ( g >= self . g_threshold ) and ( acceptor not in path ) : found = True list_of_weights . append ( list_of_weights [ j ] + [ g * list_of_weights[j ][ -1 ] ] ) list_of_paths . append ( path + [ acceptor ] ) if proton > 0 and all ( w >= 0.9 for w in list_of_weights [ j ] ) : donor = acceptor transfers . append (( int ( acceptor ), int ( proton ))) if found : list_of_paths . pop ( j ) list_of_weights . pop ( j ) indicator_position = self . calculate_position ( list_of_paths , list_of_weights ) result = AnalysisResult ( indicator_position = tuple ( indicator_position [ 0 ] ), transfers = transfers ) return donor , result SystemInfo class SystemInfo ( initial_donor : int , u : MDAnalysis . core . universe . Universe , cell : List [ float ], acceptor_elements : List [ str ] ) SystemInfo(initial_donor: int, u: MDAnalysis.core.universe.Universe, cell: List[float], acceptor_elements: List[str]) View Source class SystemInfo : # Information about the system initial_donor : int u : Universe cell : List [ float ] acceptor_elements : List [ str ]","title":"Proton Transfer"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#module-ai2_kitalgorithmproton_transfer","text":"View Source from MDAnalysis import Universe from MDAnalysis.lib.distances import minimize_vectors from multiprocessing.pool import Pool import ase.io as ai from ase import Atom from functools import partial from dataclasses import dataclass from typing import List , Tuple , NamedTuple import fire import numpy as np import json import os import io # TODO: use array instead of list for better performance when possible # TODO: use numba to speed up the calculation class AnalysisResult ( NamedTuple ): indicator_position : Tuple [ float , float , float ] transfers : List [ Tuple [ int , int ]] @dataclass class SystemInfo : # Information about the system initial_donor : int u : Universe cell : List [ float ] acceptor_elements : List [ str ] # The relevant parameters of the algorithm class AlgorithmParameter ( NamedTuple ): r_a : float # The radius used to search for acceptor r_h : float # The radius used to search for H rho_0 : float # Control the rate of the weights change rho_max : float # The critical value of proton transfer max_depth : int # The maximum length of the path g_threshold : float # The threshold for whether to join the node to the path class System ( object ): def __init__ ( self , sys_info : SystemInfo , parameter : AlgorithmParameter ): self . u = sys_info . u self . cell = sys_info . cell self . r_a = parameter . r_a self . r_h = parameter . r_h self . g_threshold = parameter . g_threshold self . max_depth = parameter . max_depth self . rho_0 = parameter . rho_0 self . rho_max = parameter . rho_max self . acceptor_elements = sys_info . acceptor_elements def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ): self . u . trajectory [ time ] donor = prev_donor transfers = [] list_of_paths = [[ prev_donor ]] list_of_weights = [[ 1 ]] for depth in range ( self . max_depth ): for j , path in enumerate ( list_of_paths ): found = False if depth == len ( path ) - 1 : acceptors = self . u . select_atoms ( f \"(around { self . r_a } index { path [ - 1 ] } ) and ( { acceptor_query } )\" ) protons = self . u . select_atoms ( f \"(around { self . r_h } index { path [ - 1 ] } ) and (name H)\" ) for i , acceptor in enumerate ( acceptors . ix ): g , proton = self . calculate_g ( path [ - 1 ], acceptor , protons . ix ) if ( g >= self . g_threshold ) and ( acceptor not in path ): found = True list_of_weights . append ( list_of_weights [ j ] + [ g * list_of_weights [ j ][ - 1 ]]) list_of_paths . append ( path + [ acceptor ]) if proton > 0 and all ( w >= 0.9 for w in list_of_weights [ j ]): donor = acceptor transfers . append (( int ( acceptor ), int ( proton ))) if found : list_of_paths . pop ( j ) list_of_weights . pop ( j ) indicator_position = self . calculate_position ( list_of_paths , list_of_weights ) result = AnalysisResult ( indicator_position = tuple ( indicator_position [ 0 ]), transfers = transfers ) return donor , result def calculate_g ( self , donor : int , acceptor : int , protons : list ): donor_pos = self . u . atoms [ donor ] . position acceptor_pos = self . u . atoms [ acceptor ] . position g_value = 0 proton_index = - 1 for i , proton in enumerate ( protons ): proton_pos = self . u . atoms [ proton ] . position r_da = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) r_dh = minimize_vectors ( proton_pos - donor_pos , self . cell ) z1 = np . dot ( r_dh , r_da ) z2 = np . dot ( r_da , r_da ) z = ( z1 / z2 ) p = (( self . rho_max - z ) / ( self . rho_max - self . rho_0 )) if p >= 1 : g = 0 elif p <= 0 : g = 1 proton_index = protons [ i ] else : g = - 6 * ( p ** 5 ) + 15 * ( p ** 4 ) - 10 * ( p ** 3 ) + 1 g_value = g_value + g return g_value , proton_index def calculate_position ( self , paths : list , weights : list ): positions_all = [] nodes_all = [] weights_all = [] for i , path in enumerate ( paths ): for j , node in enumerate ( path ): if node not in nodes_all : donor_pos = self . u . atoms [ path [ 0 ]] . position if j == 0 : positions_all . append ( donor_pos ) else : acceptor_pos = self . u . atoms [ node ] . position min_vector = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) real_acceptor_pos = min_vector + donor_pos positions_all . append ( real_acceptor_pos ) nodes_all . append ( node ) weights_all . append ( weights [ i ][ j ]) else : index = nodes_all . index ( node ) weights_all [ index ] = max ( weights [ i ][ j ], weights_all [ index ]) p = np . array ( positions_all ) . reshape ( - 1 , 3 ) w = np . array ( weights_all ) . reshape ( 1 , - 1 ) z = w @ p pos_ind = z / w . sum () return pos_ind def analysis ( self , initial_donor : int , out_dir : str ): donor = initial_donor acceptor_query = ' or ' . join ( [ f '(name { el } )' for el in self . acceptor_elements ]) rand_file = io . FileIO ( os . path . join ( out_dir , f ' { initial_donor } .jsonl' ), 'w' ) writer = io . BufferedWriter ( rand_file ) line = ( tuple ( self . u . atoms [ initial_donor ] . position . astype ( float )), []) writer . write (( json . dumps ( line ) + ' \\n ' ) . encode ( 'utf-8' )) for time in range ( self . u . trajectory . n_frames - 1 ): donor , result = self . frame_analysis ( donor , acceptor_query , time + 1 ) line = ( result . indicator_position , result . transfers ) writer . write (( json . dumps ( line ) + ' \\n ' ) . encode ( 'utf-8' )) writer . flush () def proton_transfer_detection ( input_traj : str , out_dir : str , cell : List [ float ], acceptor_elements : List [ str ], initial_donors : List [ int ], core_num : int = 4 , dt : float = 0.0005 , r_a : float = 4.0 , r_h : float = 1.3 , rho_0 : float = 1 / 2.2 , rho_max : float = 0.5 , max_depth : int = 4 , g_threshold : float = 0.0001 , ): os . makedirs ( out_dir , exist_ok = True ) u = Universe ( input_traj ) u . trajectory . ts . dt = dt u . dimensions = np . array ( cell ) sys_info = SystemInfo ( initial_donor =- 1 , u = u , cell = cell , acceptor_elements = acceptor_elements ) parameter = AlgorithmParameter ( r_a = r_a , r_h = r_h , rho_0 = rho_0 , rho_max = rho_max , max_depth = max_depth , g_threshold = g_threshold ) system = System ( sys_info , parameter , ) with Pool ( processes = core_num ) as pool : pool . map ( partial ( system . analysis , out_dir = out_dir ), initial_donors ) def visualize_transfer ( analysis_result : str , input_traj : str , output_traj : str , initial_donor : int , cell : list ): stc_list = ai . read ( input_traj , index = \":\" ) donor = initial_donor with open ( os . path . join ( analysis_result , f ' { initial_donor } .jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) stc_list [ i ][ donor ] . symbol = 'N' stc_list [ i ] . set_cell ( cell ) stc_list [ i ] . set_pbc ( True ) if line [ 1 ]: donor = line [ 1 ][ - 1 ][ 0 ] pos = line [ 0 ] ind = Atom ( 'F' , pos ) stc_list [ i ] . append ( ind ) ai . write ( output_traj , stc_list ) def analysis_transfer_paths ( analysis_result : str , initial_donor : int ): donor = initial_donor print ( \"transfer_paths\" ) fmt = \" {:^40} \\t {:^8} \" content = fmt . format ( \"transfer_path_index\" , \"Snapshot\" ) print ( f \" { content } \" ) with open ( os . path . join ( analysis_result , f ' { initial_donor } .jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): acceptor = event [ 0 ] proton = event [ 1 ] content = f \" { donor } ( { proton } )->\" donor = acceptor content = content + f \" { acceptor } \" content = fmt . format ( f \" { content } \" , f \" { i } \" ) print ( content ) def detect_type_change ( analysis_result : str , atom_types : dict , donors : list ): type_change_event = [] type_list = [] type_change_name = [] for i in range ( len ( atom_types )): for j in range ( i + 1 ): type_change_event . append (([], [])) type_list . append ([ j , i ]) type_change_name . append ( f \" { list ( atom_types . keys ())[ j ] } <-> { list ( atom_types . keys ())[ i ] } \" ) for donor in donors : with open ( os . path . join ( analysis_result , f ' { donor } .jsonl' ), mode = 'r' ) as reader : change_times = 0 change_info = [] change = False for k in range ( len ( atom_types )): if donor in list ( atom_types . values ())[ k ]: change_info . append (( donor , k , 0 )) change_times = 1 for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): for k in range ( len ( atom_types )): if event [ 0 ] in list ( atom_types . values ())[ k ]: change_info . append (( event [ 0 ], k , i )) change_times = change_times + 1 change = True if change == False and change_times > 0 : change_info . append (( event [ 0 ], - 1 , i )) if change_times == 2 : real_type = change_info [ - 1 ][ 1 ] type = [ min ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ]), max ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ])] index = [ x [ 0 ] for x in change_info ] time = [ x [ 2 ] for x in change_info ] if index not in type_change_event [ type_list . index ( type )][ 0 ]: type_change_event [ type_list . index ( type )][ 0 ] . append ( index ) type_change_event [ type_list . index ( type )][ 1 ] . append ( time ) change_info = [( index [ - 1 ], real_type , time [ - 1 ])] change_times = 1 change = False for j , type_change in enumerate ( type_change_event ): type_change [ 0 ] . sort ( key = lambda x : len ( x )) type_change [ 1 ] . sort ( key = lambda x : len ( x )) print ( \"proton transfer type change\" ) print ( \"-------------------------------------\" ) fmt = \" {:^25} \\t {:^15} \\t {:^15} \" content = fmt . format ( \"Path_index\" , \"start_Snapshot\" , \"end_Snapshot\" ) print ( f \" { content } \" ) for i in range ( len ( type_change_name )): print ( type_change_name [ i ]) for j in range ( len ( type_change_event [ i ][ 0 ])): content = ' -> ' . join ([ f ' { el } ' for el in type_change_event [ i ][ 0 ][ j ]]) content = fmt . format ( f \" { content } \" , f \" { type_change_event [ i ][ 1 ][ j ][ 0 ] } \" , f \" { type_change_event [ i ][ 1 ][ j ][ - 1 ] } \" ) print ( f \" { content } \" ) if __name__ == '__main__' : fire . Fire ({ \"analyze\" : proton_transfer_detection , \"visualize\" : visualize_transfer , \"show-transfer-paths\" : analysis_transfer_paths , \"show-type-change\" : detect_type_change })","title":"Module ai2_kit.algorithm.proton_transfer"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#analysis_transfer_paths","text":"def analysis_transfer_paths ( analysis_result : str , initial_donor : int ) View Source def analysis_transfer_paths ( analysis_result : str , initial_donor : int ): donor = initial_donor print ( \"transfer_paths\" ) fmt = \"{:^40} \\t {:^8}\" content = fmt . format ( \"transfer_path_index\" , \"Snapshot\" ) print ( f \"{content}\" ) with open ( os . path . join ( analysis_result , f '{initial_donor}.jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): acceptor = event [ 0 ] proton = event [ 1 ] content = f \"{donor}({proton})->\" donor = acceptor content = content + f \"{acceptor}\" content = fmt . format ( f \"{content}\" , f \"{i}\" ) print ( content )","title":"analysis_transfer_paths"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#detect_type_change","text":"def detect_type_change ( analysis_result : str , atom_types : dict , donors : list ) View Source def detect_type_change ( analysis_result : str , atom_types : dict , donors : list ): type_change_event = [] type_list = [] type_change_name = [] for i in range ( len ( atom_types )): for j in range ( i + 1 ): type_change_event . append (([], [])) type_list . append ([ j , i ]) type_change_name . append ( f \"{list(atom_types.keys())[j]}<->{list(atom_types.keys())[i]}\" ) for donor in donors : with open ( os . path . join ( analysis_result , f '{donor}.jsonl' ), mode = 'r' ) as reader : change_times = 0 change_info = [] change = False for k in range ( len ( atom_types )): if donor in list ( atom_types . values ())[ k ]: change_info . append (( donor , k , 0 )) change_times = 1 for i , line in enumerate ( reader ): line = json . loads ( line ) if line [ 1 ]: for j , event in enumerate ( line [ 1 ]): for k in range ( len ( atom_types )): if event [ 0 ] in list ( atom_types . values ())[ k ]: change_info . append (( event [ 0 ], k , i )) change_times = change_times + 1 change = True if change == False and change_times > 0 : change_info . append (( event [ 0 ], - 1 , i )) if change_times == 2 : real_type = change_info [ - 1 ][ 1 ] type = [ min ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ]), max ( change_info [ 0 ][ 1 ], change_info [ - 1 ][ 1 ])] index = [ x [ 0 ] for x in change_info ] time = [ x [ 2 ] for x in change_info ] if index not in type_change_event [ type_list . index ( type )][ 0 ]: type_change_event [ type_list . index ( type )][ 0 ] . append ( index ) type_change_event [ type_list . index ( type )][ 1 ] . append ( time ) change_info = [( index [ - 1 ], real_type , time [ - 1 ])] change_times = 1 change = False for j , type_change in enumerate ( type_change_event ): type_change [ 0 ] . sort ( key = lambda x : len ( x )) type_change [ 1 ] . sort ( key = lambda x : len ( x )) print ( \"proton transfer type change\" ) print ( \"-------------------------------------\" ) fmt = \"{:^25} \\t {:^15} \\t {:^15}\" content = fmt . format ( \"Path_index\" , \"start_Snapshot\" , \"end_Snapshot\" ) print ( f \"{content}\" ) for i in range ( len ( type_change_name )): print ( type_change_name [ i ]) for j in range ( len ( type_change_event [ i ][ 0 ])): content = ' -> ' . join ([ f '{el}' for el in type_change_event [ i ][ 0 ][ j ]]) content = fmt . format ( f \"{content}\" , f \"{type_change_event[i][1][j][0]}\" , f \"{type_change_event[i][1][j][-1]}\" ) print ( f \"{content}\" )","title":"detect_type_change"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#proton_transfer_detection","text":"def proton_transfer_detection ( input_traj : str , out_dir : str , cell : List [ float ], acceptor_elements : List [ str ], initial_donors : List [ int ], core_num : int = 4 , dt : float = 0.0005 , r_a : float = 4.0 , r_h : float = 1.3 , rho_0 : float = 0.45454545454545453 , rho_max : float = 0.5 , max_depth : int = 4 , g_threshold : float = 0.0001 ) View Source def proton_transfer_detection ( input_traj : str , out_dir : str , cell : List [ float ] , acceptor_elements : List [ str ] , initial_donors : List [ int ] , core_num : int = 4 , dt : float = 0.0005 , r_a : float = 4.0 , r_h : float = 1.3 , rho_0 : float = 1 / 2.2 , rho_max : float = 0.5 , max_depth : int = 4 , g_threshold : float = 0.0001 , ) : os . makedirs ( out_dir , exist_ok = True ) u = Universe ( input_traj ) u . trajectory . ts . dt = dt u . dimensions = np . array ( cell ) sys_info = SystemInfo ( initial_donor =- 1 , u = u , cell = cell , acceptor_elements = acceptor_elements ) parameter = AlgorithmParameter ( r_a = r_a , r_h = r_h , rho_0 = rho_0 , rho_max = rho_max , max_depth = max_depth , g_threshold = g_threshold ) system = System ( sys_info , parameter , ) with Pool ( processes = core_num ) as pool : pool . map ( partial ( system . analysis , out_dir = out_dir ), initial_donors )","title":"proton_transfer_detection"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#visualize_transfer","text":"def visualize_transfer ( analysis_result : str , input_traj : str , output_traj : str , initial_donor : int , cell : list ) View Source def visualize_transfer ( analysis_result : str , input_traj : str , output_traj : str , initial_donor : int , cell : list ): stc_list = ai . read ( input_traj , index = \":\" ) donor = initial_donor with open ( os . path . join ( analysis_result , f '{initial_donor}.jsonl' ), mode = 'r' ) as reader : for i , line in enumerate ( reader ): line = json . loads ( line ) stc_list [ i ][ donor ] . symbol = 'N' stc_list [ i ] . set_cell ( cell ) stc_list [ i ] . set_pbc ( True ) if line [ 1 ]: donor = line [ 1 ][ - 1 ][ 0 ] pos = line [ 0 ] ind = Atom ( 'F' , pos ) stc_list [ i ] . append ( ind ) ai . write ( output_traj , stc_list )","title":"visualize_transfer"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#algorithmparameter","text":"class AlgorithmParameter ( / , * args , ** kwargs ) AlgorithmParameter(r_a, r_h, rho_0, rho_max, max_depth, g_threshold) View Source class AlgorithmParameter ( NamedTuple ): r_a: float # The radius used to search for acceptor r_h: float # The radius used to search for H rho_0: float # Control the rate of the weights change rho_max: float # The critical value of proton transfer max_depth: int # The maximum length of the path g_threshold: float # The threshold for whether to join the node to the path","title":"AlgorithmParameter"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#class-variables","text":"g_threshold max_depth r_a r_h rho_0 rho_max","title":"Class variables"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#analysisresult","text":"class AnalysisResult ( / , * args , ** kwargs ) AnalysisResult(indicator_position, transfers) View Source class AnalysisResult ( NamedTuple ): indicator_position: Tuple [ float , float , float ] transfers: List [ Tuple [ int , int ]]","title":"AnalysisResult"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#ancestors-in-mro_1","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#class-variables_1","text":"indicator_position transfers","title":"Class variables"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#count_1","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#index_1","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#system","text":"class System ( sys_info : ai2_kit . algorithm . proton_transfer . SystemInfo , parameter : ai2_kit . algorithm . proton_transfer . AlgorithmParameter ) View Source class System ( object ) : def __init__ ( self , sys_info : SystemInfo , parameter : AlgorithmParameter ) : self . u = sys_info . u self . cell = sys_info . cell self . r_a = parameter . r_a self . r_h = parameter . r_h self . g_threshold = parameter . g_threshold self . max_depth = parameter . max_depth self . rho_0 = parameter . rho_0 self . rho_max = parameter . rho_max self . acceptor_elements = sys_info . acceptor_elements def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ) : self . u . trajectory [ time ] donor = prev_donor transfers = [] list_of_paths = [ [prev_donor ] ] list_of_weights = [ [1 ] ] for depth in range ( self . max_depth ) : for j , path in enumerate ( list_of_paths ) : found = False if depth == len ( path ) - 1 : acceptors = self . u . select_atoms ( f \"(around {self.r_a} index {path[-1]}) and ({acceptor_query})\" ) protons = self . u . select_atoms ( f \"(around {self.r_h} index {path[-1]}) and (name H)\" ) for i , acceptor in enumerate ( acceptors . ix ) : g , proton = self . calculate_g ( path [ -1 ] , acceptor , protons . ix ) if ( g >= self . g_threshold ) and ( acceptor not in path ) : found = True list_of_weights . append ( list_of_weights [ j ] + [ g * list_of_weights[j ][ -1 ] ] ) list_of_paths . append ( path + [ acceptor ] ) if proton > 0 and all ( w >= 0.9 for w in list_of_weights [ j ] ) : donor = acceptor transfers . append (( int ( acceptor ), int ( proton ))) if found : list_of_paths . pop ( j ) list_of_weights . pop ( j ) indicator_position = self . calculate_position ( list_of_paths , list_of_weights ) result = AnalysisResult ( indicator_position = tuple ( indicator_position [ 0 ] ), transfers = transfers ) return donor , result def calculate_g ( self , donor : int , acceptor : int , protons : list ) : donor_pos = self . u . atoms [ donor ] . position acceptor_pos = self . u . atoms [ acceptor ] . position g_value = 0 proton_index = - 1 for i , proton in enumerate ( protons ) : proton_pos = self . u . atoms [ proton ] . position r_da = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) r_dh = minimize_vectors ( proton_pos - donor_pos , self . cell ) z1 = np . dot ( r_dh , r_da ) z2 = np . dot ( r_da , r_da ) z = ( z1 / z2 ) p = (( self . rho_max - z ) / ( self . rho_max - self . rho_0 )) if p >= 1 : g = 0 elif p <= 0 : g = 1 proton_index = protons [ i ] else : g = - 6 * ( p ** 5 ) + 15 * ( p ** 4 ) - 10 * ( p ** 3 ) + 1 g_value = g_value + g return g_value , proton_index def calculate_position ( self , paths : list , weights : list ) : positions_all = [] nodes_all = [] weights_all = [] for i , path in enumerate ( paths ) : for j , node in enumerate ( path ) : if node not in nodes_all : donor_pos = self . u . atoms [ path[0 ] ] . position if j == 0 : positions_all . append ( donor_pos ) else : acceptor_pos = self . u . atoms [ node ] . position min_vector = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) real_acceptor_pos = min_vector + donor_pos positions_all . append ( real_acceptor_pos ) nodes_all . append ( node ) weights_all . append ( weights [ i ][ j ] ) else : index = nodes_all . index ( node ) weights_all [ index ] = max ( weights [ i ][ j ] , weights_all [ index ] ) p = np . array ( positions_all ). reshape ( - 1 , 3 ) w = np . array ( weights_all ). reshape ( 1 , - 1 ) z = w @ p pos_ind = z / w . sum () return pos_ind def analysis ( self , initial_donor : int , out_dir : str ) : donor = initial_donor acceptor_query = ' or ' . join ( [ f'(name {el})' for el in self.acceptor_elements ] ) rand_file = io . FileIO ( os . path . join ( out_dir , f '{initial_donor}.jsonl' ), 'w' ) writer = io . BufferedWriter ( rand_file ) line = ( tuple ( self . u . atoms [ initial_donor ] . position . astype ( float )), [] ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) for time in range ( self . u . trajectory . n_frames - 1 ) : donor , result = self . frame_analysis ( donor , acceptor_query , time + 1 ) line = ( result . indicator_position , result . transfers ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) writer . flush ()","title":"System"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#analysis","text":"def analysis ( self , initial_donor : int , out_dir : str ) View Source def analysis ( self , initial_donor : int , out_dir : str ) : donor = initial_donor acceptor_query = ' or ' . join ( [ f'(name {el})' for el in self.acceptor_elements ] ) rand_file = io . FileIO ( os . path . join ( out_dir , f '{initial_donor}.jsonl' ), 'w' ) writer = io . BufferedWriter ( rand_file ) line = ( tuple ( self . u . atoms [ initial_donor ] . position . astype ( float )), [] ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) for time in range ( self . u . trajectory . n_frames - 1 ) : donor , result = self . frame_analysis ( donor , acceptor_query , time + 1 ) line = ( result . indicator_position , result . transfers ) writer . write (( json . dumps ( line ) + '\\n' ). encode ( 'utf-8' )) writer . flush ()","title":"analysis"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#calculate_g","text":"def calculate_g ( self , donor : int , acceptor : int , protons : list ) View Source def calculate_g ( self , donor : int , acceptor : int , protons : list ) : donor_pos = self . u . atoms [ donor ] . position acceptor_pos = self . u . atoms [ acceptor ] . position g_value = 0 proton_index = - 1 for i , proton in enumerate ( protons ) : proton_pos = self . u . atoms [ proton ] . position r_da = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) r_dh = minimize_vectors ( proton_pos - donor_pos , self . cell ) z1 = np . dot ( r_dh , r_da ) z2 = np . dot ( r_da , r_da ) z = ( z1 / z2 ) p = (( self . rho_max - z ) / ( self . rho_max - self . rho_0 )) if p >= 1 : g = 0 elif p <= 0 : g = 1 proton_index = protons [ i ] else : g = - 6 * ( p ** 5 ) + 15 * ( p ** 4 ) - 10 * ( p ** 3 ) + 1 g_value = g_value + g return g_value , proton_index","title":"calculate_g"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#calculate_position","text":"def calculate_position ( self , paths : list , weights : list ) View Source def calculate_position ( self , paths : list , weights : list ) : positions_all = [] nodes_all = [] weights_all = [] for i , path in enumerate ( paths ) : for j , node in enumerate ( path ) : if node not in nodes_all : donor_pos = self . u . atoms [ path[0 ] ] . position if j == 0 : positions_all . append ( donor_pos ) else : acceptor_pos = self . u . atoms [ node ] . position min_vector = minimize_vectors ( acceptor_pos - donor_pos , self . cell ) real_acceptor_pos = min_vector + donor_pos positions_all . append ( real_acceptor_pos ) nodes_all . append ( node ) weights_all . append ( weights [ i ][ j ] ) else : index = nodes_all . index ( node ) weights_all [ index ] = max ( weights [ i ][ j ] , weights_all [ index ] ) p = np . array ( positions_all ). reshape ( - 1 , 3 ) w = np . array ( weights_all ). reshape ( 1 , - 1 ) z = w @ p pos_ind = z / w . sum () return pos_ind","title":"calculate_position"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#frame_analysis","text":"def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ) View Source def frame_analysis ( self , prev_donor : int , acceptor_query : str , time : int ) : self . u . trajectory [ time ] donor = prev_donor transfers = [] list_of_paths = [ [prev_donor ] ] list_of_weights = [ [1 ] ] for depth in range ( self . max_depth ) : for j , path in enumerate ( list_of_paths ) : found = False if depth == len ( path ) - 1 : acceptors = self . u . select_atoms ( f \"(around {self.r_a} index {path[-1]}) and ({acceptor_query})\" ) protons = self . u . select_atoms ( f \"(around {self.r_h} index {path[-1]}) and (name H)\" ) for i , acceptor in enumerate ( acceptors . ix ) : g , proton = self . calculate_g ( path [ -1 ] , acceptor , protons . ix ) if ( g >= self . g_threshold ) and ( acceptor not in path ) : found = True list_of_weights . append ( list_of_weights [ j ] + [ g * list_of_weights[j ][ -1 ] ] ) list_of_paths . append ( path + [ acceptor ] ) if proton > 0 and all ( w >= 0.9 for w in list_of_weights [ j ] ) : donor = acceptor transfers . append (( int ( acceptor ), int ( proton ))) if found : list_of_paths . pop ( j ) list_of_weights . pop ( j ) indicator_position = self . calculate_position ( list_of_paths , list_of_weights ) result = AnalysisResult ( indicator_position = tuple ( indicator_position [ 0 ] ), transfers = transfers ) return donor , result","title":"frame_analysis"},{"location":"reference/ai2_kit/algorithm/proton_transfer/#systeminfo","text":"class SystemInfo ( initial_donor : int , u : MDAnalysis . core . universe . Universe , cell : List [ float ], acceptor_elements : List [ str ] ) SystemInfo(initial_donor: int, u: MDAnalysis.core.universe.Universe, cell: List[float], acceptor_elements: List[str]) View Source class SystemInfo : # Information about the system initial_donor : int u : Universe cell : List [ float ] acceptor_elements : List [ str ]","title":"SystemInfo"},{"location":"reference/ai2_kit/core/","text":"Module ai2_kit.core Sub-modules ai2_kit.core.artifact ai2_kit.core.checkpoint ai2_kit.core.cmd ai2_kit.core.connector ai2_kit.core.executor ai2_kit.core.future ai2_kit.core.job ai2_kit.core.log ai2_kit.core.queue_system ai2_kit.core.resource_manager ai2_kit.core.script ai2_kit.core.util","title":"Index"},{"location":"reference/ai2_kit/core/#module-ai2_kitcore","text":"","title":"Module ai2_kit.core"},{"location":"reference/ai2_kit/core/#sub-modules","text":"ai2_kit.core.artifact ai2_kit.core.checkpoint ai2_kit.core.cmd ai2_kit.core.connector ai2_kit.core.executor ai2_kit.core.future ai2_kit.core.job ai2_kit.core.log ai2_kit.core.queue_system ai2_kit.core.resource_manager ai2_kit.core.script ai2_kit.core.util","title":"Sub-modules"},{"location":"reference/ai2_kit/core/artifact/","text":"Module ai2_kit.core.artifact View Source from pydantic import BaseModel from typing import Optional , Mapping , TypedDict import os import copy def __ArtifactDict (): class ArtifactDict ( TypedDict ): \"\"\" A dict representation of Artifact. Use this when you need to run a remote function call as pydantic model is not pickleable. referrer is not included in this dict as it is not pickleable. \"\"\" url : str attrs : dict executor : Optional [ str ] format : Optional [ str ] includes : Optional [ str ] key : Optional [ str ] return ArtifactDict ArtifactDict = __ArtifactDict () class Artifact ( BaseModel ): key : Optional [ str ] executor : Optional [ str ] url : str format : Optional [ str ] includes : Optional [ str ] attrs : dict = dict () @classmethod def of ( cls , url : str , key : Optional [ str ] = None , executor : Optional [ str ] = None , includes : Optional [ str ] = None , attrs : Optional [ dict ] = None , format : Optional [ str ] = None ,): \"\"\"Create an Artifact instance. Use this instead of __init__ to avoid type error of pydantic\"\"\" return cls ( url = url , executor = executor , format = format , includes = includes , attrs = dict () if attrs is None else copy . deepcopy ( attrs ), # deepcopy to reduce chance of mistake key = key ) def to_dict ( self ) -> ArtifactDict : \"\"\"Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable.\"\"\" return self . dict () # type: ignore def join ( self , * paths , ** kwargs ): url = os . path . join ( self . url , * paths ) return Artifact . of ( url = url , executor = self . executor , ** kwargs ) ArtifactMap = Mapping [ str , Artifact ] Variables ArtifactMap Classes Artifact class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( BaseModel ) : key : Optional [ str ] executor : Optional [ str ] url : str format : Optional [ str ] includes : Optional [ str ] attrs : dict = dict () @classmethod def of ( cls , url : str , key : Optional [ str ] = None , executor : Optional [ str ] = None , includes : Optional [ str ] = None , attrs : Optional [ dict ] = None , format : Optional [ str ] = None ,) : \"\"\"Create an Artifact instance. Use this instead of __init__ to avoid type error of pydantic\"\"\" return cls ( url = url , executor = executor , format = format , includes = includes , attrs = dict () if attrs is None else copy . deepcopy ( attrs ), # deepcopy to reduce chance of mistake key = key ) def to_dict ( self ) -> ArtifactDict : \"\"\"Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable.\"\"\" return self . dict () # type : ignore def join ( self , * paths , ** kwargs ) : url = os . path . join ( self . url , * paths ) return Artifact . of ( url = url , executor = self . executor , ** kwargs ) Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' of def of ( url : str , key : Union [ str , NoneType ] = None , executor : Union [ str , NoneType ] = None , includes : Union [ str , NoneType ] = None , attrs : Union [ dict , NoneType ] = None , format : Union [ str , NoneType ] = None ) Create an Artifact instance. Use this instead of init to avoid type error of pydantic View Source @classmethod def of ( cls , url : str , key : Optional [ str ] = None , executor : Optional [ str ] = None , includes : Optional [ str ] = None , attrs : Optional [ dict ] = None , format : Optional [ str ] = None ,) : \"\"\"Create an Artifact instance. Use this instead of __init__ to avoid type error of pydantic\"\"\" return cls ( url = url , executor = executor , format = format , includes = includes , attrs = dict () if attrs is None else copy . deepcopy ( attrs ), # deepcopy to reduce chance of mistake key = key ) parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. join def join ( self , * paths , ** kwargs ) View Source def join ( self , * paths , ** kwargs ) : url = os . path . join ( self . url , * paths ) return Artifact . of ( url = url , executor = self . executor , ** kwargs ) json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . to_dict def to_dict ( self ) -> ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable. View Source def to_dict ( self ) -> ArtifactDict : \"\"\"Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable.\"\"\" return self . dict () # type : ignore ArtifactDict class ArtifactDict ( / , * args , ** kwargs ) A dict representation of Artifact. Use this when you need to run a remote function call as pydantic model is not pickleable. referrer is not included in this dict as it is not pickleable. View Source class ArtifactDict ( TypedDict ) : \"\"\" A dict representation of Artifact. Use this when you need to run a remote function call as pydantic model is not pickleable. referrer is not included in this dict as it is not pickleable. \"\"\" url : str attrs : dict executor : Optional [ str ] format : Optional [ str ] includes : Optional [ str ] key : Optional [ str ] Ancestors (in MRO) builtins.dict Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Artifact"},{"location":"reference/ai2_kit/core/artifact/#module-ai2_kitcoreartifact","text":"View Source from pydantic import BaseModel from typing import Optional , Mapping , TypedDict import os import copy def __ArtifactDict (): class ArtifactDict ( TypedDict ): \"\"\" A dict representation of Artifact. Use this when you need to run a remote function call as pydantic model is not pickleable. referrer is not included in this dict as it is not pickleable. \"\"\" url : str attrs : dict executor : Optional [ str ] format : Optional [ str ] includes : Optional [ str ] key : Optional [ str ] return ArtifactDict ArtifactDict = __ArtifactDict () class Artifact ( BaseModel ): key : Optional [ str ] executor : Optional [ str ] url : str format : Optional [ str ] includes : Optional [ str ] attrs : dict = dict () @classmethod def of ( cls , url : str , key : Optional [ str ] = None , executor : Optional [ str ] = None , includes : Optional [ str ] = None , attrs : Optional [ dict ] = None , format : Optional [ str ] = None ,): \"\"\"Create an Artifact instance. Use this instead of __init__ to avoid type error of pydantic\"\"\" return cls ( url = url , executor = executor , format = format , includes = includes , attrs = dict () if attrs is None else copy . deepcopy ( attrs ), # deepcopy to reduce chance of mistake key = key ) def to_dict ( self ) -> ArtifactDict : \"\"\"Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable.\"\"\" return self . dict () # type: ignore def join ( self , * paths , ** kwargs ): url = os . path . join ( self . url , * paths ) return Artifact . of ( url = url , executor = self . executor , ** kwargs ) ArtifactMap = Mapping [ str , Artifact ]","title":"Module ai2_kit.core.artifact"},{"location":"reference/ai2_kit/core/artifact/#variables","text":"ArtifactMap","title":"Variables"},{"location":"reference/ai2_kit/core/artifact/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/artifact/#artifact","text":"class Artifact ( __pydantic_self__ , ** data : Any ) View Source class Artifact ( BaseModel ) : key : Optional [ str ] executor : Optional [ str ] url : str format : Optional [ str ] includes : Optional [ str ] attrs : dict = dict () @classmethod def of ( cls , url : str , key : Optional [ str ] = None , executor : Optional [ str ] = None , includes : Optional [ str ] = None , attrs : Optional [ dict ] = None , format : Optional [ str ] = None ,) : \"\"\"Create an Artifact instance. Use this instead of __init__ to avoid type error of pydantic\"\"\" return cls ( url = url , executor = executor , format = format , includes = includes , attrs = dict () if attrs is None else copy . deepcopy ( attrs ), # deepcopy to reduce chance of mistake key = key ) def to_dict ( self ) -> ArtifactDict : \"\"\"Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable.\"\"\" return self . dict () # type : ignore def join ( self , * paths , ** kwargs ) : url = os . path . join ( self . url , * paths ) return Artifact . of ( url = url , executor = self . executor , ** kwargs )","title":"Artifact"},{"location":"reference/ai2_kit/core/artifact/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/artifact/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/core/artifact/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/artifact/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/artifact/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/artifact/#of","text":"def of ( url : str , key : Union [ str , NoneType ] = None , executor : Union [ str , NoneType ] = None , includes : Union [ str , NoneType ] = None , attrs : Union [ dict , NoneType ] = None , format : Union [ str , NoneType ] = None ) Create an Artifact instance. Use this instead of init to avoid type error of pydantic View Source @classmethod def of ( cls , url : str , key : Optional [ str ] = None , executor : Optional [ str ] = None , includes : Optional [ str ] = None , attrs : Optional [ dict ] = None , format : Optional [ str ] = None ,) : \"\"\"Create an Artifact instance. Use this instead of __init__ to avoid type error of pydantic\"\"\" return cls ( url = url , executor = executor , format = format , includes = includes , attrs = dict () if attrs is None else copy . deepcopy ( attrs ), # deepcopy to reduce chance of mistake key = key )","title":"of"},{"location":"reference/ai2_kit/core/artifact/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/artifact/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/artifact/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/artifact/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/artifact/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/artifact/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/artifact/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/artifact/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/artifact/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/artifact/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/artifact/#join","text":"def join ( self , * paths , ** kwargs ) View Source def join ( self , * paths , ** kwargs ) : url = os . path . join ( self . url , * paths ) return Artifact . of ( url = url , executor = self . executor , ** kwargs )","title":"join"},{"location":"reference/ai2_kit/core/artifact/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/artifact/#to_dict","text":"def to_dict ( self ) -> ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable. View Source def to_dict ( self ) -> ArtifactDict : \"\"\"Convert to a dict representation. Use this when you need to run a remote function call as pydantic model is not pickleable.\"\"\" return self . dict () # type : ignore","title":"to_dict"},{"location":"reference/ai2_kit/core/artifact/#artifactdict","text":"class ArtifactDict ( / , * args , ** kwargs ) A dict representation of Artifact. Use this when you need to run a remote function call as pydantic model is not pickleable. referrer is not included in this dict as it is not pickleable. View Source class ArtifactDict ( TypedDict ) : \"\"\" A dict representation of Artifact. Use this when you need to run a remote function call as pydantic model is not pickleable. referrer is not included in this dict as it is not pickleable. \"\"\" url : str attrs : dict executor : Optional [ str ] format : Optional [ str ] includes : Optional [ str ] key : Optional [ str ]","title":"ArtifactDict"},{"location":"reference/ai2_kit/core/artifact/#ancestors-in-mro_1","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/artifact/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/artifact/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/ai2_kit/core/artifact/#copy_1","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/ai2_kit/core/artifact/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/ai2_kit/core/artifact/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/ai2_kit/core/artifact/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/ai2_kit/core/artifact/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/ai2_kit/core/artifact/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/ai2_kit/core/artifact/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/ai2_kit/core/artifact/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/ai2_kit/core/artifact/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/ai2_kit/core/artifact/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/ai2_kit/core/checkpoint/","text":"Module ai2_kit.core.checkpoint View Source from typing import TypeVar , Union , Callable , NamedTuple , Optional from threading import Lock import functools import cloudpickle import os import inspect from .log import get_logger from .util import to_awaitable logger = get_logger ( __name__ ) _lock = Lock () _checkpoint_file : Optional [ str ] = None _checkpoint_data : Optional [ dict ] = None class FnInfo ( NamedTuple ): fn_name : str args : tuple kwargs : dict call_site : str KeyFn = Callable [[ FnInfo ], str ] EMPTY = object () def set_checkpoint_file ( path : str ): global _checkpoint_file if _checkpoint_file is not None : raise RuntimeError ( \"checkpoint path has been set to {} \" . format ( _checkpoint_file )) _checkpoint_file = path _load_checkpoint () def apply_checkpoint ( key_fn : Union [ str , KeyFn ], disable = False ): \"\"\" apply checkpoint for function. Note: This checkpoint implementation doesn't support multiprocess. To support multiple process we need to have a dedicated background process to read/write checkpoint, which will require message queue (e.g. nanomsg or nng) to implement it. Example: >>> set_checkpoint_file('/tmp/test.ckpt') >>> task_fn = lambda a, b: a + b >>> checkpoint('task_1+2')(task_fn)(1, 2) \"\"\" call_site = inspect . getframeinfo ( inspect . stack ()[ 1 ][ 0 ]) T = TypeVar ( 'T' , bound = Callable ) def _checkpoint ( fn : T ) -> T : @functools . wraps ( fn ) def wrapper ( * args , ** kwargs ): fn_info = FnInfo ( fn_name = fn . __name__ , args = args , kwargs = kwargs , call_site = f ' { call_site . filename } : { call_site . lineno } ' , ) key = key_fn if isinstance ( key_fn , str ) else key_fn ( fn_info ) if disable or _checkpoint_file is None : return fn ( * args , ** kwargs ) ret = _get_checkpoint ( key ) if ret is not EMPTY : return ret ret = fn ( * args , ** kwargs ) if inspect . isawaitable ( ret ): async def _wrap_fn (): _ret = await ret _set_checkpoint ( key , _ret , fn_info , True ) return _ret return _wrap_fn () else : _set_checkpoint ( key , ret , fn_info , False ) return ret return wrapper # type: ignore return _checkpoint def _load_checkpoint (): global _checkpoint_data if _checkpoint_data is not None : return assert _checkpoint_file is not None , '_checkpoint_path should not be None!' if os . path . exists ( _checkpoint_file ): with open ( _checkpoint_file , 'rb' ) as f : _checkpoint_data = cloudpickle . load ( f ) else : _checkpoint_data = dict () def _dump_checkpoint (): assert _checkpoint_data is not None , '_checkpoint_data should not be None!' with open ( _checkpoint_file , 'wb' ) as f : # type: ignore cloudpickle . dump ( _checkpoint_data , f ) def _get_checkpoint ( key : str ): try : with _lock : _load_checkpoint () assert _checkpoint_data is not None value = _checkpoint_data . get ( key , None ) if value is None : return EMPTY logger . info ( f \"Hit checkpoint: { key } \" ) if value [ 'is_awaitable' ]: return to_awaitable ( value [ 'return' ]) else : return value [ 'return' ] except Exception as e : logger . error ( f \"Fail to get checkpoint: { key } \" , e ) return EMPTY def _set_checkpoint ( key : str , value , info : FnInfo , is_awaitable : bool = False ): try : with _lock : assert _checkpoint_data is not None # args, kwargs may contain unpickable objects _checkpoint_data [ key ] = { 'return' : value , 'is_awaitable' : is_awaitable , 'info' : { 'fn_name' : info . fn_name , 'call_site' : info . call_site , } } _dump_checkpoint () except Exception as e : logger . error ( 'Fail to set checkpoint' , e ) def del_checkpoint ( key : str ): try : with _lock : _load_checkpoint () assert _checkpoint_data is not None if key in _checkpoint_data : del _checkpoint_data [ key ] _dump_checkpoint () except Exception as e : logger . error ( 'Fail to delete checkpoint' , e ) class CheckpointCmd : \"\"\"checkpoint command line interface\"\"\" def __init__ ( self ) -> None : ... def load ( self , file ): set_checkpoint_file ( file ) return self def ls ( self ): '''list all the checkpoint entries in the checkpoint file''' assert _checkpoint_data is not None for i , ( key , value ) in enumerate ( _checkpoint_data . items ()): print ( ' \\n ' . join ([ '=' * 80 , f 'Key: \\t { key } ' , f 'Call Site: \\t { value [ \"info\" ][ \"call_site\" ] } ' , f 'Function: \\t { value [ \"info\" ][ \"fn_name\" ] } ' , ])) print ( '=' * 80 ) def rm ( self , prefix : str ): \"\"\"remove checkpoint entries with the given prefix\"\"\" assert _checkpoint_data is not None for key in list ( _checkpoint_data . keys ()): if key . startswith ( prefix ): logger . info ( f \"Remove checkpoint: { key } \" ) del _checkpoint_data [ key ] _dump_checkpoint () Variables EMPTY KeyFn logger Functions apply_checkpoint def apply_checkpoint ( key_fn : Union [ str , Callable [[ ai2_kit . core . checkpoint . FnInfo ], str ]], disable = False ) apply checkpoint for function. Note: This checkpoint implementation doesn't support multiprocess. To support multiple process we need to have a dedicated background process to read/write checkpoint, which will require message queue (e.g. nanomsg or nng) to implement it. View Source def apply_checkpoint ( key_fn : Union [ str , KeyFn ], disable = False ): \"\"\" apply checkpoint for function. Note: This checkpoint implementation doesn't support multiprocess. To support multiple process we need to have a dedicated background process to read/write checkpoint, which will require message queue (e.g. nanomsg or nng) to implement it. Example: >>> set_checkpoint_file('/tmp/test.ckpt') >>> task_fn = lambda a, b: a + b >>> checkpoint('task_1+2')(task_fn)(1, 2) \"\"\" call_site = inspect . getframeinfo ( inspect . stack ()[ 1 ][ 0 ]) T = TypeVar ( 'T' , bound = Callable ) def _checkpoint ( fn : T ) -> T : @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ): fn_info = FnInfo ( fn_name = fn . __name__ , args = args , kwargs = kwargs , call_site = f ' { call_site . filename }:{ call_site . lineno } ' , ) key = key_fn if isinstance ( key_fn , str ) else key_fn ( fn_info ) if disable or _checkpoint_file is None : return fn ( * args , ** kwargs ) ret = _get_checkpoint ( key ) if ret is not EMPTY : return ret ret = fn ( * args , ** kwargs ) if inspect . isawaitable ( ret ): async def _wrap_fn (): _ret = await ret _set_checkpoint ( key , _ret , fn_info , True ) return _ret return _wrap_fn () else : _set_checkpoint ( key , ret , fn_info , False ) return ret return wrapper # type : ignore return _checkpoint del_checkpoint def del_checkpoint ( key : str ) View Source def del_checkpoint ( key : str ) : try : with _lock : _load_checkpoint () assert _checkpoint_data is not None if key in _checkpoint_data : del _checkpoint_data [ key ] _dump_checkpoint () except Exception as e : logger . error ( 'Fail to delete checkpoint' , e ) set_checkpoint_file def set_checkpoint_file ( path : str ) View Source def set_checkpoint_file ( path : str ): global _checkpoint_file if _checkpoint_file is not None : raise RuntimeError ( \"checkpoint path has been set to {}\" . format ( _checkpoint_file )) _checkpoint_file = path _load_checkpoint () Classes CheckpointCmd class CheckpointCmd ( ) checkpoint command line interface View Source class CheckpointCmd : \"\"\"checkpoint command line interface\"\"\" def __init__ ( self ) -> None : ... def load ( self , file ): set_checkpoint_file ( file ) return self def ls ( self ): '''list all the checkpoint entries in the checkpoint file''' assert _checkpoint_data is not None for i , ( key , value ) in enumerate ( _checkpoint_data . items ()): print ( ' \\n ' . join ([ '=' * 80 , f 'Key: \\t {key}' , f 'Call Site: \\t {value[\"info\"][\"call_site\"]}' , f 'Function: \\t {value[\"info\"][\"fn_name\"]}' , ])) print ( '=' * 80 ) def rm ( self , prefix : str ): \"\"\"remove checkpoint entries with the given prefix\"\"\" assert _checkpoint_data is not None for key in list ( _checkpoint_data . keys ()): if key . startswith ( prefix ): logger . info ( f \"Remove checkpoint: {key}\" ) del _checkpoint_data [ key ] _dump_checkpoint () Methods load def load ( self , file ) View Source def load ( self , file ): set_checkpoint_file ( file ) return self ls def ls ( self ) list all the checkpoint entries in the checkpoint file View Source def ls ( self ): '''list all the checkpoint entries in the checkpoint file''' assert _checkpoint_data is not None for i , ( key , value ) in enumerate ( _checkpoint_data . items ()): print ( ' \\n ' . join ([ '=' * 80 , f 'Key: \\t {key}' , f 'Call Site: \\t {value[\"info\"][\"call_site\"]}' , f 'Function: \\t {value[\"info\"][\"fn_name\"]}' , ])) print ( '=' * 80 ) rm def rm ( self , prefix : str ) remove checkpoint entries with the given prefix View Source def rm ( self , prefix : str ) : \"\"\"remove checkpoint entries with the given prefix\"\"\" assert _checkpoint_data is not None for key in list ( _checkpoint_data . keys ()) : if key . startswith ( prefix ) : logger . info ( f \"Remove checkpoint: {key}\" ) del _checkpoint_data [ key ] _dump_checkpoint () FnInfo class FnInfo ( / , * args , ** kwargs ) FnInfo(fn_name, args, kwargs, call_site) View Source class FnInfo ( NamedTuple ): fn_name: str args: tuple kwargs: dict call_site: str Ancestors (in MRO) builtins.tuple Class variables args call_site fn_name kwargs Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"Checkpoint"},{"location":"reference/ai2_kit/core/checkpoint/#module-ai2_kitcorecheckpoint","text":"View Source from typing import TypeVar , Union , Callable , NamedTuple , Optional from threading import Lock import functools import cloudpickle import os import inspect from .log import get_logger from .util import to_awaitable logger = get_logger ( __name__ ) _lock = Lock () _checkpoint_file : Optional [ str ] = None _checkpoint_data : Optional [ dict ] = None class FnInfo ( NamedTuple ): fn_name : str args : tuple kwargs : dict call_site : str KeyFn = Callable [[ FnInfo ], str ] EMPTY = object () def set_checkpoint_file ( path : str ): global _checkpoint_file if _checkpoint_file is not None : raise RuntimeError ( \"checkpoint path has been set to {} \" . format ( _checkpoint_file )) _checkpoint_file = path _load_checkpoint () def apply_checkpoint ( key_fn : Union [ str , KeyFn ], disable = False ): \"\"\" apply checkpoint for function. Note: This checkpoint implementation doesn't support multiprocess. To support multiple process we need to have a dedicated background process to read/write checkpoint, which will require message queue (e.g. nanomsg or nng) to implement it. Example: >>> set_checkpoint_file('/tmp/test.ckpt') >>> task_fn = lambda a, b: a + b >>> checkpoint('task_1+2')(task_fn)(1, 2) \"\"\" call_site = inspect . getframeinfo ( inspect . stack ()[ 1 ][ 0 ]) T = TypeVar ( 'T' , bound = Callable ) def _checkpoint ( fn : T ) -> T : @functools . wraps ( fn ) def wrapper ( * args , ** kwargs ): fn_info = FnInfo ( fn_name = fn . __name__ , args = args , kwargs = kwargs , call_site = f ' { call_site . filename } : { call_site . lineno } ' , ) key = key_fn if isinstance ( key_fn , str ) else key_fn ( fn_info ) if disable or _checkpoint_file is None : return fn ( * args , ** kwargs ) ret = _get_checkpoint ( key ) if ret is not EMPTY : return ret ret = fn ( * args , ** kwargs ) if inspect . isawaitable ( ret ): async def _wrap_fn (): _ret = await ret _set_checkpoint ( key , _ret , fn_info , True ) return _ret return _wrap_fn () else : _set_checkpoint ( key , ret , fn_info , False ) return ret return wrapper # type: ignore return _checkpoint def _load_checkpoint (): global _checkpoint_data if _checkpoint_data is not None : return assert _checkpoint_file is not None , '_checkpoint_path should not be None!' if os . path . exists ( _checkpoint_file ): with open ( _checkpoint_file , 'rb' ) as f : _checkpoint_data = cloudpickle . load ( f ) else : _checkpoint_data = dict () def _dump_checkpoint (): assert _checkpoint_data is not None , '_checkpoint_data should not be None!' with open ( _checkpoint_file , 'wb' ) as f : # type: ignore cloudpickle . dump ( _checkpoint_data , f ) def _get_checkpoint ( key : str ): try : with _lock : _load_checkpoint () assert _checkpoint_data is not None value = _checkpoint_data . get ( key , None ) if value is None : return EMPTY logger . info ( f \"Hit checkpoint: { key } \" ) if value [ 'is_awaitable' ]: return to_awaitable ( value [ 'return' ]) else : return value [ 'return' ] except Exception as e : logger . error ( f \"Fail to get checkpoint: { key } \" , e ) return EMPTY def _set_checkpoint ( key : str , value , info : FnInfo , is_awaitable : bool = False ): try : with _lock : assert _checkpoint_data is not None # args, kwargs may contain unpickable objects _checkpoint_data [ key ] = { 'return' : value , 'is_awaitable' : is_awaitable , 'info' : { 'fn_name' : info . fn_name , 'call_site' : info . call_site , } } _dump_checkpoint () except Exception as e : logger . error ( 'Fail to set checkpoint' , e ) def del_checkpoint ( key : str ): try : with _lock : _load_checkpoint () assert _checkpoint_data is not None if key in _checkpoint_data : del _checkpoint_data [ key ] _dump_checkpoint () except Exception as e : logger . error ( 'Fail to delete checkpoint' , e ) class CheckpointCmd : \"\"\"checkpoint command line interface\"\"\" def __init__ ( self ) -> None : ... def load ( self , file ): set_checkpoint_file ( file ) return self def ls ( self ): '''list all the checkpoint entries in the checkpoint file''' assert _checkpoint_data is not None for i , ( key , value ) in enumerate ( _checkpoint_data . items ()): print ( ' \\n ' . join ([ '=' * 80 , f 'Key: \\t { key } ' , f 'Call Site: \\t { value [ \"info\" ][ \"call_site\" ] } ' , f 'Function: \\t { value [ \"info\" ][ \"fn_name\" ] } ' , ])) print ( '=' * 80 ) def rm ( self , prefix : str ): \"\"\"remove checkpoint entries with the given prefix\"\"\" assert _checkpoint_data is not None for key in list ( _checkpoint_data . keys ()): if key . startswith ( prefix ): logger . info ( f \"Remove checkpoint: { key } \" ) del _checkpoint_data [ key ] _dump_checkpoint ()","title":"Module ai2_kit.core.checkpoint"},{"location":"reference/ai2_kit/core/checkpoint/#variables","text":"EMPTY KeyFn logger","title":"Variables"},{"location":"reference/ai2_kit/core/checkpoint/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/checkpoint/#apply_checkpoint","text":"def apply_checkpoint ( key_fn : Union [ str , Callable [[ ai2_kit . core . checkpoint . FnInfo ], str ]], disable = False ) apply checkpoint for function. Note: This checkpoint implementation doesn't support multiprocess. To support multiple process we need to have a dedicated background process to read/write checkpoint, which will require message queue (e.g. nanomsg or nng) to implement it. View Source def apply_checkpoint ( key_fn : Union [ str , KeyFn ], disable = False ): \"\"\" apply checkpoint for function. Note: This checkpoint implementation doesn't support multiprocess. To support multiple process we need to have a dedicated background process to read/write checkpoint, which will require message queue (e.g. nanomsg or nng) to implement it. Example: >>> set_checkpoint_file('/tmp/test.ckpt') >>> task_fn = lambda a, b: a + b >>> checkpoint('task_1+2')(task_fn)(1, 2) \"\"\" call_site = inspect . getframeinfo ( inspect . stack ()[ 1 ][ 0 ]) T = TypeVar ( 'T' , bound = Callable ) def _checkpoint ( fn : T ) -> T : @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ): fn_info = FnInfo ( fn_name = fn . __name__ , args = args , kwargs = kwargs , call_site = f ' { call_site . filename }:{ call_site . lineno } ' , ) key = key_fn if isinstance ( key_fn , str ) else key_fn ( fn_info ) if disable or _checkpoint_file is None : return fn ( * args , ** kwargs ) ret = _get_checkpoint ( key ) if ret is not EMPTY : return ret ret = fn ( * args , ** kwargs ) if inspect . isawaitable ( ret ): async def _wrap_fn (): _ret = await ret _set_checkpoint ( key , _ret , fn_info , True ) return _ret return _wrap_fn () else : _set_checkpoint ( key , ret , fn_info , False ) return ret return wrapper # type : ignore return _checkpoint","title":"apply_checkpoint"},{"location":"reference/ai2_kit/core/checkpoint/#del_checkpoint","text":"def del_checkpoint ( key : str ) View Source def del_checkpoint ( key : str ) : try : with _lock : _load_checkpoint () assert _checkpoint_data is not None if key in _checkpoint_data : del _checkpoint_data [ key ] _dump_checkpoint () except Exception as e : logger . error ( 'Fail to delete checkpoint' , e )","title":"del_checkpoint"},{"location":"reference/ai2_kit/core/checkpoint/#set_checkpoint_file","text":"def set_checkpoint_file ( path : str ) View Source def set_checkpoint_file ( path : str ): global _checkpoint_file if _checkpoint_file is not None : raise RuntimeError ( \"checkpoint path has been set to {}\" . format ( _checkpoint_file )) _checkpoint_file = path _load_checkpoint ()","title":"set_checkpoint_file"},{"location":"reference/ai2_kit/core/checkpoint/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/checkpoint/#checkpointcmd","text":"class CheckpointCmd ( ) checkpoint command line interface View Source class CheckpointCmd : \"\"\"checkpoint command line interface\"\"\" def __init__ ( self ) -> None : ... def load ( self , file ): set_checkpoint_file ( file ) return self def ls ( self ): '''list all the checkpoint entries in the checkpoint file''' assert _checkpoint_data is not None for i , ( key , value ) in enumerate ( _checkpoint_data . items ()): print ( ' \\n ' . join ([ '=' * 80 , f 'Key: \\t {key}' , f 'Call Site: \\t {value[\"info\"][\"call_site\"]}' , f 'Function: \\t {value[\"info\"][\"fn_name\"]}' , ])) print ( '=' * 80 ) def rm ( self , prefix : str ): \"\"\"remove checkpoint entries with the given prefix\"\"\" assert _checkpoint_data is not None for key in list ( _checkpoint_data . keys ()): if key . startswith ( prefix ): logger . info ( f \"Remove checkpoint: {key}\" ) del _checkpoint_data [ key ] _dump_checkpoint ()","title":"CheckpointCmd"},{"location":"reference/ai2_kit/core/checkpoint/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/checkpoint/#load","text":"def load ( self , file ) View Source def load ( self , file ): set_checkpoint_file ( file ) return self","title":"load"},{"location":"reference/ai2_kit/core/checkpoint/#ls","text":"def ls ( self ) list all the checkpoint entries in the checkpoint file View Source def ls ( self ): '''list all the checkpoint entries in the checkpoint file''' assert _checkpoint_data is not None for i , ( key , value ) in enumerate ( _checkpoint_data . items ()): print ( ' \\n ' . join ([ '=' * 80 , f 'Key: \\t {key}' , f 'Call Site: \\t {value[\"info\"][\"call_site\"]}' , f 'Function: \\t {value[\"info\"][\"fn_name\"]}' , ])) print ( '=' * 80 )","title":"ls"},{"location":"reference/ai2_kit/core/checkpoint/#rm","text":"def rm ( self , prefix : str ) remove checkpoint entries with the given prefix View Source def rm ( self , prefix : str ) : \"\"\"remove checkpoint entries with the given prefix\"\"\" assert _checkpoint_data is not None for key in list ( _checkpoint_data . keys ()) : if key . startswith ( prefix ) : logger . info ( f \"Remove checkpoint: {key}\" ) del _checkpoint_data [ key ] _dump_checkpoint ()","title":"rm"},{"location":"reference/ai2_kit/core/checkpoint/#fninfo","text":"class FnInfo ( / , * args , ** kwargs ) FnInfo(fn_name, args, kwargs, call_site) View Source class FnInfo ( NamedTuple ): fn_name: str args: tuple kwargs: dict call_site: str","title":"FnInfo"},{"location":"reference/ai2_kit/core/checkpoint/#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/checkpoint/#class-variables","text":"args call_site fn_name kwargs","title":"Class variables"},{"location":"reference/ai2_kit/core/checkpoint/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/checkpoint/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/ai2_kit/core/checkpoint/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/ai2_kit/core/cmd/","text":"Module ai2_kit.core.cmd View Source class CmdGroup: \"\"\"A group of commands\"\"\" def __init__ ( self , items: dict , doc: str = '' ) -> None: self . __doc__ = doc self . __dict__ . update ( items ) Classes CmdGroup class CmdGroup ( items : dict , doc : str = '' ) A group of commands View Source class CmdGroup: \"\"\"A group of commands\"\"\" def __init__ ( self , items: dict , doc: str = '' ) -> None: self . __doc__ = doc self . __dict__ . update ( items )","title":"Cmd"},{"location":"reference/ai2_kit/core/cmd/#module-ai2_kitcorecmd","text":"View Source class CmdGroup: \"\"\"A group of commands\"\"\" def __init__ ( self , items: dict , doc: str = '' ) -> None: self . __doc__ = doc self . __dict__ . update ( items )","title":"Module ai2_kit.core.cmd"},{"location":"reference/ai2_kit/core/cmd/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/cmd/#cmdgroup","text":"class CmdGroup ( items : dict , doc : str = '' ) A group of commands View Source class CmdGroup: \"\"\"A group of commands\"\"\" def __init__ ( self , items: dict , doc: str = '' ) -> None: self . __doc__ = doc self . __dict__ . update ( items )","title":"CmdGroup"},{"location":"reference/ai2_kit/core/connector/","text":"Module ai2_kit.core.connector View Source from fabric import Connection , Result from pydantic import BaseModel from typing import Optional , List from abc import ABC , abstractmethod from io import StringIO import numpy as np import shlex import invoke import os import stat import json import glob class SshConfig ( BaseModel ): host : str gateway : Optional [ 'SshConfig' ] class BaseConnector ( ABC ): @abstractmethod def dump_text ( self , text : str , path : str ): ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ]: ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def upload ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def download ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def sym_link ( self , from_path : str , to_dir : str ) -> str : ... class SshConnector ( BaseConnector ): @classmethod def from_config ( cls , config : SshConfig ): connection = Connection ( host = config . host ) next_config = config . gateway next_connection = connection while next_config is not None : next_connection . gateway = Connection ( host = next_config . host ) next_connection = next_connection . gateway next_config = next_config . gateway return cls ( connection ) def __init__ ( self , connection : Connection ): self . _connection = connection def dump_text ( self , text : str , path : str ): f = StringIO ( text ) self . put ( f , path ) def glob ( self , pattern : str ): python_script = 'from glob import glob; from json import dumps; print(dumps(glob( {} )))' . format ( repr ( pattern )) cmd = 'python -c {} ' . format ( shlex . quote ( python_script )) result = self . run ( cmd , hide = True ) return json . loads ( result . stdout ) def run ( self , script , ** kwargs ) -> Result : return self . _connection . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) return to_path def download ( self , from_path : str , to_dir : str ) -> str : sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if stat . S_ISDIR ( sftp . lstat ( from_path ) . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) return to_path def put ( self , * args , ** kwargs ): return self . _connection . put ( * args , ** kwargs ) def get ( self , * args , ** kwargs ): return self . _connection . get ( * args , ** kwargs ) def mkdir ( self , dir_path : str ): self . _connection . run ( 'mkdir -p {} ' . format ( shlex . quote ( dir_path ))) def put_dir ( self , from_dir : str , to_dir : str ): self . mkdir ( to_dir ) for item in os . listdir ( from_dir ): from_path = os . path . join ( from_dir , item ) to_path = os . path . join ( to_dir , item ) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) def get_dir ( self , from_dir : str , to_dir : str ): sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) for item in sftp . listdir_attr ( from_dir ): from_path = os . path . join ( from_dir , item . filename ) to_path = os . path . join ( to_dir , item . filename ) if stat . S_ISDIR ( item . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) def get_sftp ( self ): return self . _connection . sftp () class LocalConnector ( BaseConnector ): def dump_text ( self , text : str , path : str ): with open ( path , 'w' ) as f : f . write ( text ) def glob ( self , pattern : str ): return glob . glob ( pattern ) def run ( self , script , ** kwargs ): return invoke . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : os . makedirs ( to_dir , exist_ok = True ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) def download ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) def get_ln_cmd ( from_path : str , to_path : str ): \"\"\" The reason to `rm -d` to_path is to workaround the limit of ln. `ln` command cannot override existed directory, so we need to ensure to_path is not existed. Here we use -d option instead of -rf to avoid remove directory with content. The error of `rm -d` is suppressed as it will fail when to_path is file. `-T` option of `ln` is used to avoid some unexpected result. \"\"\" to_path = os . path . normpath ( to_path ) return 'rm -d {to_path} || true && ln -sfT {from_path} {to_path} ' . format ( from_path = shlex . quote ( from_path ), to_path = shlex . quote ( to_path ) ) def safe_basename ( path : str , default = '' ): \"\"\" Ensure return valid file name as basename \"\"\" basename = os . path . basename ( path ) if basename in ( '/' , '.' , '..' , '' ): return default return basename Functions get_ln_cmd def get_ln_cmd ( from_path : str , to_path : str ) The reason to rm -d to_path is to workaround the limit of ln. ln command cannot override existed directory, so we need to ensure to_path is not existed. Here we use -d option instead of -rf to avoid remove directory with content. The error of rm -d is suppressed as it will fail when to_path is file. -T option of ln is used to avoid some unexpected result. View Source def get_ln_cmd ( from_path : str , to_path : str ) : \" \"\" The reason to `rm -d` to_path is to workaround the limit of ln. `ln` command cannot override existed directory, so we need to ensure to_path is not existed. Here we use -d option instead of -rf to avoid remove directory with content. The error of `rm -d` is suppressed as it will fail when to_path is file. `-T` option of `ln` is used to avoid some unexpected result. \"\" \" to_path = os . path . normpath ( to_path ) return 'rm -d {to_path} || true && ln -sfT {from_path} {to_path}' . format ( from_path = shlex . quote ( from_path ), to_path = shlex . quote ( to_path ) ) safe_basename def safe_basename ( path : str , default = '' ) Ensure return valid file name as basename View Source def safe_basename ( path : str , default = '' ) : \"\" \" Ensure return valid file name as basename \"\" \" basename = os.path.basename(path) if basename in ('/', '.', '..', ''): return default return basename Classes BaseConnector class BaseConnector ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class BaseConnector ( ABC ) : @abstractmethod def dump_text ( self , text : str , path : str ) : ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def upload ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def download ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def sym_link ( self , from_path : str , to_dir : str ) -> str : ... Ancestors (in MRO) abc.ABC Descendants ai2_kit.core.connector.SshConnector ai2_kit.core.connector.LocalConnector Methods download def download ( self , from_path : str , to_dir : str ) -> str View Source @ abstractmethod def download ( self , from_path : str , to_dir : str ) -> str : ... dump_text def dump_text ( self , text : str , path : str ) View Source @abstractmethod def dump_text ( self , text : str , path : str ) : ... glob def glob ( self , pattern : str ) -> List [ str ] View Source @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ... run def run ( self , script : str , ** kwargs ) -> fabric . runners . Result View Source @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... sym_link def sym_link ( self , from_path : str , to_dir : str ) -> str View Source @abstractmethod def sym_link ( self , from_path : str , to_dir : str ) -> str : ... upload def upload ( self , from_path : str , to_dir : str ) -> str View Source @ abstractmethod def upload ( self , from_path : str , to_dir : str ) -> str : ... LocalConnector class LocalConnector ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LocalConnector ( BaseConnector ): def dump_text ( self , text : str , path : str ): with open ( path , 'w' ) as f : f . write ( text ) def glob ( self , pattern : str ): return glob . glob ( pattern ) def run ( self , script , ** kwargs ): return invoke . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : os . makedirs ( to_dir , exist_ok = True ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) def download ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) Ancestors (in MRO) ai2_kit.core.connector.BaseConnector abc.ABC Methods download def download ( self , from_path : str , to_dir : str ) -> str View Source def download ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) dump_text def dump_text ( self , text : str , path : str ) View Source def dump_text(self, text: str, path: str): with open(path, 'w') as f: f.write(text) glob def glob ( self , pattern : str ) View Source def glob ( self , pattern : str ) : return glob . glob ( pattern ) run def run ( self , script , ** kwargs ) View Source def run ( self , script , ** kwargs ) : return invoke . run ( script , ** kwargs ) sym_link def sym_link ( self , from_path : str , to_dir : str ) -> str View Source def sym_link ( self , from_path : str , to_dir : str ) -> str : os . makedirs ( to_dir , exist_ok = True ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path upload def upload ( self , from_path : str , to_dir : str ) -> str View Source def upload ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) SshConfig class SshConfig ( __pydantic_self__ , ** data : Any ) View Source class SshConfig ( BaseModel ): host: str gateway: Optional [ 'SshConfig' ] Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . SshConnector class SshConnector ( connection : fabric . connection . Connection ) Helper class that provides a standard way to create an ABC using inheritance. View Source class SshConnector ( BaseConnector ): @classmethod def from_config ( cls , config : SshConfig ): connection = Connection ( host = config . host ) next_config = config . gateway next_connection = connection while next_config is not None : next_connection . gateway = Connection ( host = next_config . host ) next_connection = next_connection . gateway next_config = next_config . gateway return cls ( connection ) def __init__ ( self , connection : Connection ): self . _connection = connection def dump_text ( self , text : str , path : str ): f = StringIO ( text ) self . put ( f , path ) def glob ( self , pattern : str ): python_script = 'from glob import glob; from json import dumps; print(dumps(glob( {} )))' . format ( repr ( pattern )) cmd = 'python -c {} ' . format ( shlex . quote ( python_script )) result = self . run ( cmd , hide = True ) return json . loads ( result . stdout ) def run ( self , script , ** kwargs ) -> Result : return self . _connection . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) return to_path def download ( self , from_path : str , to_dir : str ) -> str : sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if stat . S_ISDIR ( sftp . lstat ( from_path ) . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) return to_path def put ( self , * args , ** kwargs ): return self . _connection . put ( * args , ** kwargs ) def get ( self , * args , ** kwargs ): return self . _connection . get ( * args , ** kwargs ) def mkdir ( self , dir_path : str ): self . _connection . run ( 'mkdir -p {} ' . format ( shlex . quote ( dir_path ))) def put_dir ( self , from_dir : str , to_dir : str ): self . mkdir ( to_dir ) for item in os . listdir ( from_dir ): from_path = os . path . join ( from_dir , item ) to_path = os . path . join ( to_dir , item ) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) def get_dir ( self , from_dir : str , to_dir : str ): sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) for item in sftp . listdir_attr ( from_dir ): from_path = os . path . join ( from_dir , item . filename ) to_path = os . path . join ( to_dir , item . filename ) if stat . S_ISDIR ( item . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) def get_sftp ( self ): return self . _connection . sftp () Ancestors (in MRO) ai2_kit.core.connector.BaseConnector abc.ABC Static methods from_config def from_config ( config : ai2_kit . core . connector . SshConfig ) View Source @classmethod def from_config ( cls , config : SshConfig ) : connection = Connection ( host = config . host ) next_config = config . gateway next_connection = connection while next_config is not None : next_connection . gateway = Connection ( host = next_config . host ) next_connection = next_connection . gateway next_config = next_config . gateway return cls ( connection ) Methods download def download ( self , from_path : str , to_dir : str ) -> str View Source def download ( self , from_path : str , to_dir : str ) -> str : sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if stat . S_ISDIR ( sftp . lstat ( from_path ) . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) return to_path dump_text def dump_text ( self , text : str , path : str ) View Source def dump_text(self, text: str, path: str): f = StringIO(text) self.put(f, path) get def get ( self , * args , ** kwargs ) View Source def get ( self , * args , ** kwargs ) : return self . _connection . get ( * args , ** kwargs ) get_dir def get_dir ( self , from_dir : str , to_dir : str ) View Source def get_dir ( self , from_dir : str , to_dir : str ): sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) for item in sftp . listdir_attr ( from_dir ): from_path = os . path . join ( from_dir , item . filename ) to_path = os . path . join ( to_dir , item . filename ) if stat . S_ISDIR ( item . st_mode ): # type : ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) get_sftp def get_sftp ( self ) View Source def get_sftp ( self ) : return self . _connection . sftp () glob def glob ( self , pattern : str ) View Source def glob ( self , pattern : str ): python_script = 'from glob import glob; from json import dumps; print(dumps(glob( {} )))' . format ( repr ( pattern )) cmd = 'python -c {} ' . format ( shlex . quote ( python_script )) result = self . run ( cmd , hide = True ) return json . loads ( result . stdout ) mkdir def mkdir ( self , dir_path : str ) View Source def mkdir(self, dir_path: str): self._connection.run('mkdir -p {}'.format(shlex.quote(dir_path))) put def put ( self , * args , ** kwargs ) View Source def put ( self , * args , ** kwargs ) : return self . _connection . put ( * args , ** kwargs ) put_dir def put_dir ( self , from_dir : str , to_dir : str ) View Source def put_dir ( self , from_dir : str , to_dir : str ) : self . mkdir ( to_dir ) for item in os . listdir ( from_dir ) : from_path = os . path . join ( from_dir , item ) to_path = os . path . join ( to_dir , item ) if os . path . isdir ( from_path ) : self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) run def run ( self , script , ** kwargs ) -> fabric . runners . Result View Source def run ( self , script , ** kwargs ) -> Result : return self . _connection . run ( script , ** kwargs ) sym_link def sym_link ( self , from_path : str , to_dir : str ) -> str View Source def sym_link ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path upload def upload ( self , from_path : str , to_dir : str ) -> str View Source def upload ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) return to_path","title":"Connector"},{"location":"reference/ai2_kit/core/connector/#module-ai2_kitcoreconnector","text":"View Source from fabric import Connection , Result from pydantic import BaseModel from typing import Optional , List from abc import ABC , abstractmethod from io import StringIO import numpy as np import shlex import invoke import os import stat import json import glob class SshConfig ( BaseModel ): host : str gateway : Optional [ 'SshConfig' ] class BaseConnector ( ABC ): @abstractmethod def dump_text ( self , text : str , path : str ): ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ]: ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def upload ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def download ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def sym_link ( self , from_path : str , to_dir : str ) -> str : ... class SshConnector ( BaseConnector ): @classmethod def from_config ( cls , config : SshConfig ): connection = Connection ( host = config . host ) next_config = config . gateway next_connection = connection while next_config is not None : next_connection . gateway = Connection ( host = next_config . host ) next_connection = next_connection . gateway next_config = next_config . gateway return cls ( connection ) def __init__ ( self , connection : Connection ): self . _connection = connection def dump_text ( self , text : str , path : str ): f = StringIO ( text ) self . put ( f , path ) def glob ( self , pattern : str ): python_script = 'from glob import glob; from json import dumps; print(dumps(glob( {} )))' . format ( repr ( pattern )) cmd = 'python -c {} ' . format ( shlex . quote ( python_script )) result = self . run ( cmd , hide = True ) return json . loads ( result . stdout ) def run ( self , script , ** kwargs ) -> Result : return self . _connection . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) return to_path def download ( self , from_path : str , to_dir : str ) -> str : sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if stat . S_ISDIR ( sftp . lstat ( from_path ) . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) return to_path def put ( self , * args , ** kwargs ): return self . _connection . put ( * args , ** kwargs ) def get ( self , * args , ** kwargs ): return self . _connection . get ( * args , ** kwargs ) def mkdir ( self , dir_path : str ): self . _connection . run ( 'mkdir -p {} ' . format ( shlex . quote ( dir_path ))) def put_dir ( self , from_dir : str , to_dir : str ): self . mkdir ( to_dir ) for item in os . listdir ( from_dir ): from_path = os . path . join ( from_dir , item ) to_path = os . path . join ( to_dir , item ) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) def get_dir ( self , from_dir : str , to_dir : str ): sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) for item in sftp . listdir_attr ( from_dir ): from_path = os . path . join ( from_dir , item . filename ) to_path = os . path . join ( to_dir , item . filename ) if stat . S_ISDIR ( item . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) def get_sftp ( self ): return self . _connection . sftp () class LocalConnector ( BaseConnector ): def dump_text ( self , text : str , path : str ): with open ( path , 'w' ) as f : f . write ( text ) def glob ( self , pattern : str ): return glob . glob ( pattern ) def run ( self , script , ** kwargs ): return invoke . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : os . makedirs ( to_dir , exist_ok = True ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) def download ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) def get_ln_cmd ( from_path : str , to_path : str ): \"\"\" The reason to `rm -d` to_path is to workaround the limit of ln. `ln` command cannot override existed directory, so we need to ensure to_path is not existed. Here we use -d option instead of -rf to avoid remove directory with content. The error of `rm -d` is suppressed as it will fail when to_path is file. `-T` option of `ln` is used to avoid some unexpected result. \"\"\" to_path = os . path . normpath ( to_path ) return 'rm -d {to_path} || true && ln -sfT {from_path} {to_path} ' . format ( from_path = shlex . quote ( from_path ), to_path = shlex . quote ( to_path ) ) def safe_basename ( path : str , default = '' ): \"\"\" Ensure return valid file name as basename \"\"\" basename = os . path . basename ( path ) if basename in ( '/' , '.' , '..' , '' ): return default return basename","title":"Module ai2_kit.core.connector"},{"location":"reference/ai2_kit/core/connector/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/connector/#get_ln_cmd","text":"def get_ln_cmd ( from_path : str , to_path : str ) The reason to rm -d to_path is to workaround the limit of ln. ln command cannot override existed directory, so we need to ensure to_path is not existed. Here we use -d option instead of -rf to avoid remove directory with content. The error of rm -d is suppressed as it will fail when to_path is file. -T option of ln is used to avoid some unexpected result. View Source def get_ln_cmd ( from_path : str , to_path : str ) : \" \"\" The reason to `rm -d` to_path is to workaround the limit of ln. `ln` command cannot override existed directory, so we need to ensure to_path is not existed. Here we use -d option instead of -rf to avoid remove directory with content. The error of `rm -d` is suppressed as it will fail when to_path is file. `-T` option of `ln` is used to avoid some unexpected result. \"\" \" to_path = os . path . normpath ( to_path ) return 'rm -d {to_path} || true && ln -sfT {from_path} {to_path}' . format ( from_path = shlex . quote ( from_path ), to_path = shlex . quote ( to_path ) )","title":"get_ln_cmd"},{"location":"reference/ai2_kit/core/connector/#safe_basename","text":"def safe_basename ( path : str , default = '' ) Ensure return valid file name as basename View Source def safe_basename ( path : str , default = '' ) : \"\" \" Ensure return valid file name as basename \"\" \" basename = os.path.basename(path) if basename in ('/', '.', '..', ''): return default return basename","title":"safe_basename"},{"location":"reference/ai2_kit/core/connector/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/connector/#baseconnector","text":"class BaseConnector ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class BaseConnector ( ABC ) : @abstractmethod def dump_text ( self , text : str , path : str ) : ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def upload ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def download ( self , from_path : str , to_dir : str ) -> str : ... @abstractmethod def sym_link ( self , from_path : str , to_dir : str ) -> str : ...","title":"BaseConnector"},{"location":"reference/ai2_kit/core/connector/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/connector/#descendants","text":"ai2_kit.core.connector.SshConnector ai2_kit.core.connector.LocalConnector","title":"Descendants"},{"location":"reference/ai2_kit/core/connector/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/connector/#download","text":"def download ( self , from_path : str , to_dir : str ) -> str View Source @ abstractmethod def download ( self , from_path : str , to_dir : str ) -> str : ...","title":"download"},{"location":"reference/ai2_kit/core/connector/#dump_text","text":"def dump_text ( self , text : str , path : str ) View Source @abstractmethod def dump_text ( self , text : str , path : str ) : ...","title":"dump_text"},{"location":"reference/ai2_kit/core/connector/#glob","text":"def glob ( self , pattern : str ) -> List [ str ] View Source @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ...","title":"glob"},{"location":"reference/ai2_kit/core/connector/#run","text":"def run ( self , script : str , ** kwargs ) -> fabric . runners . Result View Source @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ...","title":"run"},{"location":"reference/ai2_kit/core/connector/#sym_link","text":"def sym_link ( self , from_path : str , to_dir : str ) -> str View Source @abstractmethod def sym_link ( self , from_path : str , to_dir : str ) -> str : ...","title":"sym_link"},{"location":"reference/ai2_kit/core/connector/#upload","text":"def upload ( self , from_path : str , to_dir : str ) -> str View Source @ abstractmethod def upload ( self , from_path : str , to_dir : str ) -> str : ...","title":"upload"},{"location":"reference/ai2_kit/core/connector/#localconnector","text":"class LocalConnector ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class LocalConnector ( BaseConnector ): def dump_text ( self , text : str , path : str ): with open ( path , 'w' ) as f : f . write ( text ) def glob ( self , pattern : str ): return glob . glob ( pattern ) def run ( self , script , ** kwargs ): return invoke . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : os . makedirs ( to_dir , exist_ok = True ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir ) def download ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir )","title":"LocalConnector"},{"location":"reference/ai2_kit/core/connector/#ancestors-in-mro_1","text":"ai2_kit.core.connector.BaseConnector abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/connector/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/connector/#download_1","text":"def download ( self , from_path : str , to_dir : str ) -> str View Source def download ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir )","title":"download"},{"location":"reference/ai2_kit/core/connector/#dump_text_1","text":"def dump_text ( self , text : str , path : str ) View Source def dump_text(self, text: str, path: str): with open(path, 'w') as f: f.write(text)","title":"dump_text"},{"location":"reference/ai2_kit/core/connector/#glob_1","text":"def glob ( self , pattern : str ) View Source def glob ( self , pattern : str ) : return glob . glob ( pattern )","title":"glob"},{"location":"reference/ai2_kit/core/connector/#run_1","text":"def run ( self , script , ** kwargs ) View Source def run ( self , script , ** kwargs ) : return invoke . run ( script , ** kwargs )","title":"run"},{"location":"reference/ai2_kit/core/connector/#sym_link_1","text":"def sym_link ( self , from_path : str , to_dir : str ) -> str View Source def sym_link ( self , from_path : str , to_dir : str ) -> str : os . makedirs ( to_dir , exist_ok = True ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path","title":"sym_link"},{"location":"reference/ai2_kit/core/connector/#upload_1","text":"def upload ( self , from_path : str , to_dir : str ) -> str View Source def upload ( self , from_path : str , to_dir : str ) -> str : return self . sym_link ( from_path , to_dir )","title":"upload"},{"location":"reference/ai2_kit/core/connector/#sshconfig","text":"class SshConfig ( __pydantic_self__ , ** data : Any ) View Source class SshConfig ( BaseModel ): host: str gateway: Optional [ 'SshConfig' ]","title":"SshConfig"},{"location":"reference/ai2_kit/core/connector/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/connector/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/core/connector/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/connector/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/connector/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/connector/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/connector/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/connector/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/connector/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/connector/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/connector/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/connector/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/connector/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/connector/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/connector/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/connector/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/connector/#sshconnector","text":"class SshConnector ( connection : fabric . connection . Connection ) Helper class that provides a standard way to create an ABC using inheritance. View Source class SshConnector ( BaseConnector ): @classmethod def from_config ( cls , config : SshConfig ): connection = Connection ( host = config . host ) next_config = config . gateway next_connection = connection while next_config is not None : next_connection . gateway = Connection ( host = next_config . host ) next_connection = next_connection . gateway next_config = next_config . gateway return cls ( connection ) def __init__ ( self , connection : Connection ): self . _connection = connection def dump_text ( self , text : str , path : str ): f = StringIO ( text ) self . put ( f , path ) def glob ( self , pattern : str ): python_script = 'from glob import glob; from json import dumps; print(dumps(glob( {} )))' . format ( repr ( pattern )) cmd = 'python -c {} ' . format ( shlex . quote ( python_script )) result = self . run ( cmd , hide = True ) return json . loads ( result . stdout ) def run ( self , script , ** kwargs ) -> Result : return self . _connection . run ( script , ** kwargs ) def sym_link ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path def upload ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) return to_path def download ( self , from_path : str , to_dir : str ) -> str : sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if stat . S_ISDIR ( sftp . lstat ( from_path ) . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) return to_path def put ( self , * args , ** kwargs ): return self . _connection . put ( * args , ** kwargs ) def get ( self , * args , ** kwargs ): return self . _connection . get ( * args , ** kwargs ) def mkdir ( self , dir_path : str ): self . _connection . run ( 'mkdir -p {} ' . format ( shlex . quote ( dir_path ))) def put_dir ( self , from_dir : str , to_dir : str ): self . mkdir ( to_dir ) for item in os . listdir ( from_dir ): from_path = os . path . join ( from_dir , item ) to_path = os . path . join ( to_dir , item ) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) def get_dir ( self , from_dir : str , to_dir : str ): sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) for item in sftp . listdir_attr ( from_dir ): from_path = os . path . join ( from_dir , item . filename ) to_path = os . path . join ( to_dir , item . filename ) if stat . S_ISDIR ( item . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) def get_sftp ( self ): return self . _connection . sftp ()","title":"SshConnector"},{"location":"reference/ai2_kit/core/connector/#ancestors-in-mro_3","text":"ai2_kit.core.connector.BaseConnector abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/connector/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/connector/#from_config","text":"def from_config ( config : ai2_kit . core . connector . SshConfig ) View Source @classmethod def from_config ( cls , config : SshConfig ) : connection = Connection ( host = config . host ) next_config = config . gateway next_connection = connection while next_config is not None : next_connection . gateway = Connection ( host = next_config . host ) next_connection = next_connection . gateway next_config = next_config . gateway return cls ( connection )","title":"from_config"},{"location":"reference/ai2_kit/core/connector/#methods_3","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/connector/#download_2","text":"def download ( self , from_path : str , to_dir : str ) -> str View Source def download ( self , from_path : str , to_dir : str ) -> str : sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if stat . S_ISDIR ( sftp . lstat ( from_path ) . st_mode ): # type: ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path ) return to_path","title":"download"},{"location":"reference/ai2_kit/core/connector/#dump_text_2","text":"def dump_text ( self , text : str , path : str ) View Source def dump_text(self, text: str, path: str): f = StringIO(text) self.put(f, path)","title":"dump_text"},{"location":"reference/ai2_kit/core/connector/#get","text":"def get ( self , * args , ** kwargs ) View Source def get ( self , * args , ** kwargs ) : return self . _connection . get ( * args , ** kwargs )","title":"get"},{"location":"reference/ai2_kit/core/connector/#get_dir","text":"def get_dir ( self , from_dir : str , to_dir : str ) View Source def get_dir ( self , from_dir : str , to_dir : str ): sftp = self . get_sftp () os . makedirs ( to_dir , exist_ok = True ) for item in sftp . listdir_attr ( from_dir ): from_path = os . path . join ( from_dir , item . filename ) to_path = os . path . join ( to_dir , item . filename ) if stat . S_ISDIR ( item . st_mode ): # type : ignore self . get_dir ( from_path , to_path ) else : sftp . get ( from_path , to_path )","title":"get_dir"},{"location":"reference/ai2_kit/core/connector/#get_sftp","text":"def get_sftp ( self ) View Source def get_sftp ( self ) : return self . _connection . sftp ()","title":"get_sftp"},{"location":"reference/ai2_kit/core/connector/#glob_2","text":"def glob ( self , pattern : str ) View Source def glob ( self , pattern : str ): python_script = 'from glob import glob; from json import dumps; print(dumps(glob( {} )))' . format ( repr ( pattern )) cmd = 'python -c {} ' . format ( shlex . quote ( python_script )) result = self . run ( cmd , hide = True ) return json . loads ( result . stdout )","title":"glob"},{"location":"reference/ai2_kit/core/connector/#mkdir","text":"def mkdir ( self , dir_path : str ) View Source def mkdir(self, dir_path: str): self._connection.run('mkdir -p {}'.format(shlex.quote(dir_path)))","title":"mkdir"},{"location":"reference/ai2_kit/core/connector/#put","text":"def put ( self , * args , ** kwargs ) View Source def put ( self , * args , ** kwargs ) : return self . _connection . put ( * args , ** kwargs )","title":"put"},{"location":"reference/ai2_kit/core/connector/#put_dir","text":"def put_dir ( self , from_dir : str , to_dir : str ) View Source def put_dir ( self , from_dir : str , to_dir : str ) : self . mkdir ( to_dir ) for item in os . listdir ( from_dir ) : from_path = os . path . join ( from_dir , item ) to_path = os . path . join ( to_dir , item ) if os . path . isdir ( from_path ) : self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path )","title":"put_dir"},{"location":"reference/ai2_kit/core/connector/#run_2","text":"def run ( self , script , ** kwargs ) -> fabric . runners . Result View Source def run ( self , script , ** kwargs ) -> Result : return self . _connection . run ( script , ** kwargs )","title":"run"},{"location":"reference/ai2_kit/core/connector/#sym_link_2","text":"def sym_link ( self , from_path : str , to_dir : str ) -> str View Source def sym_link ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) basename = safe_basename ( from_path ) to_path = os . path . join ( to_dir , basename ) self . run ( get_ln_cmd ( from_path , to_path )) return to_path","title":"sym_link"},{"location":"reference/ai2_kit/core/connector/#upload_2","text":"def upload ( self , from_path : str , to_dir : str ) -> str View Source def upload ( self , from_path : str , to_dir : str ) -> str : self . mkdir ( to_dir ) to_path = os . path . join ( to_dir , safe_basename ( from_path )) if os . path . isdir ( from_path ): self . put_dir ( from_path , to_path ) else : self . put ( from_path , to_path ) return to_path","title":"upload"},{"location":"reference/ai2_kit/core/executor/","text":"Module ai2_kit.core.executor View Source from .queue_system import QueueSystemConfig , BaseQueueSystem , Slurm , Lsf from .job import JobFuture from .artifact import Artifact from .connector import SshConfig , BaseConnector , SshConnector , LocalConnector from .util import s_uuid from .log import get_logger logger = get_logger ( __name__ ) from pydantic import BaseModel from typing import Optional , Dict , List , TypeVar , Callable , Mapping from abc import ABC , abstractmethod from invoke import Result import os import shlex import base64 import bz2 import cloudpickle class BaseExecutorConfig ( BaseModel ): ssh : Optional [ SshConfig ] queue_system : QueueSystemConfig work_dir : str python_cmd : str = 'python' ExecutorMap = Mapping [ str , BaseExecutorConfig ] FnType = TypeVar ( 'FnType' , bound = Callable ) class Executor ( ABC ): name : str work_dir : str tmp_dir : str python_cmd : str def init ( self ): ... @abstractmethod def mkdir ( self , path : str ): ... @abstractmethod def run_python_script ( self , script : str , python_cmd = None ): ... @abstractmethod def run_python_fn ( self , fn : FnType , python_cmd = None ) -> FnType : ... @abstractmethod def dump_text ( self , text : str , path : str ): ... @abstractmethod def load_text ( self , path : str ) -> str : ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ]: ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def submit ( self , script : str , ** kwargs ) -> JobFuture : ... @abstractmethod def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def resolve_artifact ( self , artifact : Artifact ) -> List [ str ]: ... def setup_workspace ( self , workspace_dir : str , dirs : List [ str ]): paths = [ os . path . join ( workspace_dir , dir ) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s ' , path ) return paths class HpcExecutor ( Executor ): @classmethod def from_config ( cls , config : BaseExecutorConfig , name : str ): if config . ssh : connector = SshConnector . from_config ( config . ssh ) else : connector = LocalConnector () queue_system = None if config . queue_system . slurm : queue_system = Slurm () queue_system . config = config . queue_system . slurm elif config . queue_system . lsf : queue_system = Lsf () queue_system . config = config . queue_system . lsf if queue_system is None : raise ValueError ( 'Queue system config is missing!' ) queue_system . connector = connector return cls ( connector , queue_system , config . work_dir , config . python_cmd , name ) @property def is_local ( self ): return isinstance ( self . connector , LocalConnector ) def __init__ ( self , connector : BaseConnector , queue_system : BaseQueueSystem , work_dir : str , python_cmd : str , name : str ): self . name = name self . connector = connector self . queue_system = queue_system self . work_dir = work_dir self . python_cmd = python_cmd self . tmp_dir = os . path . join ( self . work_dir , '.tmp' ) # TODO: make it configurable def init ( self ): # if work_dir is relative path, it will be relative to user home if not os . path . isabs ( self . work_dir ): user_home = self . run ( 'echo $HOME' , hide = True ) . stdout . strip () self . work_dir = os . path . normpath ( os . path . join ( user_home , self . work_dir )) self . mkdir ( self . work_dir ) self . mkdir ( self . tmp_dir ) def mkdir ( self , path : str ): return self . connector . run ( 'mkdir -p {} ' . format ( shlex . quote ( path ))) def dump_text ( self , text : str , path : str ): return self . connector . dump_text ( text , path ) # TODO: handle error properly def load_text ( self , path : str ) -> str : return self . connector . run ( 'cat {} ' . format ( shlex . quote ( path )), hide = True ) . stdout def glob ( self , pattern : str ): return self . connector . glob ( pattern ) def run ( self , script : str , ** kwargs ): return self . connector . run ( script , ** kwargs ) def run_python_script ( self , script : str , python_cmd = None , cwd = None ): if python_cmd is None : python_cmd = self . python_cmd if cwd is None : cwd = self . work_dir cd_cwd = f 'cd { shlex . quote ( cwd ) } &&' script_len = len ( script ) logger . info ( 'the size of generated python script is %s ' , script_len ) if script_len < 100_000 : # ssh connection will be closed of the size of command is too large return self . connector . run ( f ' { cd_cwd } { python_cmd } -c { shlex . quote ( script ) } ' , hide = True ) else : script_path = os . path . join ( self . tmp_dir , f 'run_python_script_ { s_uuid () } .py' ) self . dump_text ( script , script_path ) ret = self . connector . run ( f ' { cd_cwd } { python_cmd } { shlex . quote ( script_path ) } ' , hide = True ) self . connector . run ( f 'rm { shlex . quote ( script_path ) } ' ) return ret def run_python_fn ( self , fn : FnType , python_cmd = None , cwd = None ) -> FnType : def remote_fn ( * args , ** kwargs ): script = fn_to_script ( lambda : fn ( * args , ** kwargs ), delimiter = '@' ) ret = self . run_python_script ( script = script , python_cmd = python_cmd , cwd = None ) _ , r = ret . stdout . rsplit ( '@' ) return cloudpickle . loads ( bz2 . decompress ( base64 . b64decode ( r ))) return remote_fn # type: ignore def submit ( self , script : str , cwd : str , ** kwargs ): return self . queue_system . submit ( script , cwd = cwd , ** kwargs ) def resolve_artifact ( self , artifact : Artifact ) -> List [ str ]: if artifact . includes is None : return [ artifact . url ] pattern = os . path . join ( artifact . url , artifact . includes ) return self . glob ( pattern ) def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . upload ( from_artifact . url , to_dir ) return Artifact ( executor = self . name , url = dest_path , attrs = from_artifact . attrs , ) # type: ignore def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . download ( from_artifact . url , to_dir ) return Artifact ( url = dest_path , attrs = from_artifact . attrs , ) # type: ignore def create_executor ( config : BaseExecutorConfig , name : str ) -> Executor : if config . queue_system is not None : return HpcExecutor . from_config ( config , name ) raise RuntimeError ( 'The executor configuration is not supported!' ) class ExecutorManager : def __init__ ( self , executor_configs : Mapping [ str , BaseExecutorConfig ]): self . _executor_configs = executor_configs self . _executors : Dict [ str , Executor ] = dict () def get_executor ( self , name : str ): config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) self . _executors [ name ] = executor return self . _executors [ name ] def fn_to_script ( fn : Callable , delimiter = '@' ): dumped_fn = base64 . b64encode ( bz2 . compress ( cloudpickle . dumps ( fn , protocol = cloudpickle . DEFAULT_PROTOCOL ), 5 )) script = [ f '''import base64,bz2,sys,cloudpickle as cp''' , f '''r=cp.loads(bz2.decompress(base64.b64decode( { repr ( dumped_fn ) } )))()''' , f '''print( { repr ( delimiter ) } +base64.b64encode(bz2.compress(cp.dumps(r, protocol=cp.DEFAULT_PROTOCOL),5)).decode('ascii'))''' , f '''sys.stdout.flush()''' , # ensure all output is printed ] return ';' . join ( script ) Variables ExecutorMap FnType logger Functions create_executor def create_executor ( config : ai2_kit . core . executor . BaseExecutorConfig , name : str ) -> ai2_kit . core . executor . Executor View Source def create_executor ( config : BaseExecutorConfig , name : str ) -> Executor : if config . queue_system is not None : return HpcExecutor . from_config ( config , name ) raise RuntimeError ( 'The executor configuration is not supported!' ) fn_to_script def fn_to_script ( fn : Callable , delimiter = '@' ) View Source def fn_to_script ( fn : Callable , delimiter = '@' ): dumped_fn = base64 . b64encode ( bz2 . compress ( cloudpickle . dumps ( fn , protocol = cloudpickle . DEFAULT_PROTOCOL ), 5 )) script = [ f '''import base64,bz2,sys,cloudpickle as cp''' , f '''r=cp.loads(bz2.decompress(base64.b64decode( { repr ( dumped_fn ) } )))()''' , f '''print( { repr ( delimiter ) } +base64.b64encode(bz2.compress(cp.dumps(r, protocol=cp.DEFAULT_PROTOCOL),5)).decode('ascii'))''' , f '''sys.stdout.flush()''' , # ensure all output is printed ] return ';' . join ( script ) Classes BaseExecutorConfig class BaseExecutorConfig ( __pydantic_self__ , ** data : Any ) View Source class BaseExecutorConfig ( BaseModel ) : ssh : Optional [ SshConfig ] queue_system : QueueSystemConfig work_dir : str python_cmd : str = 'python' Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Descendants ai2_kit.workflow.cll_mlp.CllWorkflowExecutorConfig ai2_kit.workflow.fep_mlp.FepExecutorConfig Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Executor class Executor ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Executor ( ABC ) : name : str work_dir : str tmp_dir : str python_cmd : str def init ( self ) : ... @abstractmethod def mkdir ( self , path : str ) : ... @abstractmethod def run_python_script ( self , script : str , python_cmd = None ) : ... @abstractmethod def run_python_fn ( self , fn : FnType , python_cmd = None ) -> FnType : ... @abstractmethod def dump_text ( self , text : str , path : str ) : ... @abstractmethod def load_text ( self , path : str ) -> str : ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def submit ( self , script : str , ** kwargs ) -> JobFuture : ... @abstractmethod def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : ... def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) : paths = [ os.path.join(workspace_dir, dir) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s' , path ) return paths Ancestors (in MRO) abc.ABC Descendants ai2_kit.core.executor.HpcExecutor Methods download def download ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source @ abstractmethod def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... dump_text def dump_text ( self , text : str , path : str ) View Source @abstractmethod def dump_text ( self , text : str , path : str ) : ... glob def glob ( self , pattern : str ) -> List [ str ] View Source @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ... init def init ( self ) View Source def init(self): ... load_text def load_text ( self , path : str ) -> str View Source @ abstractmethod def load_text ( self , path : str ) -> str : ... mkdir def mkdir ( self , path : str ) View Source @abstractmethod def mkdir ( self , path : str ) : ... resolve_artifact def resolve_artifact ( self , artifact : ai2_kit . core . artifact . Artifact ) -> List [ str ] View Source @abstractmethod def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : ... run def run ( self , script : str , ** kwargs ) -> invoke . runners . Result View Source @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... run_python_fn def run_python_fn ( self , fn : ~ FnType , python_cmd = None ) -> ~ FnType View Source @abstractmethod def run_python_fn ( self , fn : FnType , python_cmd = None ) -> FnType : ... run_python_script def run_python_script ( self , script : str , python_cmd = None ) View Source @abstractmethod def run_python_script ( self , script : str , python_cmd = None ) : ... setup_workspace def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) View Source def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) : paths = [ os.path.join(workspace_dir, dir) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s' , path ) return paths submit def submit ( self , script : str , ** kwargs ) -> ai2_kit . core . job . JobFuture View Source @abstractmethod def submit ( self , script : str , ** kwargs ) -> JobFuture : ... upload def upload ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source @ abstractmethod def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... ExecutorManager class ExecutorManager ( executor_configs : Mapping [ str , ai2_kit . core . executor . BaseExecutorConfig ] ) View Source class ExecutorManager : def __init__ ( self , executor_configs : Mapping [ str, BaseExecutorConfig ] ) : self . _executor_configs = executor_configs self . _executors : Dict [ str, Executor ] = dict () def get_executor ( self , name : str ) : config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) self . _executors [ name ] = executor return self . _executors [ name ] Methods get_executor def get_executor ( self , name : str ) View Source def get_executor ( self , name : str ) : config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) self . _executors [ name ] = executor return self . _executors [ name ] HpcExecutor class HpcExecutor ( connector : ai2_kit . core . connector . BaseConnector , queue_system : ai2_kit . core . queue_system . BaseQueueSystem , work_dir : str , python_cmd : str , name : str ) Helper class that provides a standard way to create an ABC using inheritance. View Source class HpcExecutor ( Executor ) : @classmethod def from_config ( cls , config : BaseExecutorConfig , name : str ) : if config . ssh : connector = SshConnector . from_config ( config . ssh ) else : connector = LocalConnector () queue_system = None if config . queue_system . slurm : queue_system = Slurm () queue_system . config = config . queue_system . slurm elif config . queue_system . lsf : queue_system = Lsf () queue_system . config = config . queue_system . lsf if queue_system is None : raise ValueError ( 'Queue system config is missing!' ) queue_system . connector = connector return cls ( connector , queue_system , config . work_dir , config . python_cmd , name ) @property def is_local ( self ) : return isinstance ( self . connector , LocalConnector ) def __init__ ( self , connector : BaseConnector , queue_system : BaseQueueSystem , work_dir : str , python_cmd : str , name : str ) : self . name = name self . connector = connector self . queue_system = queue_system self . work_dir = work_dir self . python_cmd = python_cmd self . tmp_dir = os . path . join ( self . work_dir , '.tmp' ) # TODO : make it configurable def init ( self ) : # if work_dir is relative path , it will be relative to user home if not os . path . isabs ( self . work_dir ) : user_home = self . run ( 'echo $HOME' , hide = True ). stdout . strip () self . work_dir = os . path . normpath ( os . path . join ( user_home , self . work_dir )) self . mkdir ( self . work_dir ) self . mkdir ( self . tmp_dir ) def mkdir ( self , path : str ) : return self . connector . run ( 'mkdir -p {}' . format ( shlex . quote ( path ))) def dump_text ( self , text : str , path : str ) : return self . connector . dump_text ( text , path ) # TODO : handle error properly def load_text ( self , path : str ) -> str : return self . connector . run ( 'cat {}' . format ( shlex . quote ( path )), hide = True ). stdout def glob ( self , pattern : str ) : return self . connector . glob ( pattern ) def run ( self , script : str , ** kwargs ) : return self . connector . run ( script , ** kwargs ) def run_python_script ( self , script : str , python_cmd = None , cwd = None ) : if python_cmd is None : python_cmd = self . python_cmd if cwd is None : cwd = self . work_dir cd_cwd = f 'cd {shlex.quote(cwd)} &&' script_len = len ( script ) logger . info ( 'the size of generated python script is %s' , script_len ) if script_len < 100 _000 : # ssh connection will be closed of the size of command is too large return self . connector . run ( f '{cd_cwd} {python_cmd} -c {shlex.quote(script)}' , hide = True ) else : script_path = os . path . join ( self . tmp_dir , f 'run_python_script_{s_uuid()}.py' ) self . dump_text ( script , script_path ) ret = self . connector . run ( f '{cd_cwd} {python_cmd} {shlex.quote(script_path)}' , hide = True ) self . connector . run ( f 'rm {shlex.quote(script_path)}' ) return ret def run_python_fn ( self , fn : FnType , python_cmd = None , cwd = None ) -> FnType : def remote_fn ( * args , ** kwargs ) : script = fn_to_script ( lambda : fn ( * args , ** kwargs ), delimiter = '@' ) ret = self . run_python_script ( script = script , python_cmd = python_cmd , cwd = None ) _ , r = ret . stdout . rsplit ( '@' ) return cloudpickle . loads ( bz2 . decompress ( base64 . b64decode ( r ))) return remote_fn # type : ignore def submit ( self , script : str , cwd : str , ** kwargs ) : return self . queue_system . submit ( script , cwd = cwd , ** kwargs ) def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : if artifact . includes is None : return [ artifact.url ] pattern = os . path . join ( artifact . url , artifact . includes ) return self . glob ( pattern ) def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . upload ( from_artifact . url , to_dir ) return Artifact ( executor = self . name , url = dest_path , attrs = from_artifact . attrs , ) # type : ignore def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . download ( from_artifact . url , to_dir ) return Artifact ( url = dest_path , attrs = from_artifact . attrs , ) # type : ignore Ancestors (in MRO) ai2_kit.core.executor.Executor abc.ABC Static methods from_config def from_config ( config : ai2_kit . core . executor . BaseExecutorConfig , name : str ) View Source @classmethod def from_config ( cls , config : BaseExecutorConfig , name : str ) : if config . ssh : connector = SshConnector . from_config ( config . ssh ) else : connector = LocalConnector () queue_system = None if config . queue_system . slurm : queue_system = Slurm () queue_system . config = config . queue_system . slurm elif config . queue_system . lsf : queue_system = Lsf () queue_system . config = config . queue_system . lsf if queue_system is None : raise ValueError ( 'Queue system config is missing!' ) queue_system . connector = connector return cls ( connector , queue_system , config . work_dir , config . python_cmd , name ) Instance variables is_local Methods download def download ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . download ( from_artifact . url , to_dir ) return Artifact ( url = dest_path , attrs = from_artifact . attrs , ) # type: ignore dump_text def dump_text ( self , text : str , path : str ) View Source def dump_text ( self , text : str , path : str ) : return self . connector . dump_text ( text , path ) glob def glob ( self , pattern : str ) View Source def glob ( self , pattern : str ) : return self . connector . glob ( pattern ) init def init ( self ) View Source def init ( self ) : # if work_dir is relative path , it will be relative to user home if not os . path . isabs ( self . work_dir ) : user_home = self . run ( 'echo $HOME' , hide = True ) . stdout . strip () self . work_dir = os . path . normpath ( os . path . join ( user_home , self . work_dir )) self . mkdir ( self . work_dir ) self . mkdir ( self . tmp_dir ) load_text def load_text ( self , path : str ) -> str View Source def load_text ( self , path : str ) -> str : return self . connector . run ( 'cat {}' . format ( shlex . quote ( path )), hide = True ) . stdout mkdir def mkdir ( self , path : str ) View Source def mkdir ( self , path : str ) : return self . connector . run ( 'mkdir -p {}' . format ( shlex . quote ( path ))) resolve_artifact def resolve_artifact ( self , artifact : ai2_kit . core . artifact . Artifact ) -> List [ str ] View Source def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : if artifact . includes is None : return [ artifact.url ] pattern = os . path . join ( artifact . url , artifact . includes ) return self . glob ( pattern ) run def run ( self , script : str , ** kwargs ) View Source def run ( self , script : str , ** kwargs ) : return self . connector . run ( script , ** kwargs ) run_python_fn def run_python_fn ( self , fn : ~ FnType , python_cmd = None , cwd = None ) -> ~ FnType View Source def run_python_fn ( self , fn : FnType , python_cmd = None , cwd = None ) -> FnType : def remote_fn ( * args , ** kwargs ): script = fn_to_script ( lambda : fn ( * args , ** kwargs ), delimiter = '@' ) ret = self . run_python_script ( script = script , python_cmd = python_cmd , cwd = None ) _ , r = ret . stdout . rsplit ( '@' ) return cloudpickle . loads ( bz2 . decompress ( base64 . b64decode ( r ))) return remote_fn # type: ignore run_python_script def run_python_script ( self , script : str , python_cmd = None , cwd = None ) View Source def run_python_script ( self , script : str , python_cmd = None , cwd = None ) : if python_cmd is None : python_cmd = self . python_cmd if cwd is None : cwd = self . work_dir cd_cwd = f 'cd {shlex.quote(cwd)} &&' script_len = len ( script ) logger . info ( 'the size of generated python script is %s' , script_len ) if script_len < 100 _000 : # ssh connection will be closed of the size of command is too large return self . connector . run ( f '{cd_cwd} {python_cmd} -c {shlex.quote(script)}' , hide = True ) else : script_path = os . path . join ( self . tmp_dir , f 'run_python_script_{s_uuid()}.py' ) self . dump_text ( script , script_path ) ret = self . connector . run ( f '{cd_cwd} {python_cmd} {shlex.quote(script_path)}' , hide = True ) self . connector . run ( f 'rm {shlex.quote(script_path)}' ) return ret setup_workspace def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) View Source def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) : paths = [ os.path.join(workspace_dir, dir) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s' , path ) return paths submit def submit ( self , script : str , cwd : str , ** kwargs ) View Source def submit ( self , script : str , cwd : str , ** kwargs ) : return self . queue_system . submit ( script , cwd = cwd , ** kwargs ) upload def upload ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . upload ( from_artifact . url , to_dir ) return Artifact ( executor = self . name , url = dest_path , attrs = from_artifact . attrs , ) # type: ignore","title":"Executor"},{"location":"reference/ai2_kit/core/executor/#module-ai2_kitcoreexecutor","text":"View Source from .queue_system import QueueSystemConfig , BaseQueueSystem , Slurm , Lsf from .job import JobFuture from .artifact import Artifact from .connector import SshConfig , BaseConnector , SshConnector , LocalConnector from .util import s_uuid from .log import get_logger logger = get_logger ( __name__ ) from pydantic import BaseModel from typing import Optional , Dict , List , TypeVar , Callable , Mapping from abc import ABC , abstractmethod from invoke import Result import os import shlex import base64 import bz2 import cloudpickle class BaseExecutorConfig ( BaseModel ): ssh : Optional [ SshConfig ] queue_system : QueueSystemConfig work_dir : str python_cmd : str = 'python' ExecutorMap = Mapping [ str , BaseExecutorConfig ] FnType = TypeVar ( 'FnType' , bound = Callable ) class Executor ( ABC ): name : str work_dir : str tmp_dir : str python_cmd : str def init ( self ): ... @abstractmethod def mkdir ( self , path : str ): ... @abstractmethod def run_python_script ( self , script : str , python_cmd = None ): ... @abstractmethod def run_python_fn ( self , fn : FnType , python_cmd = None ) -> FnType : ... @abstractmethod def dump_text ( self , text : str , path : str ): ... @abstractmethod def load_text ( self , path : str ) -> str : ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ]: ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def submit ( self , script : str , ** kwargs ) -> JobFuture : ... @abstractmethod def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def resolve_artifact ( self , artifact : Artifact ) -> List [ str ]: ... def setup_workspace ( self , workspace_dir : str , dirs : List [ str ]): paths = [ os . path . join ( workspace_dir , dir ) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s ' , path ) return paths class HpcExecutor ( Executor ): @classmethod def from_config ( cls , config : BaseExecutorConfig , name : str ): if config . ssh : connector = SshConnector . from_config ( config . ssh ) else : connector = LocalConnector () queue_system = None if config . queue_system . slurm : queue_system = Slurm () queue_system . config = config . queue_system . slurm elif config . queue_system . lsf : queue_system = Lsf () queue_system . config = config . queue_system . lsf if queue_system is None : raise ValueError ( 'Queue system config is missing!' ) queue_system . connector = connector return cls ( connector , queue_system , config . work_dir , config . python_cmd , name ) @property def is_local ( self ): return isinstance ( self . connector , LocalConnector ) def __init__ ( self , connector : BaseConnector , queue_system : BaseQueueSystem , work_dir : str , python_cmd : str , name : str ): self . name = name self . connector = connector self . queue_system = queue_system self . work_dir = work_dir self . python_cmd = python_cmd self . tmp_dir = os . path . join ( self . work_dir , '.tmp' ) # TODO: make it configurable def init ( self ): # if work_dir is relative path, it will be relative to user home if not os . path . isabs ( self . work_dir ): user_home = self . run ( 'echo $HOME' , hide = True ) . stdout . strip () self . work_dir = os . path . normpath ( os . path . join ( user_home , self . work_dir )) self . mkdir ( self . work_dir ) self . mkdir ( self . tmp_dir ) def mkdir ( self , path : str ): return self . connector . run ( 'mkdir -p {} ' . format ( shlex . quote ( path ))) def dump_text ( self , text : str , path : str ): return self . connector . dump_text ( text , path ) # TODO: handle error properly def load_text ( self , path : str ) -> str : return self . connector . run ( 'cat {} ' . format ( shlex . quote ( path )), hide = True ) . stdout def glob ( self , pattern : str ): return self . connector . glob ( pattern ) def run ( self , script : str , ** kwargs ): return self . connector . run ( script , ** kwargs ) def run_python_script ( self , script : str , python_cmd = None , cwd = None ): if python_cmd is None : python_cmd = self . python_cmd if cwd is None : cwd = self . work_dir cd_cwd = f 'cd { shlex . quote ( cwd ) } &&' script_len = len ( script ) logger . info ( 'the size of generated python script is %s ' , script_len ) if script_len < 100_000 : # ssh connection will be closed of the size of command is too large return self . connector . run ( f ' { cd_cwd } { python_cmd } -c { shlex . quote ( script ) } ' , hide = True ) else : script_path = os . path . join ( self . tmp_dir , f 'run_python_script_ { s_uuid () } .py' ) self . dump_text ( script , script_path ) ret = self . connector . run ( f ' { cd_cwd } { python_cmd } { shlex . quote ( script_path ) } ' , hide = True ) self . connector . run ( f 'rm { shlex . quote ( script_path ) } ' ) return ret def run_python_fn ( self , fn : FnType , python_cmd = None , cwd = None ) -> FnType : def remote_fn ( * args , ** kwargs ): script = fn_to_script ( lambda : fn ( * args , ** kwargs ), delimiter = '@' ) ret = self . run_python_script ( script = script , python_cmd = python_cmd , cwd = None ) _ , r = ret . stdout . rsplit ( '@' ) return cloudpickle . loads ( bz2 . decompress ( base64 . b64decode ( r ))) return remote_fn # type: ignore def submit ( self , script : str , cwd : str , ** kwargs ): return self . queue_system . submit ( script , cwd = cwd , ** kwargs ) def resolve_artifact ( self , artifact : Artifact ) -> List [ str ]: if artifact . includes is None : return [ artifact . url ] pattern = os . path . join ( artifact . url , artifact . includes ) return self . glob ( pattern ) def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . upload ( from_artifact . url , to_dir ) return Artifact ( executor = self . name , url = dest_path , attrs = from_artifact . attrs , ) # type: ignore def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . download ( from_artifact . url , to_dir ) return Artifact ( url = dest_path , attrs = from_artifact . attrs , ) # type: ignore def create_executor ( config : BaseExecutorConfig , name : str ) -> Executor : if config . queue_system is not None : return HpcExecutor . from_config ( config , name ) raise RuntimeError ( 'The executor configuration is not supported!' ) class ExecutorManager : def __init__ ( self , executor_configs : Mapping [ str , BaseExecutorConfig ]): self . _executor_configs = executor_configs self . _executors : Dict [ str , Executor ] = dict () def get_executor ( self , name : str ): config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) self . _executors [ name ] = executor return self . _executors [ name ] def fn_to_script ( fn : Callable , delimiter = '@' ): dumped_fn = base64 . b64encode ( bz2 . compress ( cloudpickle . dumps ( fn , protocol = cloudpickle . DEFAULT_PROTOCOL ), 5 )) script = [ f '''import base64,bz2,sys,cloudpickle as cp''' , f '''r=cp.loads(bz2.decompress(base64.b64decode( { repr ( dumped_fn ) } )))()''' , f '''print( { repr ( delimiter ) } +base64.b64encode(bz2.compress(cp.dumps(r, protocol=cp.DEFAULT_PROTOCOL),5)).decode('ascii'))''' , f '''sys.stdout.flush()''' , # ensure all output is printed ] return ';' . join ( script )","title":"Module ai2_kit.core.executor"},{"location":"reference/ai2_kit/core/executor/#variables","text":"ExecutorMap FnType logger","title":"Variables"},{"location":"reference/ai2_kit/core/executor/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/executor/#create_executor","text":"def create_executor ( config : ai2_kit . core . executor . BaseExecutorConfig , name : str ) -> ai2_kit . core . executor . Executor View Source def create_executor ( config : BaseExecutorConfig , name : str ) -> Executor : if config . queue_system is not None : return HpcExecutor . from_config ( config , name ) raise RuntimeError ( 'The executor configuration is not supported!' )","title":"create_executor"},{"location":"reference/ai2_kit/core/executor/#fn_to_script","text":"def fn_to_script ( fn : Callable , delimiter = '@' ) View Source def fn_to_script ( fn : Callable , delimiter = '@' ): dumped_fn = base64 . b64encode ( bz2 . compress ( cloudpickle . dumps ( fn , protocol = cloudpickle . DEFAULT_PROTOCOL ), 5 )) script = [ f '''import base64,bz2,sys,cloudpickle as cp''' , f '''r=cp.loads(bz2.decompress(base64.b64decode( { repr ( dumped_fn ) } )))()''' , f '''print( { repr ( delimiter ) } +base64.b64encode(bz2.compress(cp.dumps(r, protocol=cp.DEFAULT_PROTOCOL),5)).decode('ascii'))''' , f '''sys.stdout.flush()''' , # ensure all output is printed ] return ';' . join ( script )","title":"fn_to_script"},{"location":"reference/ai2_kit/core/executor/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/executor/#baseexecutorconfig","text":"class BaseExecutorConfig ( __pydantic_self__ , ** data : Any ) View Source class BaseExecutorConfig ( BaseModel ) : ssh : Optional [ SshConfig ] queue_system : QueueSystemConfig work_dir : str python_cmd : str = 'python'","title":"BaseExecutorConfig"},{"location":"reference/ai2_kit/core/executor/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/executor/#descendants","text":"ai2_kit.workflow.cll_mlp.CllWorkflowExecutorConfig ai2_kit.workflow.fep_mlp.FepExecutorConfig","title":"Descendants"},{"location":"reference/ai2_kit/core/executor/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/core/executor/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/executor/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/executor/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/executor/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/executor/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/executor/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/executor/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/executor/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/executor/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/executor/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/executor/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/executor/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/executor/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/executor/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/executor/#executor","text":"class Executor ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Executor ( ABC ) : name : str work_dir : str tmp_dir : str python_cmd : str def init ( self ) : ... @abstractmethod def mkdir ( self , path : str ) : ... @abstractmethod def run_python_script ( self , script : str , python_cmd = None ) : ... @abstractmethod def run_python_fn ( self , fn : FnType , python_cmd = None ) -> FnType : ... @abstractmethod def dump_text ( self , text : str , path : str ) : ... @abstractmethod def load_text ( self , path : str ) -> str : ... @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ... @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ... @abstractmethod def submit ( self , script : str , ** kwargs ) -> JobFuture : ... @abstractmethod def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ... @abstractmethod def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : ... def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) : paths = [ os.path.join(workspace_dir, dir) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s' , path ) return paths","title":"Executor"},{"location":"reference/ai2_kit/core/executor/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/executor/#descendants_1","text":"ai2_kit.core.executor.HpcExecutor","title":"Descendants"},{"location":"reference/ai2_kit/core/executor/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/executor/#download","text":"def download ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source @ abstractmethod def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ...","title":"download"},{"location":"reference/ai2_kit/core/executor/#dump_text","text":"def dump_text ( self , text : str , path : str ) View Source @abstractmethod def dump_text ( self , text : str , path : str ) : ...","title":"dump_text"},{"location":"reference/ai2_kit/core/executor/#glob","text":"def glob ( self , pattern : str ) -> List [ str ] View Source @abstractmethod def glob ( self , pattern : str ) -> List [ str ] : ...","title":"glob"},{"location":"reference/ai2_kit/core/executor/#init","text":"def init ( self ) View Source def init(self): ...","title":"init"},{"location":"reference/ai2_kit/core/executor/#load_text","text":"def load_text ( self , path : str ) -> str View Source @ abstractmethod def load_text ( self , path : str ) -> str : ...","title":"load_text"},{"location":"reference/ai2_kit/core/executor/#mkdir","text":"def mkdir ( self , path : str ) View Source @abstractmethod def mkdir ( self , path : str ) : ...","title":"mkdir"},{"location":"reference/ai2_kit/core/executor/#resolve_artifact","text":"def resolve_artifact ( self , artifact : ai2_kit . core . artifact . Artifact ) -> List [ str ] View Source @abstractmethod def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : ...","title":"resolve_artifact"},{"location":"reference/ai2_kit/core/executor/#run","text":"def run ( self , script : str , ** kwargs ) -> invoke . runners . Result View Source @abstractmethod def run ( self , script : str , ** kwargs ) -> Result : ...","title":"run"},{"location":"reference/ai2_kit/core/executor/#run_python_fn","text":"def run_python_fn ( self , fn : ~ FnType , python_cmd = None ) -> ~ FnType View Source @abstractmethod def run_python_fn ( self , fn : FnType , python_cmd = None ) -> FnType : ...","title":"run_python_fn"},{"location":"reference/ai2_kit/core/executor/#run_python_script","text":"def run_python_script ( self , script : str , python_cmd = None ) View Source @abstractmethod def run_python_script ( self , script : str , python_cmd = None ) : ...","title":"run_python_script"},{"location":"reference/ai2_kit/core/executor/#setup_workspace","text":"def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) View Source def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) : paths = [ os.path.join(workspace_dir, dir) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s' , path ) return paths","title":"setup_workspace"},{"location":"reference/ai2_kit/core/executor/#submit","text":"def submit ( self , script : str , ** kwargs ) -> ai2_kit . core . job . JobFuture View Source @abstractmethod def submit ( self , script : str , ** kwargs ) -> JobFuture : ...","title":"submit"},{"location":"reference/ai2_kit/core/executor/#upload","text":"def upload ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source @ abstractmethod def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : ...","title":"upload"},{"location":"reference/ai2_kit/core/executor/#executormanager","text":"class ExecutorManager ( executor_configs : Mapping [ str , ai2_kit . core . executor . BaseExecutorConfig ] ) View Source class ExecutorManager : def __init__ ( self , executor_configs : Mapping [ str, BaseExecutorConfig ] ) : self . _executor_configs = executor_configs self . _executors : Dict [ str, Executor ] = dict () def get_executor ( self , name : str ) : config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) self . _executors [ name ] = executor return self . _executors [ name ]","title":"ExecutorManager"},{"location":"reference/ai2_kit/core/executor/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/executor/#get_executor","text":"def get_executor ( self , name : str ) View Source def get_executor ( self , name : str ) : config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) self . _executors [ name ] = executor return self . _executors [ name ]","title":"get_executor"},{"location":"reference/ai2_kit/core/executor/#hpcexecutor","text":"class HpcExecutor ( connector : ai2_kit . core . connector . BaseConnector , queue_system : ai2_kit . core . queue_system . BaseQueueSystem , work_dir : str , python_cmd : str , name : str ) Helper class that provides a standard way to create an ABC using inheritance. View Source class HpcExecutor ( Executor ) : @classmethod def from_config ( cls , config : BaseExecutorConfig , name : str ) : if config . ssh : connector = SshConnector . from_config ( config . ssh ) else : connector = LocalConnector () queue_system = None if config . queue_system . slurm : queue_system = Slurm () queue_system . config = config . queue_system . slurm elif config . queue_system . lsf : queue_system = Lsf () queue_system . config = config . queue_system . lsf if queue_system is None : raise ValueError ( 'Queue system config is missing!' ) queue_system . connector = connector return cls ( connector , queue_system , config . work_dir , config . python_cmd , name ) @property def is_local ( self ) : return isinstance ( self . connector , LocalConnector ) def __init__ ( self , connector : BaseConnector , queue_system : BaseQueueSystem , work_dir : str , python_cmd : str , name : str ) : self . name = name self . connector = connector self . queue_system = queue_system self . work_dir = work_dir self . python_cmd = python_cmd self . tmp_dir = os . path . join ( self . work_dir , '.tmp' ) # TODO : make it configurable def init ( self ) : # if work_dir is relative path , it will be relative to user home if not os . path . isabs ( self . work_dir ) : user_home = self . run ( 'echo $HOME' , hide = True ). stdout . strip () self . work_dir = os . path . normpath ( os . path . join ( user_home , self . work_dir )) self . mkdir ( self . work_dir ) self . mkdir ( self . tmp_dir ) def mkdir ( self , path : str ) : return self . connector . run ( 'mkdir -p {}' . format ( shlex . quote ( path ))) def dump_text ( self , text : str , path : str ) : return self . connector . dump_text ( text , path ) # TODO : handle error properly def load_text ( self , path : str ) -> str : return self . connector . run ( 'cat {}' . format ( shlex . quote ( path )), hide = True ). stdout def glob ( self , pattern : str ) : return self . connector . glob ( pattern ) def run ( self , script : str , ** kwargs ) : return self . connector . run ( script , ** kwargs ) def run_python_script ( self , script : str , python_cmd = None , cwd = None ) : if python_cmd is None : python_cmd = self . python_cmd if cwd is None : cwd = self . work_dir cd_cwd = f 'cd {shlex.quote(cwd)} &&' script_len = len ( script ) logger . info ( 'the size of generated python script is %s' , script_len ) if script_len < 100 _000 : # ssh connection will be closed of the size of command is too large return self . connector . run ( f '{cd_cwd} {python_cmd} -c {shlex.quote(script)}' , hide = True ) else : script_path = os . path . join ( self . tmp_dir , f 'run_python_script_{s_uuid()}.py' ) self . dump_text ( script , script_path ) ret = self . connector . run ( f '{cd_cwd} {python_cmd} {shlex.quote(script_path)}' , hide = True ) self . connector . run ( f 'rm {shlex.quote(script_path)}' ) return ret def run_python_fn ( self , fn : FnType , python_cmd = None , cwd = None ) -> FnType : def remote_fn ( * args , ** kwargs ) : script = fn_to_script ( lambda : fn ( * args , ** kwargs ), delimiter = '@' ) ret = self . run_python_script ( script = script , python_cmd = python_cmd , cwd = None ) _ , r = ret . stdout . rsplit ( '@' ) return cloudpickle . loads ( bz2 . decompress ( base64 . b64decode ( r ))) return remote_fn # type : ignore def submit ( self , script : str , cwd : str , ** kwargs ) : return self . queue_system . submit ( script , cwd = cwd , ** kwargs ) def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : if artifact . includes is None : return [ artifact.url ] pattern = os . path . join ( artifact . url , artifact . includes ) return self . glob ( pattern ) def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . upload ( from_artifact . url , to_dir ) return Artifact ( executor = self . name , url = dest_path , attrs = from_artifact . attrs , ) # type : ignore def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . download ( from_artifact . url , to_dir ) return Artifact ( url = dest_path , attrs = from_artifact . attrs , ) # type : ignore","title":"HpcExecutor"},{"location":"reference/ai2_kit/core/executor/#ancestors-in-mro_2","text":"ai2_kit.core.executor.Executor abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/executor/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/executor/#from_config","text":"def from_config ( config : ai2_kit . core . executor . BaseExecutorConfig , name : str ) View Source @classmethod def from_config ( cls , config : BaseExecutorConfig , name : str ) : if config . ssh : connector = SshConnector . from_config ( config . ssh ) else : connector = LocalConnector () queue_system = None if config . queue_system . slurm : queue_system = Slurm () queue_system . config = config . queue_system . slurm elif config . queue_system . lsf : queue_system = Lsf () queue_system . config = config . queue_system . lsf if queue_system is None : raise ValueError ( 'Queue system config is missing!' ) queue_system . connector = connector return cls ( connector , queue_system , config . work_dir , config . python_cmd , name )","title":"from_config"},{"location":"reference/ai2_kit/core/executor/#instance-variables","text":"is_local","title":"Instance variables"},{"location":"reference/ai2_kit/core/executor/#methods_3","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/executor/#download_1","text":"def download ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source def download ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . download ( from_artifact . url , to_dir ) return Artifact ( url = dest_path , attrs = from_artifact . attrs , ) # type: ignore","title":"download"},{"location":"reference/ai2_kit/core/executor/#dump_text_1","text":"def dump_text ( self , text : str , path : str ) View Source def dump_text ( self , text : str , path : str ) : return self . connector . dump_text ( text , path )","title":"dump_text"},{"location":"reference/ai2_kit/core/executor/#glob_1","text":"def glob ( self , pattern : str ) View Source def glob ( self , pattern : str ) : return self . connector . glob ( pattern )","title":"glob"},{"location":"reference/ai2_kit/core/executor/#init_1","text":"def init ( self ) View Source def init ( self ) : # if work_dir is relative path , it will be relative to user home if not os . path . isabs ( self . work_dir ) : user_home = self . run ( 'echo $HOME' , hide = True ) . stdout . strip () self . work_dir = os . path . normpath ( os . path . join ( user_home , self . work_dir )) self . mkdir ( self . work_dir ) self . mkdir ( self . tmp_dir )","title":"init"},{"location":"reference/ai2_kit/core/executor/#load_text_1","text":"def load_text ( self , path : str ) -> str View Source def load_text ( self , path : str ) -> str : return self . connector . run ( 'cat {}' . format ( shlex . quote ( path )), hide = True ) . stdout","title":"load_text"},{"location":"reference/ai2_kit/core/executor/#mkdir_1","text":"def mkdir ( self , path : str ) View Source def mkdir ( self , path : str ) : return self . connector . run ( 'mkdir -p {}' . format ( shlex . quote ( path )))","title":"mkdir"},{"location":"reference/ai2_kit/core/executor/#resolve_artifact_1","text":"def resolve_artifact ( self , artifact : ai2_kit . core . artifact . Artifact ) -> List [ str ] View Source def resolve_artifact ( self , artifact : Artifact ) -> List [ str ] : if artifact . includes is None : return [ artifact.url ] pattern = os . path . join ( artifact . url , artifact . includes ) return self . glob ( pattern )","title":"resolve_artifact"},{"location":"reference/ai2_kit/core/executor/#run_1","text":"def run ( self , script : str , ** kwargs ) View Source def run ( self , script : str , ** kwargs ) : return self . connector . run ( script , ** kwargs )","title":"run"},{"location":"reference/ai2_kit/core/executor/#run_python_fn_1","text":"def run_python_fn ( self , fn : ~ FnType , python_cmd = None , cwd = None ) -> ~ FnType View Source def run_python_fn ( self , fn : FnType , python_cmd = None , cwd = None ) -> FnType : def remote_fn ( * args , ** kwargs ): script = fn_to_script ( lambda : fn ( * args , ** kwargs ), delimiter = '@' ) ret = self . run_python_script ( script = script , python_cmd = python_cmd , cwd = None ) _ , r = ret . stdout . rsplit ( '@' ) return cloudpickle . loads ( bz2 . decompress ( base64 . b64decode ( r ))) return remote_fn # type: ignore","title":"run_python_fn"},{"location":"reference/ai2_kit/core/executor/#run_python_script_1","text":"def run_python_script ( self , script : str , python_cmd = None , cwd = None ) View Source def run_python_script ( self , script : str , python_cmd = None , cwd = None ) : if python_cmd is None : python_cmd = self . python_cmd if cwd is None : cwd = self . work_dir cd_cwd = f 'cd {shlex.quote(cwd)} &&' script_len = len ( script ) logger . info ( 'the size of generated python script is %s' , script_len ) if script_len < 100 _000 : # ssh connection will be closed of the size of command is too large return self . connector . run ( f '{cd_cwd} {python_cmd} -c {shlex.quote(script)}' , hide = True ) else : script_path = os . path . join ( self . tmp_dir , f 'run_python_script_{s_uuid()}.py' ) self . dump_text ( script , script_path ) ret = self . connector . run ( f '{cd_cwd} {python_cmd} {shlex.quote(script_path)}' , hide = True ) self . connector . run ( f 'rm {shlex.quote(script_path)}' ) return ret","title":"run_python_script"},{"location":"reference/ai2_kit/core/executor/#setup_workspace_1","text":"def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) View Source def setup_workspace ( self , workspace_dir : str , dirs : List [ str ] ) : paths = [ os.path.join(workspace_dir, dir) for dir in dirs ] for path in paths : self . mkdir ( path ) logger . info ( 'create path: %s' , path ) return paths","title":"setup_workspace"},{"location":"reference/ai2_kit/core/executor/#submit_1","text":"def submit ( self , script : str , cwd : str , ** kwargs ) View Source def submit ( self , script : str , cwd : str , ** kwargs ) : return self . queue_system . submit ( script , cwd = cwd , ** kwargs )","title":"submit"},{"location":"reference/ai2_kit/core/executor/#upload_1","text":"def upload ( self , from_artifact : ai2_kit . core . artifact . Artifact , to_dir : str ) -> ai2_kit . core . artifact . Artifact View Source def upload ( self , from_artifact : Artifact , to_dir : str ) -> Artifact : dest_path = self . connector . upload ( from_artifact . url , to_dir ) return Artifact ( executor = self . name , url = dest_path , attrs = from_artifact . attrs , ) # type: ignore","title":"upload"},{"location":"reference/ai2_kit/core/future/","text":"Module ai2_kit.core.future View Source from typing import Generic , Optional , TypeVar from abc import abstractmethod T = TypeVar ( 'T' ) class IFuture ( Generic [ T ]): @abstractmethod def done ( self ) -> bool : pass @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ... Variables T Classes IFuture class IFuture ( / , * args , ** kwargs ) Abstract base class for generic types. A generic type is typically declared by inheriting from this class parameterized with one or more type variables. For example, a generic mapping type might be defined as:: class Mapping(Generic[KT, VT]): def getitem (self, key: KT) -> VT: ... # Etc. This class can then be used as follows:: def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT: try: return mapping[key] except KeyError: return default View Source class IFuture ( Generic [ T ] ) : @abstractmethod def done ( self ) -> bool : pass @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ... Ancestors (in MRO) typing.Generic Descendants ai2_kit.core.job.JobFuture Methods done def done ( self ) -> bool View Source @abstractmethod def done ( self ) -> bool : pass result def result ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass result_async def result_async ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ...","title":"Future"},{"location":"reference/ai2_kit/core/future/#module-ai2_kitcorefuture","text":"View Source from typing import Generic , Optional , TypeVar from abc import abstractmethod T = TypeVar ( 'T' ) class IFuture ( Generic [ T ]): @abstractmethod def done ( self ) -> bool : pass @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ...","title":"Module ai2_kit.core.future"},{"location":"reference/ai2_kit/core/future/#variables","text":"T","title":"Variables"},{"location":"reference/ai2_kit/core/future/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/future/#ifuture","text":"class IFuture ( / , * args , ** kwargs ) Abstract base class for generic types. A generic type is typically declared by inheriting from this class parameterized with one or more type variables. For example, a generic mapping type might be defined as:: class Mapping(Generic[KT, VT]): def getitem (self, key: KT) -> VT: ... # Etc. This class can then be used as follows:: def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT: try: return mapping[key] except KeyError: return default View Source class IFuture ( Generic [ T ] ) : @abstractmethod def done ( self ) -> bool : pass @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ...","title":"IFuture"},{"location":"reference/ai2_kit/core/future/#ancestors-in-mro","text":"typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/future/#descendants","text":"ai2_kit.core.job.JobFuture","title":"Descendants"},{"location":"reference/ai2_kit/core/future/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/future/#done","text":"def done ( self ) -> bool View Source @abstractmethod def done ( self ) -> bool : pass","title":"done"},{"location":"reference/ai2_kit/core/future/#result","text":"def result ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass","title":"result"},{"location":"reference/ai2_kit/core/future/#result_async","text":"def result_async ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ...","title":"result_async"},{"location":"reference/ai2_kit/core/job/","text":"Module ai2_kit.core.job View Source from typing import List , Callable , Optional , Awaitable from enum import Enum from abc import abstractmethod import time import asyncio from .future import IFuture # Copy from parsl class JobState ( bytes , Enum ): \"\"\"Defines a set of states that a job can be in\"\"\" def __new__ ( cls , value : int , terminal : bool , status_name : str ) -> \"JobState\" : obj = bytes . __new__ ( cls , [ value ]) obj . _value_ = value obj . terminal = terminal obj . status_name = status_name return obj value : int terminal : bool status_name : str UNKNOWN = ( 0 , False , \"UNKNOWN\" ) PENDING = ( 1 , False , \"PENDING\" ) RUNNING = ( 2 , False , \"RUNNING\" ) CANCELLED = ( 3 , True , \"CANCELLED\" ) COMPLETED = ( 4 , True , \"COMPLETED\" ) FAILED = ( 5 , True , \"FAILED\" ) TIMEOUT = ( 6 , True , \"TIMEOUT\" ) HELD = ( 7 , False , \"HELD\" ) class TimeoutError ( RuntimeError ): ... class JobFuture ( IFuture [ JobState ]): @abstractmethod def get_job_state ( self ) -> JobState : ... @abstractmethod def cancel ( self ): ... @abstractmethod def is_success ( self ) -> bool : ... @abstractmethod def resubmit ( self ) -> 'JobFuture' : ... async def gather_jobs ( jobs : List [ JobFuture ], timeout = float ( 'inf' ), max_tries : int = 1 ) -> List [ JobState ]: async def wait_job ( job : JobFuture ) -> JobState : state = JobState . UNKNOWN tries = 0 while True : try : state = await job . result_async ( timeout ) if state is JobState . COMPLETED : break except TimeoutError : state = JobState . TIMEOUT tries += 1 if tries >= max_tries : break job = job . resubmit () return state return await asyncio . gather ( * [ wait_job ( job ) for job in jobs ]) Functions gather_jobs def gather_jobs ( jobs : List [ ai2_kit . core . job . JobFuture ], timeout = inf , max_tries : int = 1 ) -> List [ ai2_kit . core . job . JobState ] View Source async def gather_jobs ( jobs : List [ JobFuture ] , timeout = float ( 'inf' ), max_tries : int = 1 ) -> List [ JobState ] : async def wait_job ( job : JobFuture ) -> JobState : state = JobState . UNKNOWN tries = 0 while True : try : state = await job . result_async ( timeout ) if state is JobState . COMPLETED : break except TimeoutError : state = JobState . TIMEOUT tries += 1 if tries >= max_tries : break job = job . resubmit () return state return await asyncio . gather ( *[ wait_job(job) for job in jobs ] ) Classes JobFuture class JobFuture ( / , * args , ** kwargs ) Abstract base class for generic types. A generic type is typically declared by inheriting from this class parameterized with one or more type variables. For example, a generic mapping type might be defined as:: class Mapping(Generic[KT, VT]): def getitem (self, key: KT) -> VT: ... # Etc. This class can then be used as follows:: def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT: try: return mapping[key] except KeyError: return default View Source class JobFuture ( IFuture [ JobState ] ) : @abstractmethod def get_job_state ( self ) -> JobState : ... @abstractmethod def cancel ( self ) : ... @abstractmethod def is_success ( self ) -> bool : ... @abstractmethod def resubmit ( self ) -> 'JobFuture' : ... Ancestors (in MRO) ai2_kit.core.future.IFuture typing.Generic Descendants ai2_kit.core.queue_system.QueueJobFuture Methods cancel def cancel ( self ) View Source @abstractmethod def cancel ( self ) : ... done def done ( self ) -> bool View Source @abstractmethod def done ( self ) -> bool : pass get_job_state def get_job_state ( self ) -> ai2_kit . core . job . JobState View Source @abstractmethod def get_job_state ( self ) -> JobState : ... is_success def is_success ( self ) -> bool View Source @abstractmethod def is_success ( self ) -> bool : ... resubmit def resubmit ( self ) -> 'JobFuture' View Source @abstractmethod def resubmit ( self ) -> 'JobFuture' : ... result def result ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass result_async def result_async ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ... JobState class JobState ( / , * args , ** kwargs ) Defines a set of states that a job can be in View Source class JobState ( bytes , Enum ) : \"\"\"Defines a set of states that a job can be in\"\"\" def __new__ ( cls , value : int , terminal : bool , status_name : str ) -> \"JobState\" : obj = bytes . __new__ ( cls , [ value ] ) obj . _value_ = value obj . terminal = terminal obj . status_name = status_name return obj value : int terminal : bool status_name : str UNKNOWN = ( 0 , False , \"UNKNOWN\" ) PENDING = ( 1 , False , \"PENDING\" ) RUNNING = ( 2 , False , \"RUNNING\" ) CANCELLED = ( 3 , True , \"CANCELLED\" ) COMPLETED = ( 4 , True , \"COMPLETED\" ) FAILED = ( 5 , True , \"FAILED\" ) TIMEOUT = ( 6 , True , \"TIMEOUT\" ) HELD = ( 7 , False , \"HELD\" ) Ancestors (in MRO) builtins.bytes enum.Enum Class variables CANCELLED COMPLETED FAILED HELD PENDING RUNNING TIMEOUT UNKNOWN name value TimeoutError class TimeoutError ( / , * args , ** kwargs ) Unspecified run-time error. View Source class TimeoutError ( RuntimeError ): ... Ancestors (in MRO) builtins.RuntimeError builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Job"},{"location":"reference/ai2_kit/core/job/#module-ai2_kitcorejob","text":"View Source from typing import List , Callable , Optional , Awaitable from enum import Enum from abc import abstractmethod import time import asyncio from .future import IFuture # Copy from parsl class JobState ( bytes , Enum ): \"\"\"Defines a set of states that a job can be in\"\"\" def __new__ ( cls , value : int , terminal : bool , status_name : str ) -> \"JobState\" : obj = bytes . __new__ ( cls , [ value ]) obj . _value_ = value obj . terminal = terminal obj . status_name = status_name return obj value : int terminal : bool status_name : str UNKNOWN = ( 0 , False , \"UNKNOWN\" ) PENDING = ( 1 , False , \"PENDING\" ) RUNNING = ( 2 , False , \"RUNNING\" ) CANCELLED = ( 3 , True , \"CANCELLED\" ) COMPLETED = ( 4 , True , \"COMPLETED\" ) FAILED = ( 5 , True , \"FAILED\" ) TIMEOUT = ( 6 , True , \"TIMEOUT\" ) HELD = ( 7 , False , \"HELD\" ) class TimeoutError ( RuntimeError ): ... class JobFuture ( IFuture [ JobState ]): @abstractmethod def get_job_state ( self ) -> JobState : ... @abstractmethod def cancel ( self ): ... @abstractmethod def is_success ( self ) -> bool : ... @abstractmethod def resubmit ( self ) -> 'JobFuture' : ... async def gather_jobs ( jobs : List [ JobFuture ], timeout = float ( 'inf' ), max_tries : int = 1 ) -> List [ JobState ]: async def wait_job ( job : JobFuture ) -> JobState : state = JobState . UNKNOWN tries = 0 while True : try : state = await job . result_async ( timeout ) if state is JobState . COMPLETED : break except TimeoutError : state = JobState . TIMEOUT tries += 1 if tries >= max_tries : break job = job . resubmit () return state return await asyncio . gather ( * [ wait_job ( job ) for job in jobs ])","title":"Module ai2_kit.core.job"},{"location":"reference/ai2_kit/core/job/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/job/#gather_jobs","text":"def gather_jobs ( jobs : List [ ai2_kit . core . job . JobFuture ], timeout = inf , max_tries : int = 1 ) -> List [ ai2_kit . core . job . JobState ] View Source async def gather_jobs ( jobs : List [ JobFuture ] , timeout = float ( 'inf' ), max_tries : int = 1 ) -> List [ JobState ] : async def wait_job ( job : JobFuture ) -> JobState : state = JobState . UNKNOWN tries = 0 while True : try : state = await job . result_async ( timeout ) if state is JobState . COMPLETED : break except TimeoutError : state = JobState . TIMEOUT tries += 1 if tries >= max_tries : break job = job . resubmit () return state return await asyncio . gather ( *[ wait_job(job) for job in jobs ] )","title":"gather_jobs"},{"location":"reference/ai2_kit/core/job/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/job/#jobfuture","text":"class JobFuture ( / , * args , ** kwargs ) Abstract base class for generic types. A generic type is typically declared by inheriting from this class parameterized with one or more type variables. For example, a generic mapping type might be defined as:: class Mapping(Generic[KT, VT]): def getitem (self, key: KT) -> VT: ... # Etc. This class can then be used as follows:: def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT: try: return mapping[key] except KeyError: return default View Source class JobFuture ( IFuture [ JobState ] ) : @abstractmethod def get_job_state ( self ) -> JobState : ... @abstractmethod def cancel ( self ) : ... @abstractmethod def is_success ( self ) -> bool : ... @abstractmethod def resubmit ( self ) -> 'JobFuture' : ...","title":"JobFuture"},{"location":"reference/ai2_kit/core/job/#ancestors-in-mro","text":"ai2_kit.core.future.IFuture typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/job/#descendants","text":"ai2_kit.core.queue_system.QueueJobFuture","title":"Descendants"},{"location":"reference/ai2_kit/core/job/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/job/#cancel","text":"def cancel ( self ) View Source @abstractmethod def cancel ( self ) : ...","title":"cancel"},{"location":"reference/ai2_kit/core/job/#done","text":"def done ( self ) -> bool View Source @abstractmethod def done ( self ) -> bool : pass","title":"done"},{"location":"reference/ai2_kit/core/job/#get_job_state","text":"def get_job_state ( self ) -> ai2_kit . core . job . JobState View Source @abstractmethod def get_job_state ( self ) -> JobState : ...","title":"get_job_state"},{"location":"reference/ai2_kit/core/job/#is_success","text":"def is_success ( self ) -> bool View Source @abstractmethod def is_success ( self ) -> bool : ...","title":"is_success"},{"location":"reference/ai2_kit/core/job/#resubmit","text":"def resubmit ( self ) -> 'JobFuture' View Source @abstractmethod def resubmit ( self ) -> 'JobFuture' : ...","title":"resubmit"},{"location":"reference/ai2_kit/core/job/#result","text":"def result ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source @abstractmethod def result ( self , timeout : Optional [ float ] = None ) -> T : pass","title":"result"},{"location":"reference/ai2_kit/core/job/#result_async","text":"def result_async ( self , timeout : Union [ float , NoneType ] = None ) -> ~ T View Source async def result_async ( self , timeout : Optional [ float ] = None ) -> T : ...","title":"result_async"},{"location":"reference/ai2_kit/core/job/#jobstate","text":"class JobState ( / , * args , ** kwargs ) Defines a set of states that a job can be in View Source class JobState ( bytes , Enum ) : \"\"\"Defines a set of states that a job can be in\"\"\" def __new__ ( cls , value : int , terminal : bool , status_name : str ) -> \"JobState\" : obj = bytes . __new__ ( cls , [ value ] ) obj . _value_ = value obj . terminal = terminal obj . status_name = status_name return obj value : int terminal : bool status_name : str UNKNOWN = ( 0 , False , \"UNKNOWN\" ) PENDING = ( 1 , False , \"PENDING\" ) RUNNING = ( 2 , False , \"RUNNING\" ) CANCELLED = ( 3 , True , \"CANCELLED\" ) COMPLETED = ( 4 , True , \"COMPLETED\" ) FAILED = ( 5 , True , \"FAILED\" ) TIMEOUT = ( 6 , True , \"TIMEOUT\" ) HELD = ( 7 , False , \"HELD\" )","title":"JobState"},{"location":"reference/ai2_kit/core/job/#ancestors-in-mro_1","text":"builtins.bytes enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/job/#class-variables","text":"CANCELLED COMPLETED FAILED HELD PENDING RUNNING TIMEOUT UNKNOWN name value","title":"Class variables"},{"location":"reference/ai2_kit/core/job/#timeouterror","text":"class TimeoutError ( / , * args , ** kwargs ) Unspecified run-time error. View Source class TimeoutError ( RuntimeError ): ...","title":"TimeoutError"},{"location":"reference/ai2_kit/core/job/#ancestors-in-mro_2","text":"builtins.RuntimeError builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/job/#class-variables_1","text":"args","title":"Class variables"},{"location":"reference/ai2_kit/core/job/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/job/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/ai2_kit/core/log/","text":"Module ai2_kit.core.log View Source import logging # format to include timestamp and module logging . basicConfig ( format = ' %(asctime)s %(name)s : %(message)s ' , level = logging . INFO ) # suppress transitions logging logging . getLogger ( 'transitions.core' ) . setLevel ( logging . WARNING ) def get_logger ( name = None ): return logging . getLogger ( name ) Functions get_logger def get_logger ( name = None ) View Source def get_logger ( name = None ) : return logging . getLogger ( name )","title":"Log"},{"location":"reference/ai2_kit/core/log/#module-ai2_kitcorelog","text":"View Source import logging # format to include timestamp and module logging . basicConfig ( format = ' %(asctime)s %(name)s : %(message)s ' , level = logging . INFO ) # suppress transitions logging logging . getLogger ( 'transitions.core' ) . setLevel ( logging . WARNING ) def get_logger ( name = None ): return logging . getLogger ( name )","title":"Module ai2_kit.core.log"},{"location":"reference/ai2_kit/core/log/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/log/#get_logger","text":"def get_logger ( name = None ) View Source def get_logger ( name = None ) : return logging . getLogger ( name )","title":"get_logger"},{"location":"reference/ai2_kit/core/queue_system/","text":"Module ai2_kit.core.queue_system View Source from pydantic import BaseModel from typing import Optional , Dict from abc import ABC , abstractmethod import shlex import os import re import time import asyncio from .connector import BaseConnector from .log import get_logger from .job import JobFuture , JobState from .checkpoint import apply_checkpoint , del_checkpoint from .util import short_hash logger = get_logger ( __name__ ) class QueueSystemConfig ( BaseModel ): class Slurm ( BaseModel ): sbatch_bin : str = 'sbatch' squeue_bin : str = 'squeue' scancel_bin : str = 'scancel' polling_interval : int = 10 class Lsf ( BaseModel ): bsub_bin : str = 'bsub' bjobs_bin : str = 'bjobs' polling_interval : int = 10 slurm : Optional [ Slurm ] lsf : Optional [ Lsf ] class BaseQueueSystem ( ABC ): connector : BaseConnector @abstractmethod def get_polling_interval ( self ) -> int : ... @abstractmethod def get_script_suffix ( self ) -> str : ... @abstractmethod def get_submit_cmd ( self ) -> str : ... @abstractmethod def get_job_id_pattern ( self ) -> str : ... @abstractmethod def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : ... @abstractmethod def cancel ( self , job_id : str ): ... def _post_submit ( self , job : 'QueueJobFuture' ): ... def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ): # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = ' \\n ' . join ([ script , '' , f 'touch { shlex . quote ( success_indicator ) } ' , '' , ]) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p { quoted_cwd } ' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd { quoted_cwd } && { self . get_submit_cmd () } { shlex . quote ( name ) } \" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: { script_path } ' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job def _submit_cmd ( self , cmd : str ): result = self . connector . run ( cmd ) m = re . search ( self . get_job_id_pattern (), result . stdout ) if m is None : raise RuntimeError ( \"Unable to parse job id\" ) return m . group ( 1 ) class Slurm ( BaseQueueSystem ): config : QueueSystemConfig . Slurm _last_states : Optional [ Dict [ str , JobState ]] _last_update_at : float = 0 translate_table = { 'PD' : JobState . PENDING , 'R' : JobState . RUNNING , 'CA' : JobState . CANCELLED , 'CF' : JobState . PENDING , # (configuring), 'CG' : JobState . RUNNING , # (completing), 'CD' : JobState . COMPLETED , 'F' : JobState . FAILED , # (failed), 'TO' : JobState . TIMEOUT , # (timeout), 'NF' : JobState . FAILED , # (node failure), 'RV' : JobState . FAILED , # (revoked) and 'SE' : JobState . FAILED # (special exit state) } def get_polling_interval ( self ): return self . config . polling_interval def get_script_suffix ( self ): return '.sbatch' def get_submit_cmd ( self ): return self . config . sbatch_bin def get_job_id_pattern ( self ): # example: Submitted batch job 123 return r \"Submitted batch job\\s+(\\d+)\" def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : state = self . _get_all_states () . get ( job_id ) if state is None : cmd = 'test -f {} ' . format ( shlex . quote ( success_indicator_path )) ret = self . connector . run ( cmd , warn = True ) if ret . return_code : return JobState . FAILED else : return JobState . COMPLETED else : return state def cancel ( self , job_id : str ): cmd = ' {} {} ' . format ( self . config . scancel_bin , job_id ) self . connector . run ( cmd ) def _post_submit ( self , job : 'QueueJobFuture' ): self . _last_states = None def _translate_state ( self , slurm_state : str ) -> JobState : return self . translate_table . get ( slurm_state , JobState . UNKNOWN ) def _get_all_states ( self ) -> Dict [ str , JobState ]: current_ts = time . time () if self . _last_states is not None and current_ts - self . _last_update_at < self . config . polling_interval : return self . _last_states cmd = \" {} --noheader --format=' %i %t' -u $(whoami)\" . format ( self . config . squeue_bin ) r = self . connector . run ( cmd , hide = True ) states : Dict [ str , JobState ] = dict () for line in r . stdout . split ( ' \\n ' ): if not line : # skip empty line continue job_id , slurm_state = line . split () state = self . _translate_state ( slurm_state ) states [ job_id ] = state self . _last_update_at = current_ts self . _last_states = states return states class Lsf ( BaseQueueSystem ): config : QueueSystemConfig . Lsf def get_polling_interval ( self ): return self . config . polling_interval def get_script_suffix ( self ): return '.lsf' def get_submit_cmd ( self ): return self . config . bsub_bin + ' <' def get_job_id_pattern ( self ): # example: Job <123> is submitted to queue <small>. return r \"Job <(\\d+)> is submitted to queue\" # TODO def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : return JobState . UNKNOWN # TODO def cancel ( self , job_id : str ): ... class QueueJobFuture ( JobFuture ): def __init__ ( self , queue_system : BaseQueueSystem , job_id : str , script : str , cwd : str , name : str , success_indicator : str , checkpoint_key : Optional [ str ], polling_interval = 10 , ): self . _queue_system = queue_system self . _name = name self . _script = script self . _cwd = cwd self . _job_id = job_id self . _checkpoint_key = checkpoint_key self . _success_indicator = success_indicator self . _polling_interval = polling_interval self . _final_state = None @property def success_indicator_path ( self ): return os . path . join ( self . _cwd , self . _success_indicator ) def get_job_state ( self ): if self . _final_state is not None : return self . _final_state state = self . _queue_system . get_job_state ( self . _job_id , self . success_indicator_path ) if state . terminal : self . _final_state = state return state def resubmit ( self ): if not self . done (): raise RuntimeError ( 'Cannot resubmit an unfinished job!' ) # delete checkpoint on resubmit if self . _checkpoint_key is not None : del_checkpoint ( self . _checkpoint_key ) return self . _queue_system . submit ( script = self . _script , cwd = self . _cwd , name = self . _name , checkpoint_key = self . _checkpoint_key , success_indicator = self . _success_indicator , ) def is_success ( self ): return self . get_job_state () is JobState . COMPLETED def cancel ( self ): self . _queue_system . cancel ( self . _job_id ) def done ( self ): return self . get_job_state () . terminal def result ( self , timeout : float = float ( 'inf' )) -> JobState : return asyncio . run ( self . result_async ( timeout )) async def result_async ( self , timeout : float = float ( 'inf' )) -> JobState : ''' Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). ''' timeout_ts = time . time () + timeout while time . time () < timeout_ts : if self . done (): return self . get_job_state () else : await asyncio . sleep ( self . _polling_interval ) else : raise RuntimeError ( f 'Timeout of polling job: { self . _job_id } ' ) def __repr__ ( self ): return repr ( dict ( name = self . _name , cwd = self . _cwd , job_id = self . _job_id , success_indicator = self . _success_indicator , polling_interval = self . _polling_interval , state = self . get_job_state (), )) Variables logger Classes BaseQueueSystem class BaseQueueSystem ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class BaseQueueSystem ( ABC ) : connector : BaseConnector @abstractmethod def get_polling_interval ( self ) -> int : ... @abstractmethod def get_script_suffix ( self ) -> str : ... @abstractmethod def get_submit_cmd ( self ) -> str : ... @abstractmethod def get_job_id_pattern ( self ) -> str : ... @abstractmethod def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : ... @abstractmethod def cancel ( self , job_id : str ) : ... def _post_submit ( self , job : 'QueueJobFuture' ) : ... def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job def _submit_cmd ( self , cmd : str ) : result = self . connector . run ( cmd ) m = re . search ( self . get_job_id_pattern (), result . stdout ) if m is None : raise RuntimeError ( \"Unable to parse job id\" ) return m . group ( 1 ) Ancestors (in MRO) abc.ABC Descendants ai2_kit.core.queue_system.Slurm ai2_kit.core.queue_system.Lsf Methods cancel def cancel ( self , job_id : str ) View Source @abstractmethod def cancel ( self , job_id : str ) : ... get_job_id_pattern def get_job_id_pattern ( self ) -> str View Source @abstractmethod def get_job_id_pattern ( self ) -> str : ... get_job_state def get_job_state ( self , job_id : str , success_indicator_path : str ) -> ai2_kit . core . job . JobState View Source @abstractmethod def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : ... get_polling_interval def get_polling_interval ( self ) -> int View Source @abstractmethod def get_polling_interval ( self ) -> int : ... get_script_suffix def get_script_suffix ( self ) -> str View Source @abstractmethod def get_script_suffix ( self ) -> str : ... get_submit_cmd def get_submit_cmd ( self ) -> str View Source @abstractmethod def get_submit_cmd ( self ) -> str : ... submit def submit ( self , script : str , cwd : str , name : Union [ str , NoneType ] = None , checkpoint_key : Union [ str , NoneType ] = None , success_indicator : Union [ str , NoneType ] = None ) View Source def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job Lsf class Lsf ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Lsf ( BaseQueueSystem ): config: QueueSystemConfig . Lsf def get_polling_interval ( self ): return self . config . polling_interval def get_script_suffix ( self ): return '.lsf' def get_submit_cmd ( self ): return self . config . bsub_bin + ' <' def get_job_id_pattern ( self ): # example: Job <123> is submitted to queue <small>. return r \"Job <(\\d+)> is submitted to queue\" # TODO def get_job_state ( self , job_id: str , success_indicator_path: str ) -> JobState: return JobState . UNKNOWN # TODO def cancel ( self , job_id: str ): ... Ancestors (in MRO) ai2_kit.core.queue_system.BaseQueueSystem abc.ABC Methods cancel def cancel ( self , job_id : str ) View Source def cancel(self, job_id: str): ... get_job_id_pattern def get_job_id_pattern ( self ) View Source def get_job_id_pattern ( self ) : # example : Job < 123 > is submitted to queue < small > . return r \"Job <(\\d+)> is submitted to queue\" get_job_state def get_job_state ( self , job_id : str , success_indicator_path : str ) -> ai2_kit . core . job . JobState View Source def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : return JobState . UNKNOWN get_polling_interval def get_polling_interval ( self ) View Source def get_polling_interval ( self ) : return self . config . polling_interval get_script_suffix def get_script_suffix ( self ) View Source def get_script_suffix ( self ) : return '.lsf' get_submit_cmd def get_submit_cmd ( self ) View Source def get_submit_cmd ( self ) : return self . config . bsub_bin + ' <' submit def submit ( self , script : str , cwd : str , name : Union [ str , NoneType ] = None , checkpoint_key : Union [ str , NoneType ] = None , success_indicator : Union [ str , NoneType ] = None ) View Source def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job QueueJobFuture class QueueJobFuture ( queue_system : ai2_kit . core . queue_system . BaseQueueSystem , job_id : str , script : str , cwd : str , name : str , success_indicator : str , checkpoint_key : Union [ str , NoneType ], polling_interval = 10 ) Abstract base class for generic types. A generic type is typically declared by inheriting from this class parameterized with one or more type variables. For example, a generic mapping type might be defined as:: class Mapping(Generic[KT, VT]): def getitem (self, key: KT) -> VT: ... # Etc. This class can then be used as follows:: def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT: try: return mapping[key] except KeyError: return default View Source class QueueJobFuture ( JobFuture ) : def __init__ ( self , queue_system : BaseQueueSystem , job_id : str , script : str , cwd : str , name : str , success_indicator : str , checkpoint_key : Optional [ str ] , polling_interval = 10 , ) : self . _queue_system = queue_system self . _name = name self . _script = script self . _cwd = cwd self . _job_id = job_id self . _checkpoint_key = checkpoint_key self . _success_indicator = success_indicator self . _polling_interval = polling_interval self . _final_state = None @property def success_indicator_path ( self ) : return os . path . join ( self . _cwd , self . _success_indicator ) def get_job_state ( self ) : if self . _final_state is not None : return self . _final_state state = self . _queue_system . get_job_state ( self . _job_id , self . success_indicator_path ) if state . terminal : self . _final_state = state return state def resubmit ( self ) : if not self . done () : raise RuntimeError ( 'Cannot resubmit an unfinished job!' ) # delete checkpoint on resubmit if self . _checkpoint_key is not None : del_checkpoint ( self . _checkpoint_key ) return self . _queue_system . submit ( script = self . _script , cwd = self . _cwd , name = self . _name , checkpoint_key = self . _checkpoint_key , success_indicator = self . _success_indicator , ) def is_success ( self ) : return self . get_job_state () is JobState . COMPLETED def cancel ( self ) : self . _queue_system . cancel ( self . _job_id ) def done ( self ) : return self . get_job_state (). terminal def result ( self , timeout : float = float ( 'inf' )) -> JobState : return asyncio . run ( self . result_async ( timeout )) async def result_async ( self , timeout : float = float ( 'inf' )) -> JobState : ''' Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). ''' timeout_ts = time . time () + timeout while time . time () < timeout_ts : if self . done () : return self . get_job_state () else : await asyncio . sleep ( self . _polling_interval ) else : raise RuntimeError ( f 'Timeout of polling job: {self._job_id}' ) def __repr__ ( self ) : return repr ( dict ( name = self . _name , cwd = self . _cwd , job_id = self . _job_id , success_indicator = self . _success_indicator , polling_interval = self . _polling_interval , state = self . get_job_state (), )) Ancestors (in MRO) ai2_kit.core.job.JobFuture ai2_kit.core.future.IFuture typing.Generic Instance variables success_indicator_path Methods cancel def cancel ( self ) View Source def cancel(self): self._queue_system.cancel(self._job_id) done def done ( self ) View Source def done ( self ) : return self . get_job_state () . terminal get_job_state def get_job_state ( self ) View Source def get_job_state ( self ) : if self . _final_state is not None : return self . _final_state state = self . _queue_system . get_job_state ( self . _job_id , self . success_indicator_path ) if state . terminal : self . _final_state = state return state is_success def is_success ( self ) View Source def is_success ( self ) : return self . get_job_state () is JobState . COMPLETED resubmit def resubmit ( self ) View Source def resubmit ( self ) : if not self . done () : raise RuntimeError ( 'Cannot resubmit an unfinished job!' ) # delete checkpoint on resubmit if self . _checkpoint_key is not None : del_checkpoint ( self . _checkpoint_key ) return self . _queue_system . submit ( script = self . _script , cwd = self . _cwd , name = self . _name , checkpoint_key = self . _checkpoint_key , success_indicator = self . _success_indicator , ) result def result ( self , timeout : float = inf ) -> ai2_kit . core . job . JobState View Source def result ( self , timeout : float = float ( 'inf' )) -> JobState : return asyncio . run ( self . result_async ( timeout )) result_async def result_async ( self , timeout : float = inf ) -> ai2_kit . core . job . JobState Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). View Source async def result_async ( self , timeout : float = float ( 'inf' )) -> JobState : ''' Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). ''' timeout_ts = time . time () + timeout while time . time () < timeout_ts : if self . done () : return self . get_job_state () else : await asyncio . sleep ( self . _polling_interval ) else : raise RuntimeError ( f 'Timeout of polling job: {self._job_id}' ) QueueSystemConfig class QueueSystemConfig ( __pydantic_self__ , ** data : Any ) View Source class QueueSystemConfig ( BaseModel ) : class Slurm ( BaseModel ) : sbatch_bin : str = 'sbatch' squeue_bin : str = 'squeue' scancel_bin : str = 'scancel' polling_interval : int = 10 class Lsf ( BaseModel ) : bsub_bin : str = 'bsub' bjobs_bin : str = 'bjobs' polling_interval : int = 10 slurm : Optional [ Slurm ] lsf : Optional [ Lsf ] Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Lsf Slurm Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . Slurm class Slurm ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Slurm ( BaseQueueSystem ) : config : QueueSystemConfig . Slurm _last_states : Optional [ Dict[str, JobState ] ] _last_update_at : float = 0 translate_table = { 'PD' : JobState . PENDING , 'R' : JobState . RUNNING , 'CA' : JobState . CANCELLED , 'CF' : JobState . PENDING , # ( configuring ), 'CG' : JobState . RUNNING , # ( completing ), 'CD' : JobState . COMPLETED , 'F' : JobState . FAILED , # ( failed ), 'TO' : JobState . TIMEOUT , # ( timeout ), 'NF' : JobState . FAILED , # ( node failure ), 'RV' : JobState . FAILED , # ( revoked ) and 'SE' : JobState . FAILED # ( special exit state ) } def get_polling_interval ( self ) : return self . config . polling_interval def get_script_suffix ( self ) : return '.sbatch' def get_submit_cmd ( self ) : return self . config . sbatch_bin def get_job_id_pattern ( self ) : # example : Submitted batch job 123 return r \"Submitted batch job\\s+(\\d+)\" def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : state = self . _get_all_states (). get ( job_id ) if state is None : cmd = 'test -f {}' . format ( shlex . quote ( success_indicator_path )) ret = self . connector . run ( cmd , warn = True ) if ret . return_code : return JobState . FAILED else : return JobState . COMPLETED else : return state def cancel ( self , job_id : str ) : cmd = '{} {}' . format ( self . config . scancel_bin , job_id ) self . connector . run ( cmd ) def _post_submit ( self , job : 'QueueJobFuture' ) : self . _last_states = None def _translate_state ( self , slurm_state : str ) -> JobState : return self . translate_table . get ( slurm_state , JobState . UNKNOWN ) def _get_all_states ( self ) -> Dict [ str, JobState ] : current_ts = time . time () if self . _last_states is not None and current_ts - self . _last_update_at < self . config . polling_interval : return self . _last_states cmd = \"{} --noheader --format='%i %t' -u $(whoami)\" . format ( self . config . squeue_bin ) r = self . connector . run ( cmd , hide = True ) states : Dict [ str, JobState ] = dict () for line in r . stdout . split ( '\\n' ) : if not line : # skip empty line continue job_id , slurm_state = line . split () state = self . _translate_state ( slurm_state ) states [ job_id ] = state self . _last_update_at = current_ts self . _last_states = states return states Ancestors (in MRO) ai2_kit.core.queue_system.BaseQueueSystem abc.ABC Class variables translate_table Methods cancel def cancel ( self , job_id : str ) View Source def cancel(self, job_id: str): cmd = '{} {}'.format(self.config.scancel_bin, job_id) self.connector.run(cmd) get_job_id_pattern def get_job_id_pattern ( self ) View Source def get_job_id_pattern ( self ) : # example : Submitted batch job 123 return r \"Submitted batch job\\s+(\\d+)\" get_job_state def get_job_state ( self , job_id : str , success_indicator_path : str ) -> ai2_kit . core . job . JobState View Source def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : state = self . _get_all_states (). get ( job_id ) if state is None : cmd = 'test -f {}' . format ( shlex . quote ( success_indicator_path )) ret = self . connector . run ( cmd , warn = True ) if ret . return_code : return JobState . FAILED else : return JobState . COMPLETED else : return state get_polling_interval def get_polling_interval ( self ) View Source def get_polling_interval ( self ) : return self . config . polling_interval get_script_suffix def get_script_suffix ( self ) View Source def get_script_suffix ( self ) : return '.sbatch' get_submit_cmd def get_submit_cmd ( self ) View Source def get_submit_cmd ( self ) : return self . config . sbatch_bin submit def submit ( self , script : str , cwd : str , name : Union [ str , NoneType ] = None , checkpoint_key : Union [ str , NoneType ] = None , success_indicator : Union [ str , NoneType ] = None ) View Source def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job","title":"Queue System"},{"location":"reference/ai2_kit/core/queue_system/#module-ai2_kitcorequeue_system","text":"View Source from pydantic import BaseModel from typing import Optional , Dict from abc import ABC , abstractmethod import shlex import os import re import time import asyncio from .connector import BaseConnector from .log import get_logger from .job import JobFuture , JobState from .checkpoint import apply_checkpoint , del_checkpoint from .util import short_hash logger = get_logger ( __name__ ) class QueueSystemConfig ( BaseModel ): class Slurm ( BaseModel ): sbatch_bin : str = 'sbatch' squeue_bin : str = 'squeue' scancel_bin : str = 'scancel' polling_interval : int = 10 class Lsf ( BaseModel ): bsub_bin : str = 'bsub' bjobs_bin : str = 'bjobs' polling_interval : int = 10 slurm : Optional [ Slurm ] lsf : Optional [ Lsf ] class BaseQueueSystem ( ABC ): connector : BaseConnector @abstractmethod def get_polling_interval ( self ) -> int : ... @abstractmethod def get_script_suffix ( self ) -> str : ... @abstractmethod def get_submit_cmd ( self ) -> str : ... @abstractmethod def get_job_id_pattern ( self ) -> str : ... @abstractmethod def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : ... @abstractmethod def cancel ( self , job_id : str ): ... def _post_submit ( self , job : 'QueueJobFuture' ): ... def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ): # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = ' \\n ' . join ([ script , '' , f 'touch { shlex . quote ( success_indicator ) } ' , '' , ]) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p { quoted_cwd } ' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd { quoted_cwd } && { self . get_submit_cmd () } { shlex . quote ( name ) } \" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: { script_path } ' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job def _submit_cmd ( self , cmd : str ): result = self . connector . run ( cmd ) m = re . search ( self . get_job_id_pattern (), result . stdout ) if m is None : raise RuntimeError ( \"Unable to parse job id\" ) return m . group ( 1 ) class Slurm ( BaseQueueSystem ): config : QueueSystemConfig . Slurm _last_states : Optional [ Dict [ str , JobState ]] _last_update_at : float = 0 translate_table = { 'PD' : JobState . PENDING , 'R' : JobState . RUNNING , 'CA' : JobState . CANCELLED , 'CF' : JobState . PENDING , # (configuring), 'CG' : JobState . RUNNING , # (completing), 'CD' : JobState . COMPLETED , 'F' : JobState . FAILED , # (failed), 'TO' : JobState . TIMEOUT , # (timeout), 'NF' : JobState . FAILED , # (node failure), 'RV' : JobState . FAILED , # (revoked) and 'SE' : JobState . FAILED # (special exit state) } def get_polling_interval ( self ): return self . config . polling_interval def get_script_suffix ( self ): return '.sbatch' def get_submit_cmd ( self ): return self . config . sbatch_bin def get_job_id_pattern ( self ): # example: Submitted batch job 123 return r \"Submitted batch job\\s+(\\d+)\" def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : state = self . _get_all_states () . get ( job_id ) if state is None : cmd = 'test -f {} ' . format ( shlex . quote ( success_indicator_path )) ret = self . connector . run ( cmd , warn = True ) if ret . return_code : return JobState . FAILED else : return JobState . COMPLETED else : return state def cancel ( self , job_id : str ): cmd = ' {} {} ' . format ( self . config . scancel_bin , job_id ) self . connector . run ( cmd ) def _post_submit ( self , job : 'QueueJobFuture' ): self . _last_states = None def _translate_state ( self , slurm_state : str ) -> JobState : return self . translate_table . get ( slurm_state , JobState . UNKNOWN ) def _get_all_states ( self ) -> Dict [ str , JobState ]: current_ts = time . time () if self . _last_states is not None and current_ts - self . _last_update_at < self . config . polling_interval : return self . _last_states cmd = \" {} --noheader --format=' %i %t' -u $(whoami)\" . format ( self . config . squeue_bin ) r = self . connector . run ( cmd , hide = True ) states : Dict [ str , JobState ] = dict () for line in r . stdout . split ( ' \\n ' ): if not line : # skip empty line continue job_id , slurm_state = line . split () state = self . _translate_state ( slurm_state ) states [ job_id ] = state self . _last_update_at = current_ts self . _last_states = states return states class Lsf ( BaseQueueSystem ): config : QueueSystemConfig . Lsf def get_polling_interval ( self ): return self . config . polling_interval def get_script_suffix ( self ): return '.lsf' def get_submit_cmd ( self ): return self . config . bsub_bin + ' <' def get_job_id_pattern ( self ): # example: Job <123> is submitted to queue <small>. return r \"Job <(\\d+)> is submitted to queue\" # TODO def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : return JobState . UNKNOWN # TODO def cancel ( self , job_id : str ): ... class QueueJobFuture ( JobFuture ): def __init__ ( self , queue_system : BaseQueueSystem , job_id : str , script : str , cwd : str , name : str , success_indicator : str , checkpoint_key : Optional [ str ], polling_interval = 10 , ): self . _queue_system = queue_system self . _name = name self . _script = script self . _cwd = cwd self . _job_id = job_id self . _checkpoint_key = checkpoint_key self . _success_indicator = success_indicator self . _polling_interval = polling_interval self . _final_state = None @property def success_indicator_path ( self ): return os . path . join ( self . _cwd , self . _success_indicator ) def get_job_state ( self ): if self . _final_state is not None : return self . _final_state state = self . _queue_system . get_job_state ( self . _job_id , self . success_indicator_path ) if state . terminal : self . _final_state = state return state def resubmit ( self ): if not self . done (): raise RuntimeError ( 'Cannot resubmit an unfinished job!' ) # delete checkpoint on resubmit if self . _checkpoint_key is not None : del_checkpoint ( self . _checkpoint_key ) return self . _queue_system . submit ( script = self . _script , cwd = self . _cwd , name = self . _name , checkpoint_key = self . _checkpoint_key , success_indicator = self . _success_indicator , ) def is_success ( self ): return self . get_job_state () is JobState . COMPLETED def cancel ( self ): self . _queue_system . cancel ( self . _job_id ) def done ( self ): return self . get_job_state () . terminal def result ( self , timeout : float = float ( 'inf' )) -> JobState : return asyncio . run ( self . result_async ( timeout )) async def result_async ( self , timeout : float = float ( 'inf' )) -> JobState : ''' Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). ''' timeout_ts = time . time () + timeout while time . time () < timeout_ts : if self . done (): return self . get_job_state () else : await asyncio . sleep ( self . _polling_interval ) else : raise RuntimeError ( f 'Timeout of polling job: { self . _job_id } ' ) def __repr__ ( self ): return repr ( dict ( name = self . _name , cwd = self . _cwd , job_id = self . _job_id , success_indicator = self . _success_indicator , polling_interval = self . _polling_interval , state = self . get_job_state (), ))","title":"Module ai2_kit.core.queue_system"},{"location":"reference/ai2_kit/core/queue_system/#variables","text":"logger","title":"Variables"},{"location":"reference/ai2_kit/core/queue_system/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/queue_system/#basequeuesystem","text":"class BaseQueueSystem ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class BaseQueueSystem ( ABC ) : connector : BaseConnector @abstractmethod def get_polling_interval ( self ) -> int : ... @abstractmethod def get_script_suffix ( self ) -> str : ... @abstractmethod def get_submit_cmd ( self ) -> str : ... @abstractmethod def get_job_id_pattern ( self ) -> str : ... @abstractmethod def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : ... @abstractmethod def cancel ( self , job_id : str ) : ... def _post_submit ( self , job : 'QueueJobFuture' ) : ... def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job def _submit_cmd ( self , cmd : str ) : result = self . connector . run ( cmd ) m = re . search ( self . get_job_id_pattern (), result . stdout ) if m is None : raise RuntimeError ( \"Unable to parse job id\" ) return m . group ( 1 )","title":"BaseQueueSystem"},{"location":"reference/ai2_kit/core/queue_system/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/queue_system/#descendants","text":"ai2_kit.core.queue_system.Slurm ai2_kit.core.queue_system.Lsf","title":"Descendants"},{"location":"reference/ai2_kit/core/queue_system/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/queue_system/#cancel","text":"def cancel ( self , job_id : str ) View Source @abstractmethod def cancel ( self , job_id : str ) : ...","title":"cancel"},{"location":"reference/ai2_kit/core/queue_system/#get_job_id_pattern","text":"def get_job_id_pattern ( self ) -> str View Source @abstractmethod def get_job_id_pattern ( self ) -> str : ...","title":"get_job_id_pattern"},{"location":"reference/ai2_kit/core/queue_system/#get_job_state","text":"def get_job_state ( self , job_id : str , success_indicator_path : str ) -> ai2_kit . core . job . JobState View Source @abstractmethod def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : ...","title":"get_job_state"},{"location":"reference/ai2_kit/core/queue_system/#get_polling_interval","text":"def get_polling_interval ( self ) -> int View Source @abstractmethod def get_polling_interval ( self ) -> int : ...","title":"get_polling_interval"},{"location":"reference/ai2_kit/core/queue_system/#get_script_suffix","text":"def get_script_suffix ( self ) -> str View Source @abstractmethod def get_script_suffix ( self ) -> str : ...","title":"get_script_suffix"},{"location":"reference/ai2_kit/core/queue_system/#get_submit_cmd","text":"def get_submit_cmd ( self ) -> str View Source @abstractmethod def get_submit_cmd ( self ) -> str : ...","title":"get_submit_cmd"},{"location":"reference/ai2_kit/core/queue_system/#submit","text":"def submit ( self , script : str , cwd : str , name : Union [ str , NoneType ] = None , checkpoint_key : Union [ str , NoneType ] = None , success_indicator : Union [ str , NoneType ] = None ) View Source def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job","title":"submit"},{"location":"reference/ai2_kit/core/queue_system/#lsf","text":"class Lsf ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Lsf ( BaseQueueSystem ): config: QueueSystemConfig . Lsf def get_polling_interval ( self ): return self . config . polling_interval def get_script_suffix ( self ): return '.lsf' def get_submit_cmd ( self ): return self . config . bsub_bin + ' <' def get_job_id_pattern ( self ): # example: Job <123> is submitted to queue <small>. return r \"Job <(\\d+)> is submitted to queue\" # TODO def get_job_state ( self , job_id: str , success_indicator_path: str ) -> JobState: return JobState . UNKNOWN # TODO def cancel ( self , job_id: str ): ...","title":"Lsf"},{"location":"reference/ai2_kit/core/queue_system/#ancestors-in-mro_1","text":"ai2_kit.core.queue_system.BaseQueueSystem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/queue_system/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/queue_system/#cancel_1","text":"def cancel ( self , job_id : str ) View Source def cancel(self, job_id: str): ...","title":"cancel"},{"location":"reference/ai2_kit/core/queue_system/#get_job_id_pattern_1","text":"def get_job_id_pattern ( self ) View Source def get_job_id_pattern ( self ) : # example : Job < 123 > is submitted to queue < small > . return r \"Job <(\\d+)> is submitted to queue\"","title":"get_job_id_pattern"},{"location":"reference/ai2_kit/core/queue_system/#get_job_state_1","text":"def get_job_state ( self , job_id : str , success_indicator_path : str ) -> ai2_kit . core . job . JobState View Source def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : return JobState . UNKNOWN","title":"get_job_state"},{"location":"reference/ai2_kit/core/queue_system/#get_polling_interval_1","text":"def get_polling_interval ( self ) View Source def get_polling_interval ( self ) : return self . config . polling_interval","title":"get_polling_interval"},{"location":"reference/ai2_kit/core/queue_system/#get_script_suffix_1","text":"def get_script_suffix ( self ) View Source def get_script_suffix ( self ) : return '.lsf'","title":"get_script_suffix"},{"location":"reference/ai2_kit/core/queue_system/#get_submit_cmd_1","text":"def get_submit_cmd ( self ) View Source def get_submit_cmd ( self ) : return self . config . bsub_bin + ' <'","title":"get_submit_cmd"},{"location":"reference/ai2_kit/core/queue_system/#submit_1","text":"def submit ( self , script : str , cwd : str , name : Union [ str , NoneType ] = None , checkpoint_key : Union [ str , NoneType ] = None , success_indicator : Union [ str , NoneType ] = None ) View Source def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job","title":"submit"},{"location":"reference/ai2_kit/core/queue_system/#queuejobfuture","text":"class QueueJobFuture ( queue_system : ai2_kit . core . queue_system . BaseQueueSystem , job_id : str , script : str , cwd : str , name : str , success_indicator : str , checkpoint_key : Union [ str , NoneType ], polling_interval = 10 ) Abstract base class for generic types. A generic type is typically declared by inheriting from this class parameterized with one or more type variables. For example, a generic mapping type might be defined as:: class Mapping(Generic[KT, VT]): def getitem (self, key: KT) -> VT: ... # Etc. This class can then be used as follows:: def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT: try: return mapping[key] except KeyError: return default View Source class QueueJobFuture ( JobFuture ) : def __init__ ( self , queue_system : BaseQueueSystem , job_id : str , script : str , cwd : str , name : str , success_indicator : str , checkpoint_key : Optional [ str ] , polling_interval = 10 , ) : self . _queue_system = queue_system self . _name = name self . _script = script self . _cwd = cwd self . _job_id = job_id self . _checkpoint_key = checkpoint_key self . _success_indicator = success_indicator self . _polling_interval = polling_interval self . _final_state = None @property def success_indicator_path ( self ) : return os . path . join ( self . _cwd , self . _success_indicator ) def get_job_state ( self ) : if self . _final_state is not None : return self . _final_state state = self . _queue_system . get_job_state ( self . _job_id , self . success_indicator_path ) if state . terminal : self . _final_state = state return state def resubmit ( self ) : if not self . done () : raise RuntimeError ( 'Cannot resubmit an unfinished job!' ) # delete checkpoint on resubmit if self . _checkpoint_key is not None : del_checkpoint ( self . _checkpoint_key ) return self . _queue_system . submit ( script = self . _script , cwd = self . _cwd , name = self . _name , checkpoint_key = self . _checkpoint_key , success_indicator = self . _success_indicator , ) def is_success ( self ) : return self . get_job_state () is JobState . COMPLETED def cancel ( self ) : self . _queue_system . cancel ( self . _job_id ) def done ( self ) : return self . get_job_state (). terminal def result ( self , timeout : float = float ( 'inf' )) -> JobState : return asyncio . run ( self . result_async ( timeout )) async def result_async ( self , timeout : float = float ( 'inf' )) -> JobState : ''' Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). ''' timeout_ts = time . time () + timeout while time . time () < timeout_ts : if self . done () : return self . get_job_state () else : await asyncio . sleep ( self . _polling_interval ) else : raise RuntimeError ( f 'Timeout of polling job: {self._job_id}' ) def __repr__ ( self ) : return repr ( dict ( name = self . _name , cwd = self . _cwd , job_id = self . _job_id , success_indicator = self . _success_indicator , polling_interval = self . _polling_interval , state = self . get_job_state (), ))","title":"QueueJobFuture"},{"location":"reference/ai2_kit/core/queue_system/#ancestors-in-mro_2","text":"ai2_kit.core.job.JobFuture ai2_kit.core.future.IFuture typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/queue_system/#instance-variables","text":"success_indicator_path","title":"Instance variables"},{"location":"reference/ai2_kit/core/queue_system/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/queue_system/#cancel_2","text":"def cancel ( self ) View Source def cancel(self): self._queue_system.cancel(self._job_id)","title":"cancel"},{"location":"reference/ai2_kit/core/queue_system/#done","text":"def done ( self ) View Source def done ( self ) : return self . get_job_state () . terminal","title":"done"},{"location":"reference/ai2_kit/core/queue_system/#get_job_state_2","text":"def get_job_state ( self ) View Source def get_job_state ( self ) : if self . _final_state is not None : return self . _final_state state = self . _queue_system . get_job_state ( self . _job_id , self . success_indicator_path ) if state . terminal : self . _final_state = state return state","title":"get_job_state"},{"location":"reference/ai2_kit/core/queue_system/#is_success","text":"def is_success ( self ) View Source def is_success ( self ) : return self . get_job_state () is JobState . COMPLETED","title":"is_success"},{"location":"reference/ai2_kit/core/queue_system/#resubmit","text":"def resubmit ( self ) View Source def resubmit ( self ) : if not self . done () : raise RuntimeError ( 'Cannot resubmit an unfinished job!' ) # delete checkpoint on resubmit if self . _checkpoint_key is not None : del_checkpoint ( self . _checkpoint_key ) return self . _queue_system . submit ( script = self . _script , cwd = self . _cwd , name = self . _name , checkpoint_key = self . _checkpoint_key , success_indicator = self . _success_indicator , )","title":"resubmit"},{"location":"reference/ai2_kit/core/queue_system/#result","text":"def result ( self , timeout : float = inf ) -> ai2_kit . core . job . JobState View Source def result ( self , timeout : float = float ( 'inf' )) -> JobState : return asyncio . run ( self . result_async ( timeout ))","title":"result"},{"location":"reference/ai2_kit/core/queue_system/#result_async","text":"def result_async ( self , timeout : float = inf ) -> ai2_kit . core . job . JobState Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). View Source async def result_async ( self , timeout : float = float ( 'inf' )) -> JobState : ''' Though this is not fully async, as the job submission and state polling are still blocking, but it is already good enough to handle thousands of jobs (I guess). ''' timeout_ts = time . time () + timeout while time . time () < timeout_ts : if self . done () : return self . get_job_state () else : await asyncio . sleep ( self . _polling_interval ) else : raise RuntimeError ( f 'Timeout of polling job: {self._job_id}' )","title":"result_async"},{"location":"reference/ai2_kit/core/queue_system/#queuesystemconfig","text":"class QueueSystemConfig ( __pydantic_self__ , ** data : Any ) View Source class QueueSystemConfig ( BaseModel ) : class Slurm ( BaseModel ) : sbatch_bin : str = 'sbatch' squeue_bin : str = 'squeue' scancel_bin : str = 'scancel' polling_interval : int = 10 class Lsf ( BaseModel ) : bsub_bin : str = 'bsub' bjobs_bin : str = 'bjobs' polling_interval : int = 10 slurm : Optional [ Slurm ] lsf : Optional [ Lsf ]","title":"QueueSystemConfig"},{"location":"reference/ai2_kit/core/queue_system/#ancestors-in-mro_3","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/queue_system/#class-variables","text":"Config Lsf Slurm","title":"Class variables"},{"location":"reference/ai2_kit/core/queue_system/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/queue_system/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/queue_system/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/queue_system/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/queue_system/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/queue_system/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/queue_system/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/queue_system/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/queue_system/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/queue_system/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/queue_system/#methods_3","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/queue_system/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/queue_system/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/queue_system/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/queue_system/#slurm","text":"class Slurm ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Slurm ( BaseQueueSystem ) : config : QueueSystemConfig . Slurm _last_states : Optional [ Dict[str, JobState ] ] _last_update_at : float = 0 translate_table = { 'PD' : JobState . PENDING , 'R' : JobState . RUNNING , 'CA' : JobState . CANCELLED , 'CF' : JobState . PENDING , # ( configuring ), 'CG' : JobState . RUNNING , # ( completing ), 'CD' : JobState . COMPLETED , 'F' : JobState . FAILED , # ( failed ), 'TO' : JobState . TIMEOUT , # ( timeout ), 'NF' : JobState . FAILED , # ( node failure ), 'RV' : JobState . FAILED , # ( revoked ) and 'SE' : JobState . FAILED # ( special exit state ) } def get_polling_interval ( self ) : return self . config . polling_interval def get_script_suffix ( self ) : return '.sbatch' def get_submit_cmd ( self ) : return self . config . sbatch_bin def get_job_id_pattern ( self ) : # example : Submitted batch job 123 return r \"Submitted batch job\\s+(\\d+)\" def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : state = self . _get_all_states (). get ( job_id ) if state is None : cmd = 'test -f {}' . format ( shlex . quote ( success_indicator_path )) ret = self . connector . run ( cmd , warn = True ) if ret . return_code : return JobState . FAILED else : return JobState . COMPLETED else : return state def cancel ( self , job_id : str ) : cmd = '{} {}' . format ( self . config . scancel_bin , job_id ) self . connector . run ( cmd ) def _post_submit ( self , job : 'QueueJobFuture' ) : self . _last_states = None def _translate_state ( self , slurm_state : str ) -> JobState : return self . translate_table . get ( slurm_state , JobState . UNKNOWN ) def _get_all_states ( self ) -> Dict [ str, JobState ] : current_ts = time . time () if self . _last_states is not None and current_ts - self . _last_update_at < self . config . polling_interval : return self . _last_states cmd = \"{} --noheader --format='%i %t' -u $(whoami)\" . format ( self . config . squeue_bin ) r = self . connector . run ( cmd , hide = True ) states : Dict [ str, JobState ] = dict () for line in r . stdout . split ( '\\n' ) : if not line : # skip empty line continue job_id , slurm_state = line . split () state = self . _translate_state ( slurm_state ) states [ job_id ] = state self . _last_update_at = current_ts self . _last_states = states return states","title":"Slurm"},{"location":"reference/ai2_kit/core/queue_system/#ancestors-in-mro_4","text":"ai2_kit.core.queue_system.BaseQueueSystem abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/queue_system/#class-variables_1","text":"translate_table","title":"Class variables"},{"location":"reference/ai2_kit/core/queue_system/#methods_4","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/queue_system/#cancel_3","text":"def cancel ( self , job_id : str ) View Source def cancel(self, job_id: str): cmd = '{} {}'.format(self.config.scancel_bin, job_id) self.connector.run(cmd)","title":"cancel"},{"location":"reference/ai2_kit/core/queue_system/#get_job_id_pattern_2","text":"def get_job_id_pattern ( self ) View Source def get_job_id_pattern ( self ) : # example : Submitted batch job 123 return r \"Submitted batch job\\s+(\\d+)\"","title":"get_job_id_pattern"},{"location":"reference/ai2_kit/core/queue_system/#get_job_state_3","text":"def get_job_state ( self , job_id : str , success_indicator_path : str ) -> ai2_kit . core . job . JobState View Source def get_job_state ( self , job_id : str , success_indicator_path : str ) -> JobState : state = self . _get_all_states (). get ( job_id ) if state is None : cmd = 'test -f {}' . format ( shlex . quote ( success_indicator_path )) ret = self . connector . run ( cmd , warn = True ) if ret . return_code : return JobState . FAILED else : return JobState . COMPLETED else : return state","title":"get_job_state"},{"location":"reference/ai2_kit/core/queue_system/#get_polling_interval_2","text":"def get_polling_interval ( self ) View Source def get_polling_interval ( self ) : return self . config . polling_interval","title":"get_polling_interval"},{"location":"reference/ai2_kit/core/queue_system/#get_script_suffix_2","text":"def get_script_suffix ( self ) View Source def get_script_suffix ( self ) : return '.sbatch'","title":"get_script_suffix"},{"location":"reference/ai2_kit/core/queue_system/#get_submit_cmd_2","text":"def get_submit_cmd ( self ) View Source def get_submit_cmd ( self ) : return self . config . sbatch_bin","title":"get_submit_cmd"},{"location":"reference/ai2_kit/core/queue_system/#submit_2","text":"def submit ( self , script : str , cwd : str , name : Union [ str , NoneType ] = None , checkpoint_key : Union [ str , NoneType ] = None , success_indicator : Union [ str , NoneType ] = None ) View Source def submit ( self , script : str , cwd : str , name : Optional [ str ] = None , checkpoint_key : Optional [ str ] = None , success_indicator : Optional [ str ] = None , ) : # use hash instead of uuid to ensure idempotence if name is None : name = 'job-' + short_hash ( script ) + self . get_script_suffix () quoted_cwd = shlex . quote ( cwd ) # a placeholder file that will be created when the script end without error if success_indicator is None : success_indicator = name + '.success' # create script script = '\\n' . join ( [ script, '', f'touch {shlex.quote(success_indicator)}', '', ] ) script_path = os . path . join ( cwd , name ) self . connector . run ( f 'mkdir -p {quoted_cwd}' ) self . connector . dump_text ( script , script_path ) # submit script cmd = f \"cd {quoted_cwd} && {self.get_submit_cmd()} {shlex.quote(name)}\" # apply checkpoint submit_cmd_fn = self . _submit_cmd if checkpoint_key is not None : submit_cmd_fn = apply_checkpoint ( checkpoint_key )( submit_cmd_fn ) logger . info ( f 'Submit batch script: {script_path}' ) job_id = submit_cmd_fn ( cmd ) job = QueueJobFuture ( self , job_id = job_id , name = name , script = script , cwd = cwd , checkpoint_key = checkpoint_key , success_indicator = success_indicator , polling_interval = self . get_polling_interval () // 2 , ) self . _post_submit ( job ) return job","title":"submit"},{"location":"reference/ai2_kit/core/resource_manager/","text":"Module ai2_kit.core.resource_manager View Source from typing import Dict , List , Tuple , Union , Optional , Sequence import copy from .artifact import Artifact , ArtifactMap from .executor import Executor , ExecutorMap , create_executor ArtifactOrKey = Union [ Artifact , str ] class ResourceManager : @property def default_executor ( self ): return self . get_executor () def __init__ ( self , executor_configs : ExecutorMap , artifacts : ArtifactMap , default_executor : str , ) -> None : self . _executor_configs = executor_configs self . _default_executor_name = default_executor self . _executors : Dict [ str , Executor ] = dict () # runtime check to ensure quick failure self . default_executor # fill in default values for key , artifact in artifacts . items (): if artifact . executor is None : artifact . executor = self . default_executor . name if artifact . key is None : artifact . key = key self . _artifacts = artifacts def get_executor ( self , name : Optional [ str ] = None ) -> Executor : if name is None : name = self . _default_executor_name config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) executor . init () self . _executors [ name ] = executor return self . _executors [ name ] def get_artifact ( self , key : str ) -> Artifact : # raise error it is by designed, ensure quick failure return self . _artifacts [ key ] def get_artifacts ( self , keys : List [ str ]) -> List [ Artifact ]: return [ self . get_artifact ( key ) for key in keys ] def resolve_artifact ( self , artifact : ArtifactOrKey ) -> List [ Artifact ]: # TODO: support cross executor data resolve in the future if isinstance ( artifact , str ): artifact = self . get_artifact ( artifact ) paths = self . default_executor . resolve_artifact ( artifact ) return [ Artifact . of ( url = path , format = artifact . format , includes = None , # has been consumed attrs = copy . deepcopy ( artifact . attrs ), executor = self . default_executor . name , ) for path in paths ] def resolve_artifacts ( self , artifacts : Sequence [ ArtifactOrKey ]) -> List [ Artifact ]: # flat map return [ x for a in artifacts for x in self . resolve_artifact ( a )] Variables ArtifactOrKey Classes ResourceManager class ResourceManager ( executor_configs : Mapping [ str , ai2_kit . core . executor . BaseExecutorConfig ], artifacts : Mapping [ str , ai2_kit . core . artifact . Artifact ], default_executor : str ) View Source class ResourceManager : @property def default_executor ( self ) : return self . get_executor () def __init__ ( self , executor_configs : ExecutorMap , artifacts : ArtifactMap , default_executor : str , ) -> None : self . _executor_configs = executor_configs self . _default_executor_name = default_executor self . _executors : Dict [ str, Executor ] = dict () # runtime check to ensure quick failure self . default_executor # fill in default values for key , artifact in artifacts . items () : if artifact . executor is None : artifact . executor = self . default_executor . name if artifact . key is None : artifact . key = key self . _artifacts = artifacts def get_executor ( self , name : Optional [ str ] = None ) -> Executor : if name is None : name = self . _default_executor_name config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) executor . init () self . _executors [ name ] = executor return self . _executors [ name ] def get_artifact ( self , key : str ) -> Artifact : # raise error it is by designed , ensure quick failure return self . _artifacts [ key ] def get_artifacts ( self , keys : List [ str ] ) -> List [ Artifact ] : return [ self.get_artifact(key) for key in keys ] def resolve_artifact ( self , artifact : ArtifactOrKey ) -> List [ Artifact ] : # TODO : support cross executor data resolve in the future if isinstance ( artifact , str ) : artifact = self . get_artifact ( artifact ) paths = self . default_executor . resolve_artifact ( artifact ) return [ Artifact.of( url=path, format=artifact.format, includes=None, # has been consumed attrs=copy.deepcopy(artifact.attrs), executor=self.default_executor.name, ) for path in paths ] def resolve_artifacts ( self , artifacts : Sequence [ ArtifactOrKey ] ) -> List [ Artifact ] : # flat map return [ x for a in artifacts for x in self.resolve_artifact(a) ] Instance variables default_executor Methods get_artifact def get_artifact ( self , key : str ) -> ai2_kit . core . artifact . Artifact View Source def get_artifact ( self , key : str ) -> Artifact : # raise error it is by designed , ensure quick failure return self . _artifacts [ key ] get_artifacts def get_artifacts ( self , keys : List [ str ] ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_artifacts ( self , keys : List [ str ] ) -> List [ Artifact ] : return [ self.get_artifact(key) for key in keys ] get_executor def get_executor ( self , name : Union [ str , NoneType ] = None ) -> ai2_kit . core . executor . Executor View Source def get_executor ( self , name : Optional [ str ] = None ) -> Executor : if name is None : name = self . _default_executor_name config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) executor . init () self . _executors [ name ] = executor return self . _executors [ name ] resolve_artifact def resolve_artifact ( self , artifact : Union [ ai2_kit . core . artifact . Artifact , str ] ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def resolve_artifact ( self , artifact : ArtifactOrKey ) -> List [ Artifact ] : # TODO : support cross executor data resolve in the future if isinstance ( artifact , str ) : artifact = self . get_artifact ( artifact ) paths = self . default_executor . resolve_artifact ( artifact ) return [ Artifact.of( url=path, format=artifact.format, includes=None, # has been consumed attrs=copy.deepcopy(artifact.attrs), executor=self.default_executor.name, ) for path in paths ] resolve_artifacts def resolve_artifacts ( self , artifacts : Sequence [ Union [ ai2_kit . core . artifact . Artifact , str ]] ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def resolve_artifacts ( self , artifacts : Sequence [ ArtifactOrKey ] ) -> List [ Artifact ] : # flat map return [ x for a in artifacts for x in self.resolve_artifact(a) ]","title":"Resource Manager"},{"location":"reference/ai2_kit/core/resource_manager/#module-ai2_kitcoreresource_manager","text":"View Source from typing import Dict , List , Tuple , Union , Optional , Sequence import copy from .artifact import Artifact , ArtifactMap from .executor import Executor , ExecutorMap , create_executor ArtifactOrKey = Union [ Artifact , str ] class ResourceManager : @property def default_executor ( self ): return self . get_executor () def __init__ ( self , executor_configs : ExecutorMap , artifacts : ArtifactMap , default_executor : str , ) -> None : self . _executor_configs = executor_configs self . _default_executor_name = default_executor self . _executors : Dict [ str , Executor ] = dict () # runtime check to ensure quick failure self . default_executor # fill in default values for key , artifact in artifacts . items (): if artifact . executor is None : artifact . executor = self . default_executor . name if artifact . key is None : artifact . key = key self . _artifacts = artifacts def get_executor ( self , name : Optional [ str ] = None ) -> Executor : if name is None : name = self . _default_executor_name config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) executor . init () self . _executors [ name ] = executor return self . _executors [ name ] def get_artifact ( self , key : str ) -> Artifact : # raise error it is by designed, ensure quick failure return self . _artifacts [ key ] def get_artifacts ( self , keys : List [ str ]) -> List [ Artifact ]: return [ self . get_artifact ( key ) for key in keys ] def resolve_artifact ( self , artifact : ArtifactOrKey ) -> List [ Artifact ]: # TODO: support cross executor data resolve in the future if isinstance ( artifact , str ): artifact = self . get_artifact ( artifact ) paths = self . default_executor . resolve_artifact ( artifact ) return [ Artifact . of ( url = path , format = artifact . format , includes = None , # has been consumed attrs = copy . deepcopy ( artifact . attrs ), executor = self . default_executor . name , ) for path in paths ] def resolve_artifacts ( self , artifacts : Sequence [ ArtifactOrKey ]) -> List [ Artifact ]: # flat map return [ x for a in artifacts for x in self . resolve_artifact ( a )]","title":"Module ai2_kit.core.resource_manager"},{"location":"reference/ai2_kit/core/resource_manager/#variables","text":"ArtifactOrKey","title":"Variables"},{"location":"reference/ai2_kit/core/resource_manager/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/resource_manager/#resourcemanager","text":"class ResourceManager ( executor_configs : Mapping [ str , ai2_kit . core . executor . BaseExecutorConfig ], artifacts : Mapping [ str , ai2_kit . core . artifact . Artifact ], default_executor : str ) View Source class ResourceManager : @property def default_executor ( self ) : return self . get_executor () def __init__ ( self , executor_configs : ExecutorMap , artifacts : ArtifactMap , default_executor : str , ) -> None : self . _executor_configs = executor_configs self . _default_executor_name = default_executor self . _executors : Dict [ str, Executor ] = dict () # runtime check to ensure quick failure self . default_executor # fill in default values for key , artifact in artifacts . items () : if artifact . executor is None : artifact . executor = self . default_executor . name if artifact . key is None : artifact . key = key self . _artifacts = artifacts def get_executor ( self , name : Optional [ str ] = None ) -> Executor : if name is None : name = self . _default_executor_name config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) executor . init () self . _executors [ name ] = executor return self . _executors [ name ] def get_artifact ( self , key : str ) -> Artifact : # raise error it is by designed , ensure quick failure return self . _artifacts [ key ] def get_artifacts ( self , keys : List [ str ] ) -> List [ Artifact ] : return [ self.get_artifact(key) for key in keys ] def resolve_artifact ( self , artifact : ArtifactOrKey ) -> List [ Artifact ] : # TODO : support cross executor data resolve in the future if isinstance ( artifact , str ) : artifact = self . get_artifact ( artifact ) paths = self . default_executor . resolve_artifact ( artifact ) return [ Artifact.of( url=path, format=artifact.format, includes=None, # has been consumed attrs=copy.deepcopy(artifact.attrs), executor=self.default_executor.name, ) for path in paths ] def resolve_artifacts ( self , artifacts : Sequence [ ArtifactOrKey ] ) -> List [ Artifact ] : # flat map return [ x for a in artifacts for x in self.resolve_artifact(a) ]","title":"ResourceManager"},{"location":"reference/ai2_kit/core/resource_manager/#instance-variables","text":"default_executor","title":"Instance variables"},{"location":"reference/ai2_kit/core/resource_manager/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/resource_manager/#get_artifact","text":"def get_artifact ( self , key : str ) -> ai2_kit . core . artifact . Artifact View Source def get_artifact ( self , key : str ) -> Artifact : # raise error it is by designed , ensure quick failure return self . _artifacts [ key ]","title":"get_artifact"},{"location":"reference/ai2_kit/core/resource_manager/#get_artifacts","text":"def get_artifacts ( self , keys : List [ str ] ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_artifacts ( self , keys : List [ str ] ) -> List [ Artifact ] : return [ self.get_artifact(key) for key in keys ]","title":"get_artifacts"},{"location":"reference/ai2_kit/core/resource_manager/#get_executor","text":"def get_executor ( self , name : Union [ str , NoneType ] = None ) -> ai2_kit . core . executor . Executor View Source def get_executor ( self , name : Optional [ str ] = None ) -> Executor : if name is None : name = self . _default_executor_name config = self . _executor_configs . get ( name ) if config is None : raise ValueError ( 'Executor with name {} is not found!' . format ( name )) if name not in self . _executors : executor = create_executor ( config , name ) executor . init () self . _executors [ name ] = executor return self . _executors [ name ]","title":"get_executor"},{"location":"reference/ai2_kit/core/resource_manager/#resolve_artifact","text":"def resolve_artifact ( self , artifact : Union [ ai2_kit . core . artifact . Artifact , str ] ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def resolve_artifact ( self , artifact : ArtifactOrKey ) -> List [ Artifact ] : # TODO : support cross executor data resolve in the future if isinstance ( artifact , str ) : artifact = self . get_artifact ( artifact ) paths = self . default_executor . resolve_artifact ( artifact ) return [ Artifact.of( url=path, format=artifact.format, includes=None, # has been consumed attrs=copy.deepcopy(artifact.attrs), executor=self.default_executor.name, ) for path in paths ]","title":"resolve_artifact"},{"location":"reference/ai2_kit/core/resource_manager/#resolve_artifacts","text":"def resolve_artifacts ( self , artifacts : Sequence [ Union [ ai2_kit . core . artifact . Artifact , str ]] ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def resolve_artifacts ( self , artifacts : Sequence [ ArtifactOrKey ] ) -> List [ Artifact ] : # flat map return [ x for a in artifacts for x in self.resolve_artifact(a) ]","title":"resolve_artifacts"},{"location":"reference/ai2_kit/core/script/","text":"Module ai2_kit.core.script View Source from pydantic import BaseModel from typing import Optional , List , Union import shlex def exit_on_error_statment ( v = '__EXITCODE__' ): return f \" { v } =$?; if [ $ { v } -ne 0 ]; then exit $ { v } ; fi\" def eoe_step ( cmd : str ): \"\"\" exit on error step \"\"\" return ' \\n ' . join ([ cmd , exit_on_error_statment ()]) class BashTemplate ( BaseModel ): shebang : str = '#!/bin/bash' header : str = '' setup : str = '' teardown : str = '' class BashStep ( BaseModel ): cmd : Union [ str , List [ str ]] cwd : Optional [ str ] checkpoint : Optional [ str ] exit_on_error : bool = True def render ( self ): if isinstance ( self . cmd , list ): self . cmd = ' ' . join ( self . cmd ) rendered_step = ' \\n ' . join ([ '#' * 80 , self . cmd , '#' * 80 , ]) if self . exit_on_error : rendered_step = ' \\n ' . join ([ rendered_step , exit_on_error_statment () ]) if self . checkpoint : checkpoint = shlex . quote ( self . checkpoint + '.checkpoint' ) msg = shlex . quote ( f \"pass { checkpoint } \" ) rendered_step = ' \\n ' . join ([ f 'if [ -f { checkpoint } ]; then echo { msg } ; else' , rendered_step , f 'touch { checkpoint } ; fi # create checkpoint on success' , ]) if self . cwd : cwd = shlex . quote ( self . cwd ) rendered_step = ' \\n ' . join ([ f 'pushd { cwd } || exit 1' , rendered_step , 'popd' , ]) return rendered_step BashSteps = List [ Union [ str , BashStep ]] class BashScript ( BaseModel ): template : Optional [ BashTemplate ] steps : BashSteps def render ( self ): if self . template is None : return _render_bash_steps ( self . steps ) return ' \\n ' . join ([ self . template . shebang , self . template . header , self . template . setup , _render_bash_steps ( self . steps ), self . template . teardown , ]) def _render_bash_steps ( steps : BashSteps ): rendered_steps = [] for step in steps : if isinstance ( step , str ): rendered_steps . append ( step ) else : assert isinstance ( step , BashStep ) rendered_steps . append ( step . render ()) return ' \\n\\n ' . join ( rendered_steps ) Variables BashSteps Functions eoe_step def eoe_step ( cmd : str ) exit on error step View Source def eoe_step ( cmd : str ) : \"\" \" exit on error step \"\" \" return '\\n'.join([cmd, exit_on_error_statment()]) exit_on_error_statment def exit_on_error_statment ( v = '__EXITCODE__' ) View Source def exit_on_error_statment(v='__EXITCODE__'): return f\"{v}=$?; if [ ${ v } -ne 0 ]; then exit ${ v } ; fi\" Classes BashScript class BashScript ( __pydantic_self__ , ** data : Any ) View Source class BashScript ( BaseModel ) : template : Optional [ BashTemplate ] steps : BashSteps def render ( self ) : if self . template is None : return _render_bash_steps ( self . steps ) return '\\n' . join ( [ self.template.shebang, self.template.header, self.template.setup, _render_bash_steps(self.steps), self.template.teardown, ] ) Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . render def render ( self ) View Source def render ( self ) : if self . template is None : return _render_bash_steps ( self . steps ) return '\\n' . join ( [ self . template . shebang , self . template . header , self . template . setup , _render_bash_steps ( self . steps ) , self . template . teardown , ] ) BashStep class BashStep ( __pydantic_self__ , ** data : Any ) View Source class BashStep ( BaseModel ) : cmd : Union [ str, List[str ] ] cwd : Optional [ str ] checkpoint : Optional [ str ] exit_on_error : bool = True def render ( self ) : if isinstance ( self . cmd , list ) : self . cmd = ' ' . join ( self . cmd ) rendered_step = '\\n' . join ( [ '#' * 80, self.cmd, '#' * 80, ] ) if self . exit_on_error : rendered_step = '\\n' . join ( [ rendered_step, exit_on_error_statment() ] ) if self . checkpoint : checkpoint = shlex . quote ( self . checkpoint + '.checkpoint' ) msg = shlex . quote ( f \"pass {checkpoint}\" ) rendered_step = '\\n' . join ( [ f'if [ -f {checkpoint} ] ; then echo { msg } ; else ', rendered_step, f' touch { checkpoint } ; fi # create checkpoint on success ', ]) if self.cwd: cwd = shlex.quote(self.cwd) rendered_step = ' \\ n '.join([ f' pushd { cwd } || exit 1 ', rendered_step, ' popd ' , ] ) return rendered_step Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . render def render ( self ) View Source def render ( self ): if isinstance ( self . cmd , list ): self . cmd = ' ' . join ( self . cmd ) rendered_step = '\\n' . join ( [ '#' * 80 , self. cmd , '#' * 80 , ] ) if self . exit_on_error : rendered_step = '\\n' . join ( [ rendered_step , exit_on_error_statment () ] ) if self . checkpoint : checkpoint = shlex . quote ( self . checkpoint + '.checkpoint' ) msg = shlex . quote ( f \"pass {checkpoint}\" ) rendered_step = '\\n' . join ( [ f 'if [ -f {checkpoint} ]; then echo {msg}; else' , rendered_step , f 'touch {checkpoint}; fi # create checkpoint on success' , ] ) if self . cwd : cwd = shlex . quote ( self . cwd ) rendered_step = '\\n' . join ( [ f 'pushd {cwd} || exit 1' , rendered_step , 'popd' , ] ) return rendered_step BashTemplate class BashTemplate ( __pydantic_self__ , ** data : Any ) View Source class BashTemplate ( BaseModel ): shebang: str = '#!/bin/bash' header: str = '' setup: str = '' teardown: str = '' Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Script"},{"location":"reference/ai2_kit/core/script/#module-ai2_kitcorescript","text":"View Source from pydantic import BaseModel from typing import Optional , List , Union import shlex def exit_on_error_statment ( v = '__EXITCODE__' ): return f \" { v } =$?; if [ $ { v } -ne 0 ]; then exit $ { v } ; fi\" def eoe_step ( cmd : str ): \"\"\" exit on error step \"\"\" return ' \\n ' . join ([ cmd , exit_on_error_statment ()]) class BashTemplate ( BaseModel ): shebang : str = '#!/bin/bash' header : str = '' setup : str = '' teardown : str = '' class BashStep ( BaseModel ): cmd : Union [ str , List [ str ]] cwd : Optional [ str ] checkpoint : Optional [ str ] exit_on_error : bool = True def render ( self ): if isinstance ( self . cmd , list ): self . cmd = ' ' . join ( self . cmd ) rendered_step = ' \\n ' . join ([ '#' * 80 , self . cmd , '#' * 80 , ]) if self . exit_on_error : rendered_step = ' \\n ' . join ([ rendered_step , exit_on_error_statment () ]) if self . checkpoint : checkpoint = shlex . quote ( self . checkpoint + '.checkpoint' ) msg = shlex . quote ( f \"pass { checkpoint } \" ) rendered_step = ' \\n ' . join ([ f 'if [ -f { checkpoint } ]; then echo { msg } ; else' , rendered_step , f 'touch { checkpoint } ; fi # create checkpoint on success' , ]) if self . cwd : cwd = shlex . quote ( self . cwd ) rendered_step = ' \\n ' . join ([ f 'pushd { cwd } || exit 1' , rendered_step , 'popd' , ]) return rendered_step BashSteps = List [ Union [ str , BashStep ]] class BashScript ( BaseModel ): template : Optional [ BashTemplate ] steps : BashSteps def render ( self ): if self . template is None : return _render_bash_steps ( self . steps ) return ' \\n ' . join ([ self . template . shebang , self . template . header , self . template . setup , _render_bash_steps ( self . steps ), self . template . teardown , ]) def _render_bash_steps ( steps : BashSteps ): rendered_steps = [] for step in steps : if isinstance ( step , str ): rendered_steps . append ( step ) else : assert isinstance ( step , BashStep ) rendered_steps . append ( step . render ()) return ' \\n\\n ' . join ( rendered_steps )","title":"Module ai2_kit.core.script"},{"location":"reference/ai2_kit/core/script/#variables","text":"BashSteps","title":"Variables"},{"location":"reference/ai2_kit/core/script/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/script/#eoe_step","text":"def eoe_step ( cmd : str ) exit on error step View Source def eoe_step ( cmd : str ) : \"\" \" exit on error step \"\" \" return '\\n'.join([cmd, exit_on_error_statment()])","title":"eoe_step"},{"location":"reference/ai2_kit/core/script/#exit_on_error_statment","text":"def exit_on_error_statment ( v = '__EXITCODE__' ) View Source def exit_on_error_statment(v='__EXITCODE__'): return f\"{v}=$?; if [ ${ v } -ne 0 ]; then exit ${ v } ; fi\"","title":"exit_on_error_statment"},{"location":"reference/ai2_kit/core/script/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/script/#bashscript","text":"class BashScript ( __pydantic_self__ , ** data : Any ) View Source class BashScript ( BaseModel ) : template : Optional [ BashTemplate ] steps : BashSteps def render ( self ) : if self . template is None : return _render_bash_steps ( self . steps ) return '\\n' . join ( [ self.template.shebang, self.template.header, self.template.setup, _render_bash_steps(self.steps), self.template.teardown, ] )","title":"BashScript"},{"location":"reference/ai2_kit/core/script/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/script/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/core/script/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/script/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/script/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/script/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/script/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/script/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/script/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/script/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/script/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/script/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/script/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/script/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/script/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/script/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/script/#render","text":"def render ( self ) View Source def render ( self ) : if self . template is None : return _render_bash_steps ( self . steps ) return '\\n' . join ( [ self . template . shebang , self . template . header , self . template . setup , _render_bash_steps ( self . steps ) , self . template . teardown , ] )","title":"render"},{"location":"reference/ai2_kit/core/script/#bashstep","text":"class BashStep ( __pydantic_self__ , ** data : Any ) View Source class BashStep ( BaseModel ) : cmd : Union [ str, List[str ] ] cwd : Optional [ str ] checkpoint : Optional [ str ] exit_on_error : bool = True def render ( self ) : if isinstance ( self . cmd , list ) : self . cmd = ' ' . join ( self . cmd ) rendered_step = '\\n' . join ( [ '#' * 80, self.cmd, '#' * 80, ] ) if self . exit_on_error : rendered_step = '\\n' . join ( [ rendered_step, exit_on_error_statment() ] ) if self . checkpoint : checkpoint = shlex . quote ( self . checkpoint + '.checkpoint' ) msg = shlex . quote ( f \"pass {checkpoint}\" ) rendered_step = '\\n' . join ( [ f'if [ -f {checkpoint} ] ; then echo { msg } ; else ', rendered_step, f' touch { checkpoint } ; fi # create checkpoint on success ', ]) if self.cwd: cwd = shlex.quote(self.cwd) rendered_step = ' \\ n '.join([ f' pushd { cwd } || exit 1 ', rendered_step, ' popd ' , ] ) return rendered_step","title":"BashStep"},{"location":"reference/ai2_kit/core/script/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/script/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/core/script/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/script/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/script/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/script/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/script/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/script/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/script/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/script/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/script/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/script/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/script/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/script/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/script/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/script/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/script/#render_1","text":"def render ( self ) View Source def render ( self ): if isinstance ( self . cmd , list ): self . cmd = ' ' . join ( self . cmd ) rendered_step = '\\n' . join ( [ '#' * 80 , self. cmd , '#' * 80 , ] ) if self . exit_on_error : rendered_step = '\\n' . join ( [ rendered_step , exit_on_error_statment () ] ) if self . checkpoint : checkpoint = shlex . quote ( self . checkpoint + '.checkpoint' ) msg = shlex . quote ( f \"pass {checkpoint}\" ) rendered_step = '\\n' . join ( [ f 'if [ -f {checkpoint} ]; then echo {msg}; else' , rendered_step , f 'touch {checkpoint}; fi # create checkpoint on success' , ] ) if self . cwd : cwd = shlex . quote ( self . cwd ) rendered_step = '\\n' . join ( [ f 'pushd {cwd} || exit 1' , rendered_step , 'popd' , ] ) return rendered_step","title":"render"},{"location":"reference/ai2_kit/core/script/#bashtemplate","text":"class BashTemplate ( __pydantic_self__ , ** data : Any ) View Source class BashTemplate ( BaseModel ): shebang: str = '#!/bin/bash' header: str = '' setup: str = '' teardown: str = ''","title":"BashTemplate"},{"location":"reference/ai2_kit/core/script/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/core/script/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/core/script/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/script/#construct_2","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/core/script/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/core/script/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/core/script/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/core/script/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/core/script/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/core/script/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/core/script/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/core/script/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/core/script/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/core/script/#copy_2","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/core/script/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/core/script/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/core/util/","text":"Module ai2_kit.core.util View Source from ruamel . yaml import YAML from pathlib import Path from typing import Tuple , List , TypeVar from dataclasses import field from cp2k_input_tools . parser import CP2KInputParserSimplified import shortuuid import hashlib import base64 import copy import os import io from . log import get_logger logger = get_logger ( __ name__ ) EMPTY = object () def default_mutable_field ( obj ) : return field ( default_factory = lambda : copy . copy ( obj )) def __ merge_dict () : \"\"\"cloudpickle compatible: https://stackoverflow.com/questions/75292769\"\"\" def merge_dict ( lo : dict , ro : dict , path = None ) : \"\"\" merge two dict, the left dict will be overridden this method won't merge list \"\"\" if path is None : path = [] for key , val ue in ro . items () : if key in lo : current_path = path + [ str ( key ) ] if isinstance ( lo [ key ], dict ) and isinstance ( val ue , dict ) : merge_dict ( lo [ key ], val ue , current_path ) else : print ( '.' . join ( current_path ) + ' has been overridden' ) lo [ key ] = val ue else : lo [ key ] = val ue return lo return merge_dict merge_dict = __ merge_dict () # TODO : support http ( s ) url def load_yaml_file ( path : Path ) : yaml = YAML ( typ='safe' ) JoinTag . register ( yaml ) ReadTag . register ( yaml ) return yaml . load ( path ) def load_yaml_files ( * paths : Tuple [ Path ]) : d = {} for path in paths : print ( 'load yaml file: ' , path ) d = merge_dict ( d , load_yaml_file ( Path ( path ))) # type : ignore return d def s_uuid () : \"\"\"short uuid\"\"\" return shortuuid . uuid () # TODO : this should be moved out from core module def parse_cp2k_input ( text : str ) : parser = CP2KInputParserSimplified ( key_trafo = str . upper ) return parser . parse ( io . StringIO ( text )) def dict_nested_get ( d : dict , keys : List [ str ], default = EMPTY ) : \"\"\"get value from nested dict\"\"\" for key in keys : if key not in d and default is not EMPTY : return default d = d [ key ] return d def dict_nested_set ( d : dict , keys : List [ str ], val ue ) : \"\"\"set value to nested dict\"\"\" for key in keys [ :- 1 ] : d = d [ key ] d [ keys [ - 1 ]] = val ue def sort_unique_str_list ( l : List [ str ]) -> List [ str ] : \"\"\"remove duplicate str and sort\"\"\" return list ( sorted ( set ( l ))) T = TypeVar ( 'T' ) def flatten ( l : List [ List [ T ]]) -> List [ T ] : return [ item for sublist in l for item in sublist ] def format_env_string ( s : str ) -> str : return s . format ( **os . environ ) def split_list ( l : List [ T ], n : int ) -> List [ List [ T ]] : \"\"\"split list into n chunks\"\"\" # ref : https : // stackoverflow . com / questions / 2130016 / splitting - a - list - into - n - parts - of - approximately - equal - length k , m = divmod ( len ( l ), n ) return [ l [ i * k + min ( i , m ) : ( i + 1 ) * k + min ( i + 1 , m )] for i in range ( n )] def short_hash ( s : str ) -> str : \"\"\"short hash string\"\"\" digest = hashlib . sha1 ( s . encode ( 'utf-8' )). digest () # use urlsafe encode to avoid '/' in the string , as it will cause problem in file path return base64 . urlsafe_b64encode ( digest ). decode ( 'utf-8' )[ :- 2 ] async def to_awaitable ( val ue : T ) -> T : return val ue class JoinTag : \"\"\"a tag to join strings in a list\"\"\" yaml_tag = u'!join' @classmethod def from_yaml ( cls , constructor , node ) : seq = constructor . construct_sequence ( node ) return '' . join ([ str ( i ) for i in seq ]) @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls ) class ReadTag : \"\"\"a tag to read string from file\"\"\" yaml_tag = u'!read' @classmethod def from_yaml ( cls , constructor , node ) : seq = constructor . construct_sequence ( node ) path = os . path . join ( * seq ) with open ( path , 'r' ) as f : return f . read () @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls ) Variables EMPTY T logger Functions default_mutable_field def default_mutable_field ( obj ) View Source def default_mutable_field ( obj ) : return field ( default_factory = lambda : copy . copy ( obj )) dict_nested_get def dict_nested_get ( d : dict , keys : List [ str ], default =< object object at 0x7f2cd95184d0 > ) get value from nested dict View Source def dict_nested_get ( d : dict , keys : List [ str ] , default = EMPTY ) : \"\"\"get value from nested dict\"\"\" for key in keys : if key not in d and default is not EMPTY : return default d = d [ key ] return d dict_nested_set def dict_nested_set ( d : dict , keys : List [ str ], value ) set value to nested dict View Source def dict_nested_set ( d : dict , keys : List [ str ], val ue ) : \"\"\"set value to nested dict\"\"\" for key in keys [ :- 1 ] : d = d [ key ] d [ keys [ - 1 ]] = val ue flatten def flatten ( l : List [ List [ ~ T ]] ) -> List [ ~ T ] View Source def flatten ( l : List [ List[T ] ] ) -> List [ T ] : return [ item for sublist in l for item in sublist ] format_env_string def format_env_string ( s : str ) -> str View Source def format_env_string ( s : str ) -> str : return s . format ( ** os . environ ) load_yaml_file def load_yaml_file ( path : pathlib . Path ) View Source def load_yaml_file ( path : Path ): yaml = YAML ( typ = 'safe' ) JoinTag . register ( yaml ) ReadTag . register ( yaml ) return yaml . load ( path ) load_yaml_files def load_yaml_files ( * paths : Tuple [ pathlib . Path ] ) View Source def load_yaml_files ( * paths : Tuple [ Path ] ) : d = {} for path in paths : print ( 'load yaml file: ' , path ) d = merge_dict ( d , load_yaml_file ( Path ( path ))) # type : ignore return d merge_dict def merge_dict ( lo : dict , ro : dict , path = None ) merge two dict, the left dict will be overridden this method won't merge list View Source def merge_dict ( lo : dict , ro : dict , path = None ) : \"\"\" merge two dict, the left dict will be overridden this method won't merge list \"\"\" if path is None : path = [] for key , value in ro . items () : if key in lo : current_path = path + [ str(key) ] if isinstance ( lo [ key ] , dict ) and isinstance ( value , dict ) : merge_dict ( lo [ key ] , value , current_path ) else : print ( '.' . join ( current_path ) + ' has been overridden' ) lo [ key ] = value else : lo [ key ] = value return lo parse_cp2k_input def parse_cp2k_input ( text : str ) View Source def parse_cp2k_input ( text : str ): parser = CP2KInputParserSimplified ( key_trafo = str . upper ) return parser . parse ( io . StringIO ( text )) s_uuid def s_uuid ( ) short uuid View Source def s_uuid () : \"\"\"short uuid\"\"\" return shortuuid . uuid () short_hash def short_hash ( s : str ) -> str short hash string View Source def short_hash ( s : str ) -> str : \"\"\"short hash string\"\"\" digest = hashlib . sha1 ( s . encode ( 'utf-8' )). digest () # use urlsafe encode to avoid '/' in the string , as it will cause problem in file path return base64 . urlsafe_b64encode ( digest ). decode ( 'utf-8' )[ :- 2 ] sort_unique_str_list def sort_unique_str_list ( l : List [ str ] ) -> List [ str ] remove duplicate str and sort View Source def sort_unique_str_list ( l : List [ str ] ) -> List [ str ] : \"\"\"remove duplicate str and sort\"\"\" return list ( sorted ( set ( l ))) split_list def split_list ( l : List [ ~ T ], n : int ) -> List [ List [ ~ T ]] split list into n chunks View Source def split_list ( l : List [ T ] , n : int ) -> List [ List[T ] ]: \"\"\"split list into n chunks\"\"\" # ref : https : // stackoverflow . com / questions / 2130016 / splitting - a - list - into - n - parts - of - approximately - equal - length k , m = divmod ( len ( l ), n ) return [ l[i*k+min(i, m) : (i+1)*k+min(i+1, m) ] for i in range ( n ) ] to_awaitable def to_awaitable ( value : ~ T ) -> ~ T View Source async def to_awaitable ( value : T ) -> T : return value Classes JoinTag class JoinTag ( / , * args , ** kwargs ) a tag to join strings in a list View Source class JoinTag : \"\"\"a tag to join strings in a list\"\"\" yaml_tag = u '!join' @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) return '' . join ([ str ( i ) for i in seq ]) @ classmethod def to_yaml ( cls , dumper , data ): # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @ classmethod def register ( cls , yaml : YAML ): yaml . register_class ( cls ) Class variables yaml_tag Static methods from_yaml def from_yaml ( constructor , node ) View Source @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) return '' . join ([ str ( i ) for i in seq ]) register def register ( yaml : ruamel . yaml . main . YAML ) View Source @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls ) to_yaml def to_yaml ( dumper , data ) View Source @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) ReadTag class ReadTag ( / , * args , ** kwargs ) a tag to read string from file View Source class ReadTag : \"\"\"a tag to read string from file\"\"\" yaml_tag = u '!read' @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) path = os . path . join ( * seq ) with open ( path , 'r' ) as f : return f . read () @ classmethod def to_yaml ( cls , dumper , data ): # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @ classmethod def register ( cls , yaml : YAML ): yaml . register_class ( cls ) Class variables yaml_tag Static methods from_yaml def from_yaml ( constructor , node ) View Source @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) path = os . path . join ( * seq ) with open ( path , 'r' ) as f : return f . read () register def register ( yaml : ruamel . yaml . main . YAML ) View Source @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls ) to_yaml def to_yaml ( dumper , data ) View Source @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data )","title":"Util"},{"location":"reference/ai2_kit/core/util/#module-ai2_kitcoreutil","text":"View Source from ruamel . yaml import YAML from pathlib import Path from typing import Tuple , List , TypeVar from dataclasses import field from cp2k_input_tools . parser import CP2KInputParserSimplified import shortuuid import hashlib import base64 import copy import os import io from . log import get_logger logger = get_logger ( __ name__ ) EMPTY = object () def default_mutable_field ( obj ) : return field ( default_factory = lambda : copy . copy ( obj )) def __ merge_dict () : \"\"\"cloudpickle compatible: https://stackoverflow.com/questions/75292769\"\"\" def merge_dict ( lo : dict , ro : dict , path = None ) : \"\"\" merge two dict, the left dict will be overridden this method won't merge list \"\"\" if path is None : path = [] for key , val ue in ro . items () : if key in lo : current_path = path + [ str ( key ) ] if isinstance ( lo [ key ], dict ) and isinstance ( val ue , dict ) : merge_dict ( lo [ key ], val ue , current_path ) else : print ( '.' . join ( current_path ) + ' has been overridden' ) lo [ key ] = val ue else : lo [ key ] = val ue return lo return merge_dict merge_dict = __ merge_dict () # TODO : support http ( s ) url def load_yaml_file ( path : Path ) : yaml = YAML ( typ='safe' ) JoinTag . register ( yaml ) ReadTag . register ( yaml ) return yaml . load ( path ) def load_yaml_files ( * paths : Tuple [ Path ]) : d = {} for path in paths : print ( 'load yaml file: ' , path ) d = merge_dict ( d , load_yaml_file ( Path ( path ))) # type : ignore return d def s_uuid () : \"\"\"short uuid\"\"\" return shortuuid . uuid () # TODO : this should be moved out from core module def parse_cp2k_input ( text : str ) : parser = CP2KInputParserSimplified ( key_trafo = str . upper ) return parser . parse ( io . StringIO ( text )) def dict_nested_get ( d : dict , keys : List [ str ], default = EMPTY ) : \"\"\"get value from nested dict\"\"\" for key in keys : if key not in d and default is not EMPTY : return default d = d [ key ] return d def dict_nested_set ( d : dict , keys : List [ str ], val ue ) : \"\"\"set value to nested dict\"\"\" for key in keys [ :- 1 ] : d = d [ key ] d [ keys [ - 1 ]] = val ue def sort_unique_str_list ( l : List [ str ]) -> List [ str ] : \"\"\"remove duplicate str and sort\"\"\" return list ( sorted ( set ( l ))) T = TypeVar ( 'T' ) def flatten ( l : List [ List [ T ]]) -> List [ T ] : return [ item for sublist in l for item in sublist ] def format_env_string ( s : str ) -> str : return s . format ( **os . environ ) def split_list ( l : List [ T ], n : int ) -> List [ List [ T ]] : \"\"\"split list into n chunks\"\"\" # ref : https : // stackoverflow . com / questions / 2130016 / splitting - a - list - into - n - parts - of - approximately - equal - length k , m = divmod ( len ( l ), n ) return [ l [ i * k + min ( i , m ) : ( i + 1 ) * k + min ( i + 1 , m )] for i in range ( n )] def short_hash ( s : str ) -> str : \"\"\"short hash string\"\"\" digest = hashlib . sha1 ( s . encode ( 'utf-8' )). digest () # use urlsafe encode to avoid '/' in the string , as it will cause problem in file path return base64 . urlsafe_b64encode ( digest ). decode ( 'utf-8' )[ :- 2 ] async def to_awaitable ( val ue : T ) -> T : return val ue class JoinTag : \"\"\"a tag to join strings in a list\"\"\" yaml_tag = u'!join' @classmethod def from_yaml ( cls , constructor , node ) : seq = constructor . construct_sequence ( node ) return '' . join ([ str ( i ) for i in seq ]) @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls ) class ReadTag : \"\"\"a tag to read string from file\"\"\" yaml_tag = u'!read' @classmethod def from_yaml ( cls , constructor , node ) : seq = constructor . construct_sequence ( node ) path = os . path . join ( * seq ) with open ( path , 'r' ) as f : return f . read () @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls )","title":"Module ai2_kit.core.util"},{"location":"reference/ai2_kit/core/util/#variables","text":"EMPTY T logger","title":"Variables"},{"location":"reference/ai2_kit/core/util/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/core/util/#default_mutable_field","text":"def default_mutable_field ( obj ) View Source def default_mutable_field ( obj ) : return field ( default_factory = lambda : copy . copy ( obj ))","title":"default_mutable_field"},{"location":"reference/ai2_kit/core/util/#dict_nested_get","text":"def dict_nested_get ( d : dict , keys : List [ str ], default =< object object at 0x7f2cd95184d0 > ) get value from nested dict View Source def dict_nested_get ( d : dict , keys : List [ str ] , default = EMPTY ) : \"\"\"get value from nested dict\"\"\" for key in keys : if key not in d and default is not EMPTY : return default d = d [ key ] return d","title":"dict_nested_get"},{"location":"reference/ai2_kit/core/util/#dict_nested_set","text":"def dict_nested_set ( d : dict , keys : List [ str ], value ) set value to nested dict View Source def dict_nested_set ( d : dict , keys : List [ str ], val ue ) : \"\"\"set value to nested dict\"\"\" for key in keys [ :- 1 ] : d = d [ key ] d [ keys [ - 1 ]] = val ue","title":"dict_nested_set"},{"location":"reference/ai2_kit/core/util/#flatten","text":"def flatten ( l : List [ List [ ~ T ]] ) -> List [ ~ T ] View Source def flatten ( l : List [ List[T ] ] ) -> List [ T ] : return [ item for sublist in l for item in sublist ]","title":"flatten"},{"location":"reference/ai2_kit/core/util/#format_env_string","text":"def format_env_string ( s : str ) -> str View Source def format_env_string ( s : str ) -> str : return s . format ( ** os . environ )","title":"format_env_string"},{"location":"reference/ai2_kit/core/util/#load_yaml_file","text":"def load_yaml_file ( path : pathlib . Path ) View Source def load_yaml_file ( path : Path ): yaml = YAML ( typ = 'safe' ) JoinTag . register ( yaml ) ReadTag . register ( yaml ) return yaml . load ( path )","title":"load_yaml_file"},{"location":"reference/ai2_kit/core/util/#load_yaml_files","text":"def load_yaml_files ( * paths : Tuple [ pathlib . Path ] ) View Source def load_yaml_files ( * paths : Tuple [ Path ] ) : d = {} for path in paths : print ( 'load yaml file: ' , path ) d = merge_dict ( d , load_yaml_file ( Path ( path ))) # type : ignore return d","title":"load_yaml_files"},{"location":"reference/ai2_kit/core/util/#merge_dict","text":"def merge_dict ( lo : dict , ro : dict , path = None ) merge two dict, the left dict will be overridden this method won't merge list View Source def merge_dict ( lo : dict , ro : dict , path = None ) : \"\"\" merge two dict, the left dict will be overridden this method won't merge list \"\"\" if path is None : path = [] for key , value in ro . items () : if key in lo : current_path = path + [ str(key) ] if isinstance ( lo [ key ] , dict ) and isinstance ( value , dict ) : merge_dict ( lo [ key ] , value , current_path ) else : print ( '.' . join ( current_path ) + ' has been overridden' ) lo [ key ] = value else : lo [ key ] = value return lo","title":"merge_dict"},{"location":"reference/ai2_kit/core/util/#parse_cp2k_input","text":"def parse_cp2k_input ( text : str ) View Source def parse_cp2k_input ( text : str ): parser = CP2KInputParserSimplified ( key_trafo = str . upper ) return parser . parse ( io . StringIO ( text ))","title":"parse_cp2k_input"},{"location":"reference/ai2_kit/core/util/#s_uuid","text":"def s_uuid ( ) short uuid View Source def s_uuid () : \"\"\"short uuid\"\"\" return shortuuid . uuid ()","title":"s_uuid"},{"location":"reference/ai2_kit/core/util/#short_hash","text":"def short_hash ( s : str ) -> str short hash string View Source def short_hash ( s : str ) -> str : \"\"\"short hash string\"\"\" digest = hashlib . sha1 ( s . encode ( 'utf-8' )). digest () # use urlsafe encode to avoid '/' in the string , as it will cause problem in file path return base64 . urlsafe_b64encode ( digest ). decode ( 'utf-8' )[ :- 2 ]","title":"short_hash"},{"location":"reference/ai2_kit/core/util/#sort_unique_str_list","text":"def sort_unique_str_list ( l : List [ str ] ) -> List [ str ] remove duplicate str and sort View Source def sort_unique_str_list ( l : List [ str ] ) -> List [ str ] : \"\"\"remove duplicate str and sort\"\"\" return list ( sorted ( set ( l )))","title":"sort_unique_str_list"},{"location":"reference/ai2_kit/core/util/#split_list","text":"def split_list ( l : List [ ~ T ], n : int ) -> List [ List [ ~ T ]] split list into n chunks View Source def split_list ( l : List [ T ] , n : int ) -> List [ List[T ] ]: \"\"\"split list into n chunks\"\"\" # ref : https : // stackoverflow . com / questions / 2130016 / splitting - a - list - into - n - parts - of - approximately - equal - length k , m = divmod ( len ( l ), n ) return [ l[i*k+min(i, m) : (i+1)*k+min(i+1, m) ] for i in range ( n ) ]","title":"split_list"},{"location":"reference/ai2_kit/core/util/#to_awaitable","text":"def to_awaitable ( value : ~ T ) -> ~ T View Source async def to_awaitable ( value : T ) -> T : return value","title":"to_awaitable"},{"location":"reference/ai2_kit/core/util/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/core/util/#jointag","text":"class JoinTag ( / , * args , ** kwargs ) a tag to join strings in a list View Source class JoinTag : \"\"\"a tag to join strings in a list\"\"\" yaml_tag = u '!join' @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) return '' . join ([ str ( i ) for i in seq ]) @ classmethod def to_yaml ( cls , dumper , data ): # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @ classmethod def register ( cls , yaml : YAML ): yaml . register_class ( cls )","title":"JoinTag"},{"location":"reference/ai2_kit/core/util/#class-variables","text":"yaml_tag","title":"Class variables"},{"location":"reference/ai2_kit/core/util/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/util/#from_yaml","text":"def from_yaml ( constructor , node ) View Source @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) return '' . join ([ str ( i ) for i in seq ])","title":"from_yaml"},{"location":"reference/ai2_kit/core/util/#register","text":"def register ( yaml : ruamel . yaml . main . YAML ) View Source @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls )","title":"register"},{"location":"reference/ai2_kit/core/util/#to_yaml","text":"def to_yaml ( dumper , data ) View Source @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data )","title":"to_yaml"},{"location":"reference/ai2_kit/core/util/#readtag","text":"class ReadTag ( / , * args , ** kwargs ) a tag to read string from file View Source class ReadTag : \"\"\"a tag to read string from file\"\"\" yaml_tag = u '!read' @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) path = os . path . join ( * seq ) with open ( path , 'r' ) as f : return f . read () @ classmethod def to_yaml ( cls , dumper , data ): # do nothing return dumper . represent_sequence ( cls . yaml_tag , data ) @ classmethod def register ( cls , yaml : YAML ): yaml . register_class ( cls )","title":"ReadTag"},{"location":"reference/ai2_kit/core/util/#class-variables_1","text":"yaml_tag","title":"Class variables"},{"location":"reference/ai2_kit/core/util/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/core/util/#from_yaml_1","text":"def from_yaml ( constructor , node ) View Source @ classmethod def from_yaml ( cls , constructor , node ): seq = constructor . construct_sequence ( node ) path = os . path . join ( * seq ) with open ( path , 'r' ) as f : return f . read ()","title":"from_yaml"},{"location":"reference/ai2_kit/core/util/#register_1","text":"def register ( yaml : ruamel . yaml . main . YAML ) View Source @classmethod def register ( cls , yaml : YAML ) : yaml . register_class ( cls )","title":"register"},{"location":"reference/ai2_kit/core/util/#to_yaml_1","text":"def to_yaml ( dumper , data ) View Source @classmethod def to_yaml ( cls , dumper , data ) : # do nothing return dumper . represent_sequence ( cls . yaml_tag , data )","title":"to_yaml"},{"location":"reference/ai2_kit/domain/","text":"Module ai2_kit.domain Sub-modules ai2_kit.domain.cll ai2_kit.domain.constant ai2_kit.domain.cp2k ai2_kit.domain.data_helper ai2_kit.domain.deepmd ai2_kit.domain.lammps ai2_kit.domain.selector ai2_kit.domain.updater","title":"Index"},{"location":"reference/ai2_kit/domain/#module-ai2_kitdomain","text":"","title":"Module ai2_kit.domain"},{"location":"reference/ai2_kit/domain/#sub-modules","text":"ai2_kit.domain.cll ai2_kit.domain.constant ai2_kit.domain.cp2k ai2_kit.domain.data_helper ai2_kit.domain.deepmd ai2_kit.domain.lammps ai2_kit.domain.selector ai2_kit.domain.updater","title":"Sub-modules"},{"location":"reference/ai2_kit/domain/cll/","text":"Module ai2_kit.domain.cll View Source from abc import abstractmethod , ABC from ai2_kit.core.artifact import Artifact , ArtifactMap from ai2_kit.core.future import IFuture from ai2_kit.core.resource_manager import ResourceManager from typing import List , Callable , Any from dataclasses import dataclass @dataclass class BaseCllContext : path_prefix : str resource_manager : ResourceManager ###################### # CLL Labeling Tasks # ###################### class ICllLabelOutput ( ABC ): @abstractmethod def get_labeled_system_dataset ( self ) -> List [ Artifact ]: ... CllLabelTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllLabelOutput ]] ###################### # CLL Training Tasks # ###################### class ICllTrainOutput ( ABC ): @abstractmethod def get_mlp_models ( self ) -> List [ Artifact ]: ... @abstractmethod def get_training_dataset ( self ) -> List [ Artifact ]: ... CllTrainTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllTrainOutput ]] ###################### # CLL Explore Tasks # ###################### class ICllExploreOutput ( ABC ): @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ]: ... CllExploreTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllExploreOutput ]] ###################### # CLL Select Task # ###################### class ICllSelectorOutput ( ABC ): @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ]: ... @abstractmethod def get_passing_rate ( self ) -> float : ... CllSelectorTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllSelectorOutput ]] def init_artifacts ( artifacts : ArtifactMap ): for key , artifact in artifacts . items (): artifact . attrs . setdefault ( 'ancestor' , key ) Variables CllExploreTaskType CllLabelTaskType CllSelectorTaskType CllTrainTaskType Functions init_artifacts def init_artifacts ( artifacts : Mapping [ str , ai2_kit . core . artifact . Artifact ] ) View Source def init_artifacts ( artifacts : ArtifactMap ) : for key , artifact in artifacts . items () : artifact . attrs . setdefault ( 'ancestor' , key ) Classes BaseCllContext class BaseCllContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager ) BaseCllContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager) View Source class BaseCllContext: path_prefix: str resource_manager: ResourceManager Descendants ai2_kit.domain.cp2k.GenericCp2kContext ai2_kit.domain.deepmd.GenericDeepmdContext ai2_kit.domain.lammps.GenericLammpsContext ai2_kit.domain.selector.ThresholdSelectorContext ICllExploreOutput class ICllExploreOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllExploreOutput ( ABC ) : @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ... Ancestors (in MRO) abc.ABC Descendants ai2_kit.domain.lammps.GenericLammpsOutput Methods get_model_devi_dataset def get_model_devi_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ... ICllLabelOutput class ICllLabelOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllLabelOutput ( ABC ) : @abstractmethod def get_labeled_system_dataset ( self ) -> List [ Artifact ] : ... Ancestors (in MRO) abc.ABC Descendants ai2_kit.domain.cp2k.GenericCp2kOutput Methods get_labeled_system_dataset def get_labeled_system_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_labeled_system_dataset ( self ) -> List [ Artifact ] : ... ICllSelectorOutput class ICllSelectorOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllSelectorOutput ( ABC ) : @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ... @abstractmethod def get_passing_rate ( self ) -> float : ... Ancestors (in MRO) abc.ABC Descendants ai2_kit.domain.selector.ThresholdSelectorOutput Methods get_model_devi_dataset def get_model_devi_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ... get_passing_rate def get_passing_rate ( self ) -> float View Source @abstractmethod def get_passing_rate ( self ) -> float : ... ICllTrainOutput class ICllTrainOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllTrainOutput ( ABC ) : @abstractmethod def get_mlp_models ( self ) -> List [ Artifact ] : ... @abstractmethod def get_training_dataset ( self ) -> List [ Artifact ] : ... Ancestors (in MRO) abc.ABC Descendants ai2_kit.domain.deepmd.GenericDeepmdOutput Methods get_mlp_models def get_mlp_models ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_mlp_models ( self ) -> List [ Artifact ] : ... get_training_dataset def get_training_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_training_dataset ( self ) -> List [ Artifact ] : ...","title":"Cll"},{"location":"reference/ai2_kit/domain/cll/#module-ai2_kitdomaincll","text":"View Source from abc import abstractmethod , ABC from ai2_kit.core.artifact import Artifact , ArtifactMap from ai2_kit.core.future import IFuture from ai2_kit.core.resource_manager import ResourceManager from typing import List , Callable , Any from dataclasses import dataclass @dataclass class BaseCllContext : path_prefix : str resource_manager : ResourceManager ###################### # CLL Labeling Tasks # ###################### class ICllLabelOutput ( ABC ): @abstractmethod def get_labeled_system_dataset ( self ) -> List [ Artifact ]: ... CllLabelTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllLabelOutput ]] ###################### # CLL Training Tasks # ###################### class ICllTrainOutput ( ABC ): @abstractmethod def get_mlp_models ( self ) -> List [ Artifact ]: ... @abstractmethod def get_training_dataset ( self ) -> List [ Artifact ]: ... CllTrainTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllTrainOutput ]] ###################### # CLL Explore Tasks # ###################### class ICllExploreOutput ( ABC ): @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ]: ... CllExploreTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllExploreOutput ]] ###################### # CLL Select Task # ###################### class ICllSelectorOutput ( ABC ): @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ]: ... @abstractmethod def get_passing_rate ( self ) -> float : ... CllSelectorTaskType = Callable [[ Any , BaseCllContext ], IFuture [ ICllSelectorOutput ]] def init_artifacts ( artifacts : ArtifactMap ): for key , artifact in artifacts . items (): artifact . attrs . setdefault ( 'ancestor' , key )","title":"Module ai2_kit.domain.cll"},{"location":"reference/ai2_kit/domain/cll/#variables","text":"CllExploreTaskType CllLabelTaskType CllSelectorTaskType CllTrainTaskType","title":"Variables"},{"location":"reference/ai2_kit/domain/cll/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/domain/cll/#init_artifacts","text":"def init_artifacts ( artifacts : Mapping [ str , ai2_kit . core . artifact . Artifact ] ) View Source def init_artifacts ( artifacts : ArtifactMap ) : for key , artifact in artifacts . items () : artifact . attrs . setdefault ( 'ancestor' , key )","title":"init_artifacts"},{"location":"reference/ai2_kit/domain/cll/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/cll/#basecllcontext","text":"class BaseCllContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager ) BaseCllContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager) View Source class BaseCllContext: path_prefix: str resource_manager: ResourceManager","title":"BaseCllContext"},{"location":"reference/ai2_kit/domain/cll/#descendants","text":"ai2_kit.domain.cp2k.GenericCp2kContext ai2_kit.domain.deepmd.GenericDeepmdContext ai2_kit.domain.lammps.GenericLammpsContext ai2_kit.domain.selector.ThresholdSelectorContext","title":"Descendants"},{"location":"reference/ai2_kit/domain/cll/#icllexploreoutput","text":"class ICllExploreOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllExploreOutput ( ABC ) : @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ...","title":"ICllExploreOutput"},{"location":"reference/ai2_kit/domain/cll/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cll/#descendants_1","text":"ai2_kit.domain.lammps.GenericLammpsOutput","title":"Descendants"},{"location":"reference/ai2_kit/domain/cll/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cll/#get_model_devi_dataset","text":"def get_model_devi_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ...","title":"get_model_devi_dataset"},{"location":"reference/ai2_kit/domain/cll/#iclllabeloutput","text":"class ICllLabelOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllLabelOutput ( ABC ) : @abstractmethod def get_labeled_system_dataset ( self ) -> List [ Artifact ] : ...","title":"ICllLabelOutput"},{"location":"reference/ai2_kit/domain/cll/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cll/#descendants_2","text":"ai2_kit.domain.cp2k.GenericCp2kOutput","title":"Descendants"},{"location":"reference/ai2_kit/domain/cll/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cll/#get_labeled_system_dataset","text":"def get_labeled_system_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_labeled_system_dataset ( self ) -> List [ Artifact ] : ...","title":"get_labeled_system_dataset"},{"location":"reference/ai2_kit/domain/cll/#icllselectoroutput","text":"class ICllSelectorOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllSelectorOutput ( ABC ) : @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ... @abstractmethod def get_passing_rate ( self ) -> float : ...","title":"ICllSelectorOutput"},{"location":"reference/ai2_kit/domain/cll/#ancestors-in-mro_2","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cll/#descendants_3","text":"ai2_kit.domain.selector.ThresholdSelectorOutput","title":"Descendants"},{"location":"reference/ai2_kit/domain/cll/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cll/#get_model_devi_dataset_1","text":"def get_model_devi_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_model_devi_dataset ( self ) -> List [ Artifact ] : ...","title":"get_model_devi_dataset"},{"location":"reference/ai2_kit/domain/cll/#get_passing_rate","text":"def get_passing_rate ( self ) -> float View Source @abstractmethod def get_passing_rate ( self ) -> float : ...","title":"get_passing_rate"},{"location":"reference/ai2_kit/domain/cll/#iclltrainoutput","text":"class ICllTrainOutput ( / , * args , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. View Source class ICllTrainOutput ( ABC ) : @abstractmethod def get_mlp_models ( self ) -> List [ Artifact ] : ... @abstractmethod def get_training_dataset ( self ) -> List [ Artifact ] : ...","title":"ICllTrainOutput"},{"location":"reference/ai2_kit/domain/cll/#ancestors-in-mro_3","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cll/#descendants_4","text":"ai2_kit.domain.deepmd.GenericDeepmdOutput","title":"Descendants"},{"location":"reference/ai2_kit/domain/cll/#methods_3","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cll/#get_mlp_models","text":"def get_mlp_models ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_mlp_models ( self ) -> List [ Artifact ] : ...","title":"get_mlp_models"},{"location":"reference/ai2_kit/domain/cll/#get_training_dataset","text":"def get_training_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source @abstractmethod def get_training_dataset ( self ) -> List [ Artifact ] : ...","title":"get_training_dataset"},{"location":"reference/ai2_kit/domain/constant/","text":"Module ai2_kit.domain.constant View Source DP_CHECKPOINT_FILE = 'model.ckpt' DP_DISP_FILE = 'lcurve.out' DP_PROFILING_FILE = 'timeline.json' DP_INPUT_FILE = 'input.json' DP_FROZEN_MODEL = 'frozen_model.pb' MODEL_DEVI_OUT = 'model_devi.out' MODEL_DEVI_NEU_OUT = 'model_devi_neu.out' MODEL_DEVI_RED_OUT = 'model_devi_red.out' LAMMPS_TRAJ_DIR = 'traj' LAMMPS_TRAJ_SUFFIX = '.lammpstrj' Variables DP_CHECKPOINT_FILE DP_DISP_FILE DP_FROZEN_MODEL DP_INPUT_FILE DP_PROFILING_FILE LAMMPS_TRAJ_DIR LAMMPS_TRAJ_SUFFIX MODEL_DEVI_NEU_OUT MODEL_DEVI_OUT MODEL_DEVI_RED_OUT","title":"Constant"},{"location":"reference/ai2_kit/domain/constant/#module-ai2_kitdomainconstant","text":"View Source DP_CHECKPOINT_FILE = 'model.ckpt' DP_DISP_FILE = 'lcurve.out' DP_PROFILING_FILE = 'timeline.json' DP_INPUT_FILE = 'input.json' DP_FROZEN_MODEL = 'frozen_model.pb' MODEL_DEVI_OUT = 'model_devi.out' MODEL_DEVI_NEU_OUT = 'model_devi_neu.out' MODEL_DEVI_RED_OUT = 'model_devi_red.out' LAMMPS_TRAJ_DIR = 'traj' LAMMPS_TRAJ_SUFFIX = '.lammpstrj'","title":"Module ai2_kit.domain.constant"},{"location":"reference/ai2_kit/domain/constant/#variables","text":"DP_CHECKPOINT_FILE DP_DISP_FILE DP_FROZEN_MODEL DP_INPUT_FILE DP_PROFILING_FILE LAMMPS_TRAJ_DIR LAMMPS_TRAJ_SUFFIX MODEL_DEVI_NEU_OUT MODEL_DEVI_OUT MODEL_DEVI_RED_OUT","title":"Variables"},{"location":"reference/ai2_kit/domain/cp2k/","text":"Module ai2_kit.domain.cp2k View Source from ai2_kit.core.artifact import Artifact , ArtifactDict from ai2_kit.core.script import BashScript , BashStep , BashTemplate from ai2_kit.core.job import gather_jobs from ai2_kit.core.util import merge_dict , parse_cp2k_input , dict_nested_get , dict_nested_set , split_list from ai2_kit.core.log import get_logger from typing import List , Union , Tuple from pydantic import BaseModel from dataclasses import dataclass import copy import os from .data_helper import LammpsOutputHelper , XyzHelper , Cp2kOutputHelper , ase_atoms_to_cp2k_input_data from .cll import ICllLabelOutput , BaseCllContext logger = get_logger ( __name__ ) class GenericCp2kInputConfig ( BaseModel ): init_system_files : List [ str ] = [] limit : int = 50 input_template : Union [ dict , str ] \"\"\" Input template for cp2k. Could be a dict or content of a cp2k input file. Note: If you are using files in input templates, it is recommended to use artifact name instead of literal path. String starts with '@' will be treated as artifact name. For examples, FORCE_EVAL/DFT/BASIS_SET_FILE_NAME = @cp2k/basic_set. You can still use literal path, but it is not recommended. \"\"\" class GenericCp2kContextConfig ( BaseModel ): script_template : BashTemplate cp2k_cmd : str = 'cp2k' concurrency : int = 5 @dataclass class GenericCp2kInput : config : GenericCp2kInputConfig system_files : List [ Artifact ] type_map : List [ str ] initiated : bool = False @dataclass class GenericCp2kContext ( BaseCllContext ): config : GenericCp2kContextConfig @dataclass class GenericCp2kOutput ( ICllLabelOutput ): cp2k_outputs : List [ Artifact ] def get_labeled_system_dataset ( self ): return self . cp2k_outputs async def generic_cp2k ( input : GenericCp2kInput , ctx : GenericCp2kContext ) -> GenericCp2kOutput : executor = ctx . resource_manager . default_executor # For the first round if not input . initiated : input . system_files += ctx . resource_manager . get_artifacts ( input . config . init_system_files ) # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ tasks_dir ] = executor . setup_workspace ( work_dir , [ 'tasks' ]) # prepare input template if isinstance ( input . config . input_template , str ): input_template = parse_cp2k_input ( input . config . input_template ) else : input_template = copy . deepcopy ( input . config . input_template ) fields_with_artifact = [ [ 'FORCE_EVAL' , 'DFT' , 'BASIS_SET_FILE_NAME' ], [ 'FORCE_EVAL' , 'DFT' , 'POTENTIAL_FILE_NAME' ], [ 'FORCE_EVAL' , 'DFT' , 'XC' , 'VDW_POTENTIAL' , 'PAIR_POTENTIAL' , 'PARAMETER' , 'PARAMETER_FILE_NAME' ], ] for field_path in fields_with_artifact : try : field_value = dict_nested_get ( input_template , field_path ) if isinstance ( field_value , str ) and field_value . startswith ( '@' ): logger . info ( f 'resolve artifact { field_value } ' ) dict_nested_set ( input_template , field_path , ctx . resource_manager . resolve_artifact ( field_value [ 1 :])[ 0 ] . url ) except KeyError : pass # resolve data files lammps_dump_files : List [ Artifact ] = [] xyz_files : List [ Artifact ] = [] # TODO: support POSCAR in the future # TODO: refactor the way of handling different file formats system_files = ctx . resource_manager . resolve_artifacts ( input . system_files ) for system_file in system_files : if LammpsOutputHelper . is_match ( system_file ): lammps_out = LammpsOutputHelper ( system_file ) lammps_dump_files . extend ( lammps_out . get_passed_dump_files ()) elif XyzHelper . is_match ( system_file ): xyz_files . append ( system_file ) else : raise ValueError ( f 'unsupported format { system_file . url } : { system_file . format } ' ) # create task dirs and prepare input files cp2k_task_dirs = [] if lammps_dump_files or xyz_files : cp2k_task_dirs = executor . run_python_fn ( make_cp2k_task_dirs )( lammps_dump_files = [ a . to_dict () for a in lammps_dump_files ], xyz_files = [ a . to_dict () for a in xyz_files ], type_map = input . type_map , base_dir = tasks_dir , input_template = input_template , limit = input . config . limit , ) else : logger . warn ( 'no available candidates, skip' ) return GenericCp2kOutput ( cp2k_outputs = []) # build commands steps = [] for cp2k_task_dir in cp2k_task_dirs : steps . append ( BashStep ( cwd = cp2k_task_dir [ 'url' ], cmd = [ ctx . config . cp2k_cmd , '-i input.inp 1>> output 2>> output' ], checkpoint = 'cp2k' , )) # submit tasks and wait for completion jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )): if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/cp2k/ { i } : { tasks_dir } ' ) jobs . append ( job ) jobs = await gather_jobs ( jobs , max_tries = 2 ) cp2k_outputs = [ Artifact . of ( url = a [ 'url' ], format = Cp2kOutputHelper . format , executor = executor . name , attrs = a [ 'attrs' ], ) for a in cp2k_task_dirs ] return GenericCp2kOutput ( cp2k_outputs = cp2k_outputs ) def __make_cp2k_task_dirs (): def make_cp2k_task_dirs ( lammps_dump_files : List [ ArtifactDict ], xyz_files : List [ ArtifactDict ], type_map : List [ str ], input_template : dict , base_dir : str , limit : int = 0 , input_file_name : str = 'input.inp' , ) -> List [ ArtifactDict ]: \"\"\"Generate CP2K input files from LAMMPS dump files or XYZ files.\"\"\" from cp2k_input_tools import DEFAULT_CP2K_INPUT_XML from cp2k_input_tools.generator import CP2KInputGenerator import ase.io from ase import Atoms cp2k_generator = CP2KInputGenerator ( DEFAULT_CP2K_INPUT_XML ) task_dirs = [] atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] # read atoms for dump_file in lammps_dump_files : atoms_list += [ ( dump_file , atoms ) for atoms in ase . io . read ( dump_file [ 'url' ], ':' , format = 'lammps-dump-text' , order = False , specorder = type_map ) ] # type: ignore for xyz_file in xyz_files : atoms_list += [ ( xyz_file , atoms ) for atoms in ase . io . read ( xyz_file [ 'url' ], ':' , format = 'extxyz' ) ] # type: ignore if limit > 0 : atoms_list = atoms_list [: limit ] for i , ( file , atoms ) in enumerate ( atoms_list ): # create task dir task_dir = os . path . join ( base_dir , f ' { str ( i ) . zfill ( 6 ) } ' ) os . makedirs ( task_dir , exist_ok = True ) # create input file input_data = copy . deepcopy ( input_template ) coords , cell = ase_atoms_to_cp2k_input_data ( atoms ) merge_dict ( input_data , { 'FORCE_EVAL' : { 'SUBSYS' : { 'COORD' : { '*' : coords }, 'CELL' : { 'A' : cell [ 0 ], 'B' : cell [ 1 ], 'C' : cell [ 2 ], } } } }) input_text = ' \\n ' . join ( cp2k_generator . line_iter ( input_data )) with open ( os . path . join ( task_dir , input_file_name ), 'w' ) as f : f . write ( input_text ) # inherit attrs from input file # TODO: inherit only ancestor key should be enough task_dirs . append ({ 'url' : task_dir , 'attrs' : file [ 'attrs' ], }) return task_dirs return make_cp2k_task_dirs make_cp2k_task_dirs = __make_cp2k_task_dirs () Variables logger Functions generic_cp2k def generic_cp2k ( input : ai2_kit . domain . cp2k . GenericCp2kInput , ctx : ai2_kit . domain . cp2k . GenericCp2kContext ) -> ai2_kit . domain . cp2k . GenericCp2kOutput View Source async def generic_cp2k ( input : GenericCp2kInput , ctx : GenericCp2kContext ) -> GenericCp2kOutput : executor = ctx . resource_manager . default_executor # For the first round if not input . initiated : input . system_files += ctx . resource_manager . get_artifacts ( input . config . init_system_files ) # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ tasks_dir ] = executor . setup_workspace ( work_dir , [ 'tasks' ] ) # prepare input template if isinstance ( input . config . input_template , str ) : input_template = parse_cp2k_input ( input . config . input_template ) else : input_template = copy . deepcopy ( input . config . input_template ) fields_with_artifact = [ ['FORCE_EVAL', 'DFT', 'BASIS_SET_FILE_NAME' ] , [ 'FORCE_EVAL', 'DFT', 'POTENTIAL_FILE_NAME' ] , [ 'FORCE_EVAL', 'DFT', 'XC', 'VDW_POTENTIAL', 'PAIR_POTENTIAL', 'PARAMETER', 'PARAMETER_FILE_NAME' ] , ] for field_path in fields_with_artifact : try : field_value = dict_nested_get ( input_template , field_path ) if isinstance ( field_value , str ) and field_value . startswith ( '@' ) : logger . info ( f 'resolve artifact {field_value}' ) dict_nested_set ( input_template , field_path , ctx . resource_manager . resolve_artifact ( field_value [ 1: ] ) [ 0 ] . url ) except KeyError : pass # resolve data files lammps_dump_files : List [ Artifact ] = [] xyz_files : List [ Artifact ] = [] # TODO : support POSCAR in the future # TODO : refactor the way of handling different file formats system_files = ctx . resource_manager . resolve_artifacts ( input . system_files ) for system_file in system_files : if LammpsOutputHelper . is_match ( system_file ) : lammps_out = LammpsOutputHelper ( system_file ) lammps_dump_files . extend ( lammps_out . get_passed_dump_files ()) elif XyzHelper . is_match ( system_file ) : xyz_files . append ( system_file ) else : raise ValueError ( f 'unsupported format {system_file.url}: {system_file.format}' ) # create task dirs and prepare input files cp2k_task_dirs = [] if lammps_dump_files or xyz_files : cp2k_task_dirs = executor . run_python_fn ( make_cp2k_task_dirs )( lammps_dump_files =[ a.to_dict() for a in lammps_dump_files ] , xyz_files =[ a.to_dict() for a in xyz_files ] , type_map = input . type_map , base_dir = tasks_dir , input_template = input_template , limit = input . config . limit , ) else : logger . warn ( 'no available candidates, skip' ) return GenericCp2kOutput ( cp2k_outputs = [] ) # build commands steps = [] for cp2k_task_dir in cp2k_task_dirs : steps . append ( BashStep ( cwd = cp2k_task_dir [ 'url' ] , cmd =[ ctx.config.cp2k_cmd, '-i input.inp 1>> output 2>> output' ] , checkpoint = 'cp2k' , )) # submit tasks and wait for completion jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )) : if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/cp2k/{i}:{tasks_dir}' ) jobs . append ( job ) jobs = await gather_jobs ( jobs , max_tries = 2 ) cp2k_outputs = [ Artifact.of( url=a['url' ] , format = Cp2kOutputHelper . format , executor = executor . name , attrs = a [ 'attrs' ] , ) for a in cp2k_task_dirs ] return GenericCp2kOutput ( cp2k_outputs = cp2k_outputs ) make_cp2k_task_dirs def make_cp2k_task_dirs ( lammps_dump_files : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], xyz_files : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], type_map : List [ str ], input_template : dict , base_dir : str , limit : int = 0 , input_file_name : str = 'input.inp' ) -> List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ] Generate CP2K input files from LAMMPS dump files or XYZ files. View Source def make_cp2k_task_dirs ( lammps_dump_files : List [ ArtifactDict ], xyz_files : List [ ArtifactDict ], type_map : List [ str ], input_template : dict , base_dir : str , limit : int = 0 , input_file_name : str = 'input.inp' , ) -> List [ ArtifactDict ]: \"\"\"Generate CP2K input files from LAMMPS dump files or XYZ files.\"\"\" from cp2k_input_tools import DEFAULT_CP2K_INPUT_XML from cp2k_input_tools.generator import CP2KInputGenerator import ase.io from ase import Atoms cp2k_generator = CP2KInputGenerator ( DEFAULT_CP2K_INPUT_XML ) task_dirs = [] atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] # read atoms for dump_file in lammps_dump_files : atoms_list += [ ( dump_file , atoms ) for atoms in ase . io . read ( dump_file [ 'url' ], ':' , format = 'lammps-dump-text' , order = False , specorder = type_map ) ] # type: ignore for xyz_file in xyz_files : atoms_list += [ ( xyz_file , atoms ) for atoms in ase . io . read ( xyz_file [ 'url' ], ':' , format = 'extxyz' ) ] # type: ignore if limit > 0 : atoms_list = atoms_list [: limit ] for i , ( file , atoms ) in enumerate ( atoms_list ): # create task dir task_dir = os . path . join ( base_dir , f ' { str ( i ) . zfill ( 6 ) } ' ) os . makedirs ( task_dir , exist_ok = True ) # create input file input_data = copy . deepcopy ( input_template ) coords , cell = ase_atoms_to_cp2k_input_data ( atoms ) merge_dict ( input_data , { 'FORCE_EVAL' : { 'SUBSYS' : { 'COORD' : { '*' : coords }, 'CELL' : { 'A' : cell [ 0 ], 'B' : cell [ 1 ], 'C' : cell [ 2 ], } } } }) input_text = ' \\n ' . join ( cp2k_generator . line_iter ( input_data )) with open ( os . path . join ( task_dir , input_file_name ), 'w' ) as f : f . write ( input_text ) # inherit attrs from input file # TODO: inherit only ancestor key should be enough task_dirs . append ({ 'url' : task_dir , 'attrs' : file [ 'attrs' ], }) return task_dirs Classes GenericCp2kContext class GenericCp2kContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager , config : ai2_kit . domain . cp2k . GenericCp2kContextConfig ) GenericCp2kContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager, config: ai2_kit.domain.cp2k.GenericCp2kContextConfig) View Source class GenericCp2kContext ( BaseCllContext ): config: GenericCp2kContextConfig Ancestors (in MRO) ai2_kit.domain.cll.BaseCllContext GenericCp2kContextConfig class GenericCp2kContextConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericCp2kContextConfig ( BaseModel ): script_template: BashTemplate cp2k_cmd: str = 'cp2k' concurrency: int = 5 Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericCp2kInput class GenericCp2kInput ( config : ai2_kit . domain . cp2k . GenericCp2kInputConfig , system_files : List [ ai2_kit . core . artifact . Artifact ], type_map : List [ str ], initiated : bool = False ) GenericCp2kInput(config: ai2_kit.domain.cp2k.GenericCp2kInputConfig, system_files: List[ai2_kit.core.artifact.Artifact], type_map: List[str], initiated: bool = False) View Source class GenericCp2kInput : config : GenericCp2kInputConfig system_files : List [ Artifact ] type_map : List [ str ] initiated : bool = False Class variables initiated GenericCp2kInputConfig class GenericCp2kInputConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericCp2kInputConfig ( BaseModel ) : init_system_files : List [ str ] = [] limit : int = 50 input_template : Union [ dict, str ] \"\"\" Input template for cp2k. Could be a dict or content of a cp2k input file. Note: If you are using files in input templates, it is recommended to use artifact name instead of literal path. String starts with '@' will be treated as artifact name. For examples, FORCE_EVAL/DFT/BASIS_SET_FILE_NAME = @cp2k/basic_set. You can still use literal path, but it is not recommended. \"\"\" Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericCp2kOutput class GenericCp2kOutput ( cp2k_outputs : List [ ai2_kit . core . artifact . Artifact ] ) GenericCp2kOutput(cp2k_outputs: List[ai2_kit.core.artifact.Artifact]) View Source class GenericCp2kOutput ( ICllLabelOutput ) : cp2k_outputs : List [ Artifact ] def get_labeled_system_dataset ( self ) : return self . cp2k_outputs Ancestors (in MRO) ai2_kit.domain.cll.ICllLabelOutput abc.ABC Methods get_labeled_system_dataset def get_labeled_system_dataset ( self ) View Source def get_labeled_system_dataset ( self ) : return self . cp2k_outputs","title":"Cp2K"},{"location":"reference/ai2_kit/domain/cp2k/#module-ai2_kitdomaincp2k","text":"View Source from ai2_kit.core.artifact import Artifact , ArtifactDict from ai2_kit.core.script import BashScript , BashStep , BashTemplate from ai2_kit.core.job import gather_jobs from ai2_kit.core.util import merge_dict , parse_cp2k_input , dict_nested_get , dict_nested_set , split_list from ai2_kit.core.log import get_logger from typing import List , Union , Tuple from pydantic import BaseModel from dataclasses import dataclass import copy import os from .data_helper import LammpsOutputHelper , XyzHelper , Cp2kOutputHelper , ase_atoms_to_cp2k_input_data from .cll import ICllLabelOutput , BaseCllContext logger = get_logger ( __name__ ) class GenericCp2kInputConfig ( BaseModel ): init_system_files : List [ str ] = [] limit : int = 50 input_template : Union [ dict , str ] \"\"\" Input template for cp2k. Could be a dict or content of a cp2k input file. Note: If you are using files in input templates, it is recommended to use artifact name instead of literal path. String starts with '@' will be treated as artifact name. For examples, FORCE_EVAL/DFT/BASIS_SET_FILE_NAME = @cp2k/basic_set. You can still use literal path, but it is not recommended. \"\"\" class GenericCp2kContextConfig ( BaseModel ): script_template : BashTemplate cp2k_cmd : str = 'cp2k' concurrency : int = 5 @dataclass class GenericCp2kInput : config : GenericCp2kInputConfig system_files : List [ Artifact ] type_map : List [ str ] initiated : bool = False @dataclass class GenericCp2kContext ( BaseCllContext ): config : GenericCp2kContextConfig @dataclass class GenericCp2kOutput ( ICllLabelOutput ): cp2k_outputs : List [ Artifact ] def get_labeled_system_dataset ( self ): return self . cp2k_outputs async def generic_cp2k ( input : GenericCp2kInput , ctx : GenericCp2kContext ) -> GenericCp2kOutput : executor = ctx . resource_manager . default_executor # For the first round if not input . initiated : input . system_files += ctx . resource_manager . get_artifacts ( input . config . init_system_files ) # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ tasks_dir ] = executor . setup_workspace ( work_dir , [ 'tasks' ]) # prepare input template if isinstance ( input . config . input_template , str ): input_template = parse_cp2k_input ( input . config . input_template ) else : input_template = copy . deepcopy ( input . config . input_template ) fields_with_artifact = [ [ 'FORCE_EVAL' , 'DFT' , 'BASIS_SET_FILE_NAME' ], [ 'FORCE_EVAL' , 'DFT' , 'POTENTIAL_FILE_NAME' ], [ 'FORCE_EVAL' , 'DFT' , 'XC' , 'VDW_POTENTIAL' , 'PAIR_POTENTIAL' , 'PARAMETER' , 'PARAMETER_FILE_NAME' ], ] for field_path in fields_with_artifact : try : field_value = dict_nested_get ( input_template , field_path ) if isinstance ( field_value , str ) and field_value . startswith ( '@' ): logger . info ( f 'resolve artifact { field_value } ' ) dict_nested_set ( input_template , field_path , ctx . resource_manager . resolve_artifact ( field_value [ 1 :])[ 0 ] . url ) except KeyError : pass # resolve data files lammps_dump_files : List [ Artifact ] = [] xyz_files : List [ Artifact ] = [] # TODO: support POSCAR in the future # TODO: refactor the way of handling different file formats system_files = ctx . resource_manager . resolve_artifacts ( input . system_files ) for system_file in system_files : if LammpsOutputHelper . is_match ( system_file ): lammps_out = LammpsOutputHelper ( system_file ) lammps_dump_files . extend ( lammps_out . get_passed_dump_files ()) elif XyzHelper . is_match ( system_file ): xyz_files . append ( system_file ) else : raise ValueError ( f 'unsupported format { system_file . url } : { system_file . format } ' ) # create task dirs and prepare input files cp2k_task_dirs = [] if lammps_dump_files or xyz_files : cp2k_task_dirs = executor . run_python_fn ( make_cp2k_task_dirs )( lammps_dump_files = [ a . to_dict () for a in lammps_dump_files ], xyz_files = [ a . to_dict () for a in xyz_files ], type_map = input . type_map , base_dir = tasks_dir , input_template = input_template , limit = input . config . limit , ) else : logger . warn ( 'no available candidates, skip' ) return GenericCp2kOutput ( cp2k_outputs = []) # build commands steps = [] for cp2k_task_dir in cp2k_task_dirs : steps . append ( BashStep ( cwd = cp2k_task_dir [ 'url' ], cmd = [ ctx . config . cp2k_cmd , '-i input.inp 1>> output 2>> output' ], checkpoint = 'cp2k' , )) # submit tasks and wait for completion jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )): if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/cp2k/ { i } : { tasks_dir } ' ) jobs . append ( job ) jobs = await gather_jobs ( jobs , max_tries = 2 ) cp2k_outputs = [ Artifact . of ( url = a [ 'url' ], format = Cp2kOutputHelper . format , executor = executor . name , attrs = a [ 'attrs' ], ) for a in cp2k_task_dirs ] return GenericCp2kOutput ( cp2k_outputs = cp2k_outputs ) def __make_cp2k_task_dirs (): def make_cp2k_task_dirs ( lammps_dump_files : List [ ArtifactDict ], xyz_files : List [ ArtifactDict ], type_map : List [ str ], input_template : dict , base_dir : str , limit : int = 0 , input_file_name : str = 'input.inp' , ) -> List [ ArtifactDict ]: \"\"\"Generate CP2K input files from LAMMPS dump files or XYZ files.\"\"\" from cp2k_input_tools import DEFAULT_CP2K_INPUT_XML from cp2k_input_tools.generator import CP2KInputGenerator import ase.io from ase import Atoms cp2k_generator = CP2KInputGenerator ( DEFAULT_CP2K_INPUT_XML ) task_dirs = [] atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] # read atoms for dump_file in lammps_dump_files : atoms_list += [ ( dump_file , atoms ) for atoms in ase . io . read ( dump_file [ 'url' ], ':' , format = 'lammps-dump-text' , order = False , specorder = type_map ) ] # type: ignore for xyz_file in xyz_files : atoms_list += [ ( xyz_file , atoms ) for atoms in ase . io . read ( xyz_file [ 'url' ], ':' , format = 'extxyz' ) ] # type: ignore if limit > 0 : atoms_list = atoms_list [: limit ] for i , ( file , atoms ) in enumerate ( atoms_list ): # create task dir task_dir = os . path . join ( base_dir , f ' { str ( i ) . zfill ( 6 ) } ' ) os . makedirs ( task_dir , exist_ok = True ) # create input file input_data = copy . deepcopy ( input_template ) coords , cell = ase_atoms_to_cp2k_input_data ( atoms ) merge_dict ( input_data , { 'FORCE_EVAL' : { 'SUBSYS' : { 'COORD' : { '*' : coords }, 'CELL' : { 'A' : cell [ 0 ], 'B' : cell [ 1 ], 'C' : cell [ 2 ], } } } }) input_text = ' \\n ' . join ( cp2k_generator . line_iter ( input_data )) with open ( os . path . join ( task_dir , input_file_name ), 'w' ) as f : f . write ( input_text ) # inherit attrs from input file # TODO: inherit only ancestor key should be enough task_dirs . append ({ 'url' : task_dir , 'attrs' : file [ 'attrs' ], }) return task_dirs return make_cp2k_task_dirs make_cp2k_task_dirs = __make_cp2k_task_dirs ()","title":"Module ai2_kit.domain.cp2k"},{"location":"reference/ai2_kit/domain/cp2k/#variables","text":"logger","title":"Variables"},{"location":"reference/ai2_kit/domain/cp2k/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/domain/cp2k/#generic_cp2k","text":"def generic_cp2k ( input : ai2_kit . domain . cp2k . GenericCp2kInput , ctx : ai2_kit . domain . cp2k . GenericCp2kContext ) -> ai2_kit . domain . cp2k . GenericCp2kOutput View Source async def generic_cp2k ( input : GenericCp2kInput , ctx : GenericCp2kContext ) -> GenericCp2kOutput : executor = ctx . resource_manager . default_executor # For the first round if not input . initiated : input . system_files += ctx . resource_manager . get_artifacts ( input . config . init_system_files ) # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ tasks_dir ] = executor . setup_workspace ( work_dir , [ 'tasks' ] ) # prepare input template if isinstance ( input . config . input_template , str ) : input_template = parse_cp2k_input ( input . config . input_template ) else : input_template = copy . deepcopy ( input . config . input_template ) fields_with_artifact = [ ['FORCE_EVAL', 'DFT', 'BASIS_SET_FILE_NAME' ] , [ 'FORCE_EVAL', 'DFT', 'POTENTIAL_FILE_NAME' ] , [ 'FORCE_EVAL', 'DFT', 'XC', 'VDW_POTENTIAL', 'PAIR_POTENTIAL', 'PARAMETER', 'PARAMETER_FILE_NAME' ] , ] for field_path in fields_with_artifact : try : field_value = dict_nested_get ( input_template , field_path ) if isinstance ( field_value , str ) and field_value . startswith ( '@' ) : logger . info ( f 'resolve artifact {field_value}' ) dict_nested_set ( input_template , field_path , ctx . resource_manager . resolve_artifact ( field_value [ 1: ] ) [ 0 ] . url ) except KeyError : pass # resolve data files lammps_dump_files : List [ Artifact ] = [] xyz_files : List [ Artifact ] = [] # TODO : support POSCAR in the future # TODO : refactor the way of handling different file formats system_files = ctx . resource_manager . resolve_artifacts ( input . system_files ) for system_file in system_files : if LammpsOutputHelper . is_match ( system_file ) : lammps_out = LammpsOutputHelper ( system_file ) lammps_dump_files . extend ( lammps_out . get_passed_dump_files ()) elif XyzHelper . is_match ( system_file ) : xyz_files . append ( system_file ) else : raise ValueError ( f 'unsupported format {system_file.url}: {system_file.format}' ) # create task dirs and prepare input files cp2k_task_dirs = [] if lammps_dump_files or xyz_files : cp2k_task_dirs = executor . run_python_fn ( make_cp2k_task_dirs )( lammps_dump_files =[ a.to_dict() for a in lammps_dump_files ] , xyz_files =[ a.to_dict() for a in xyz_files ] , type_map = input . type_map , base_dir = tasks_dir , input_template = input_template , limit = input . config . limit , ) else : logger . warn ( 'no available candidates, skip' ) return GenericCp2kOutput ( cp2k_outputs = [] ) # build commands steps = [] for cp2k_task_dir in cp2k_task_dirs : steps . append ( BashStep ( cwd = cp2k_task_dir [ 'url' ] , cmd =[ ctx.config.cp2k_cmd, '-i input.inp 1>> output 2>> output' ] , checkpoint = 'cp2k' , )) # submit tasks and wait for completion jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )) : if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/cp2k/{i}:{tasks_dir}' ) jobs . append ( job ) jobs = await gather_jobs ( jobs , max_tries = 2 ) cp2k_outputs = [ Artifact.of( url=a['url' ] , format = Cp2kOutputHelper . format , executor = executor . name , attrs = a [ 'attrs' ] , ) for a in cp2k_task_dirs ] return GenericCp2kOutput ( cp2k_outputs = cp2k_outputs )","title":"generic_cp2k"},{"location":"reference/ai2_kit/domain/cp2k/#make_cp2k_task_dirs","text":"def make_cp2k_task_dirs ( lammps_dump_files : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], xyz_files : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], type_map : List [ str ], input_template : dict , base_dir : str , limit : int = 0 , input_file_name : str = 'input.inp' ) -> List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ] Generate CP2K input files from LAMMPS dump files or XYZ files. View Source def make_cp2k_task_dirs ( lammps_dump_files : List [ ArtifactDict ], xyz_files : List [ ArtifactDict ], type_map : List [ str ], input_template : dict , base_dir : str , limit : int = 0 , input_file_name : str = 'input.inp' , ) -> List [ ArtifactDict ]: \"\"\"Generate CP2K input files from LAMMPS dump files or XYZ files.\"\"\" from cp2k_input_tools import DEFAULT_CP2K_INPUT_XML from cp2k_input_tools.generator import CP2KInputGenerator import ase.io from ase import Atoms cp2k_generator = CP2KInputGenerator ( DEFAULT_CP2K_INPUT_XML ) task_dirs = [] atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] # read atoms for dump_file in lammps_dump_files : atoms_list += [ ( dump_file , atoms ) for atoms in ase . io . read ( dump_file [ 'url' ], ':' , format = 'lammps-dump-text' , order = False , specorder = type_map ) ] # type: ignore for xyz_file in xyz_files : atoms_list += [ ( xyz_file , atoms ) for atoms in ase . io . read ( xyz_file [ 'url' ], ':' , format = 'extxyz' ) ] # type: ignore if limit > 0 : atoms_list = atoms_list [: limit ] for i , ( file , atoms ) in enumerate ( atoms_list ): # create task dir task_dir = os . path . join ( base_dir , f ' { str ( i ) . zfill ( 6 ) } ' ) os . makedirs ( task_dir , exist_ok = True ) # create input file input_data = copy . deepcopy ( input_template ) coords , cell = ase_atoms_to_cp2k_input_data ( atoms ) merge_dict ( input_data , { 'FORCE_EVAL' : { 'SUBSYS' : { 'COORD' : { '*' : coords }, 'CELL' : { 'A' : cell [ 0 ], 'B' : cell [ 1 ], 'C' : cell [ 2 ], } } } }) input_text = ' \\n ' . join ( cp2k_generator . line_iter ( input_data )) with open ( os . path . join ( task_dir , input_file_name ), 'w' ) as f : f . write ( input_text ) # inherit attrs from input file # TODO: inherit only ancestor key should be enough task_dirs . append ({ 'url' : task_dir , 'attrs' : file [ 'attrs' ], }) return task_dirs","title":"make_cp2k_task_dirs"},{"location":"reference/ai2_kit/domain/cp2k/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/cp2k/#genericcp2kcontext","text":"class GenericCp2kContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager , config : ai2_kit . domain . cp2k . GenericCp2kContextConfig ) GenericCp2kContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager, config: ai2_kit.domain.cp2k.GenericCp2kContextConfig) View Source class GenericCp2kContext ( BaseCllContext ): config: GenericCp2kContextConfig","title":"GenericCp2kContext"},{"location":"reference/ai2_kit/domain/cp2k/#ancestors-in-mro","text":"ai2_kit.domain.cll.BaseCllContext","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cp2k/#genericcp2kcontextconfig","text":"class GenericCp2kContextConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericCp2kContextConfig ( BaseModel ): script_template: BashTemplate cp2k_cmd: str = 'cp2k' concurrency: int = 5","title":"GenericCp2kContextConfig"},{"location":"reference/ai2_kit/domain/cp2k/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cp2k/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/cp2k/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/cp2k/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/cp2k/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/cp2k/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/cp2k/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/cp2k/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/cp2k/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/cp2k/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/cp2k/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/cp2k/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/cp2k/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cp2k/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/cp2k/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/cp2k/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/cp2k/#genericcp2kinput","text":"class GenericCp2kInput ( config : ai2_kit . domain . cp2k . GenericCp2kInputConfig , system_files : List [ ai2_kit . core . artifact . Artifact ], type_map : List [ str ], initiated : bool = False ) GenericCp2kInput(config: ai2_kit.domain.cp2k.GenericCp2kInputConfig, system_files: List[ai2_kit.core.artifact.Artifact], type_map: List[str], initiated: bool = False) View Source class GenericCp2kInput : config : GenericCp2kInputConfig system_files : List [ Artifact ] type_map : List [ str ] initiated : bool = False","title":"GenericCp2kInput"},{"location":"reference/ai2_kit/domain/cp2k/#class-variables_1","text":"initiated","title":"Class variables"},{"location":"reference/ai2_kit/domain/cp2k/#genericcp2kinputconfig","text":"class GenericCp2kInputConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericCp2kInputConfig ( BaseModel ) : init_system_files : List [ str ] = [] limit : int = 50 input_template : Union [ dict, str ] \"\"\" Input template for cp2k. Could be a dict or content of a cp2k input file. Note: If you are using files in input templates, it is recommended to use artifact name instead of literal path. String starts with '@' will be treated as artifact name. For examples, FORCE_EVAL/DFT/BASIS_SET_FILE_NAME = @cp2k/basic_set. You can still use literal path, but it is not recommended. \"\"\"","title":"GenericCp2kInputConfig"},{"location":"reference/ai2_kit/domain/cp2k/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cp2k/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/cp2k/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/cp2k/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/cp2k/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/cp2k/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/cp2k/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/cp2k/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/cp2k/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/cp2k/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/cp2k/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/cp2k/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/cp2k/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cp2k/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/cp2k/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/cp2k/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/cp2k/#genericcp2koutput","text":"class GenericCp2kOutput ( cp2k_outputs : List [ ai2_kit . core . artifact . Artifact ] ) GenericCp2kOutput(cp2k_outputs: List[ai2_kit.core.artifact.Artifact]) View Source class GenericCp2kOutput ( ICllLabelOutput ) : cp2k_outputs : List [ Artifact ] def get_labeled_system_dataset ( self ) : return self . cp2k_outputs","title":"GenericCp2kOutput"},{"location":"reference/ai2_kit/domain/cp2k/#ancestors-in-mro_3","text":"ai2_kit.domain.cll.ICllLabelOutput abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/cp2k/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/cp2k/#get_labeled_system_dataset","text":"def get_labeled_system_dataset ( self ) View Source def get_labeled_system_dataset ( self ) : return self . cp2k_outputs","title":"get_labeled_system_dataset"},{"location":"reference/ai2_kit/domain/data_helper/","text":"Module ai2_kit.domain.data_helper View Source from ai2_kit.core.artifact import Artifact , ArtifactDict from typing import List , Tuple , Optional from ase import Atoms import os import re from .constant import MODEL_DEVI_OUT , LAMMPS_TRAJ_DIR , LAMMPS_TRAJ_SUFFIX class DataHelper : format : Optional [ str ] = None suffix : Optional [ str ] = None pattern : Optional [ re . Pattern ] = None @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ): return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ): return True return False def __init__ ( self , artifact : Artifact ) -> None : self . artifact = artifact class LammpsOutputHelper ( DataHelper ): format = 'lammps/output-dir' def get_model_devi_file ( self , filename : str ) -> Artifact : return self . artifact . join ( filename ) def get_passed_dump_files ( self ) -> List [ Artifact ]: return [ self . artifact . join ( LAMMPS_TRAJ_DIR , f ' { i }{ LAMMPS_TRAJ_SUFFIX } ' ) for i in self . artifact . attrs [ 'passed' ]] class PoscarHelper ( DataHelper ): format = 'vasp/poscar' pattern = re . compile ( r 'POSCAR' ) class XyzHelper ( DataHelper ): format = 'extxyz' suffix = '.xyz' class DeepmdNpyHelper ( DataHelper ): format = 'deepmd/npy' class DeepmdModelHelper ( DataHelper ): format = 'deepmd/model' class Cp2kOutputHelper ( DataHelper ): format = 'cp2k-output-dir' def __ase_atoms_to_cp2k_input_data (): \"\"\"workaround for cloudpickle issue\"\"\" def ase_atoms_to_cp2k_input_data ( atoms : Atoms ) -> Tuple [ List [ str ], List [ List [ float ]]]: \"\"\" Convert ASE Atoms to CP2K input format \"\"\" coords = [ atom . symbol + ' ' + ' ' . join ( str ( x ) for x in atom . position ) for atom in atoms ] # type: ignore cell = [ list ( row ) for row in atoms . cell ] # type: ignore return ( coords , cell ) return ase_atoms_to_cp2k_input_data ase_atoms_to_cp2k_input_data = __ase_atoms_to_cp2k_input_data () def __convert_to_deepmd_npy (): \"\"\"workaround for cloudpickle issue\"\"\" def covert_to_deepmd_npy ( cp2k_outputs : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata from itertools import groupby atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] for cp2k_output in cp2k_outputs : dp_system = dpdata . LabeledSystem ( os . path . join ( cp2k_output [ 'url' ], 'output' ), fmt = 'cp2k/output' , type_map = type_map ) atoms_list += [ ( cp2k_output , atoms ) for atoms in dp_system . to_ase_structure () # type: ignore ] output_dirs = [] # group dataset by ancestor key for i , ( key , atoms_group ) in enumerate ( groupby ( atoms_list , key = lambda x : x [ 0 ][ 'attrs' ][ 'ancestor' ])): output_dir = os . path . join ( base_dir , key . replace ( '/' , '_' )) atoms_group = list ( item [ 1 ] for item in atoms_group ) if not atoms_group : continue dp_system = dpdata . LabeledSystem ( atoms_group [ 0 ], fmt = 'ase/structure' ) for atoms in atoms_group [ 1 :]: dp_system += dpdata . LabeledSystem ( atoms , fmt = 'ase/structure' ) dp_system . to_deepmd_npy ( output_dir , set_size = len ( dp_system )) # type: ignore # TODO: return ArtifactDict output_dirs . append ( output_dir ) return output_dirs return covert_to_deepmd_npy convert_to_deepmd_npy = __convert_to_deepmd_npy () def __convert_to_lammps_input_data (): \"\"\"workaround for cloudpickle issue\"\"\" def convert_to_lammps_input_data ( poscar_files : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata import os lammps_data_files = [] for i , poscar_file in enumerate ( poscar_files ): output_file = os . path . join ( base_dir , f ' { i : 06d } .lammps.data' ) dpdata . System ( poscar_file [ 'url' ], fmt = 'vasp/poscar' , type_map = type_map ) . to_lammps_lmp ( output_file ) # type: ignore lammps_data_files . append ({ 'url' : output_file , 'attrs' : poscar_file [ 'attrs' ], }) return lammps_data_files return convert_to_lammps_input_data convert_to_lammps_input_data = __convert_to_lammps_input_data () Variables LAMMPS_TRAJ_DIR LAMMPS_TRAJ_SUFFIX MODEL_DEVI_OUT Functions ase_atoms_to_cp2k_input_data def ase_atoms_to_cp2k_input_data ( atoms : ase . atoms . Atoms ) -> Tuple [ List [ str ], List [ List [ float ]]] Convert ASE Atoms to CP2K input format View Source def ase_atoms_to_cp2k_input_data ( atoms : Atoms ) -> Tuple [ List[str ] , List [ List[float ] ]]: \"\"\" Convert ASE Atoms to CP2K input format \"\"\" coords = [ atom.symbol + ' ' + ' '.join(str(x) for x in atom.position) for atom in atoms ] # type : ignore cell = [ list(row) for row in atoms.cell ] # type : ignore return ( coords , cell ) convert_to_deepmd_npy def convert_to_deepmd_npy ( cp2k_outputs : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], base_dir : str , type_map : List [ str ] ) View Source def covert_to_deepmd_npy ( cp2k_outputs : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata from itertools import groupby atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] for cp2k_output in cp2k_outputs : dp_system = dpdata . LabeledSystem ( os . path . join ( cp2k_output [ 'url' ], 'output' ), fmt = 'cp2k/output' , type_map = type_map ) atoms_list += [ ( cp2k_output , atoms ) for atoms in dp_system . to_ase_structure () # type: ignore ] output_dirs = [] # group dataset by ancestor key for i , ( key , atoms_group ) in enumerate ( groupby ( atoms_list , key = lambda x : x [ 0 ][ 'attrs' ][ 'ancestor' ])): output_dir = os . path . join ( base_dir , key . replace ( '/' , '_' )) atoms_group = list ( item [ 1 ] for item in atoms_group ) if not atoms_group : continue dp_system = dpdata . LabeledSystem ( atoms_group [ 0 ], fmt = 'ase/structure' ) for atoms in atoms_group [ 1 :]: dp_system += dpdata . LabeledSystem ( atoms , fmt = 'ase/structure' ) dp_system . to_deepmd_npy ( output_dir , set_size = len ( dp_system )) # type: ignore # TODO: return ArtifactDict output_dirs . append ( output_dir ) return output_dirs convert_to_lammps_input_data def convert_to_lammps_input_data ( poscar_files : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], base_dir : str , type_map : List [ str ] ) View Source def convert_to_lammps_input_data ( poscar_files : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata import os lammps_data_files = [] for i , poscar_file in enumerate ( poscar_files ): output_file = os . path . join ( base_dir , f ' { i : 06d } .lammps.data' ) dpdata . System ( poscar_file [ 'url' ], fmt = 'vasp/poscar' , type_map = type_map ) . to_lammps_lmp ( output_file ) # type: ignore lammps_data_files . append ({ 'url' : output_file , 'attrs' : poscar_file [ 'attrs' ], }) return lammps_data_files Classes Cp2kOutputHelper class Cp2kOutputHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class Cp2kOutputHelper ( DataHelper ): format = 'cp2k-output-dir' Ancestors (in MRO) ai2_kit.domain.data_helper.DataHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False DataHelper class DataHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class DataHelper : format : Optional [ str ] = None suffix : Optional [ str ] = None pattern : Optional [ re.Pattern ] = None @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False def __init__ ( self , artifact : Artifact ) -> None : self . artifact = artifact Descendants ai2_kit.domain.data_helper.LammpsOutputHelper ai2_kit.domain.data_helper.PoscarHelper ai2_kit.domain.data_helper.XyzHelper ai2_kit.domain.data_helper.DeepmdNpyHelper ai2_kit.domain.data_helper.DeepmdModelHelper ai2_kit.domain.data_helper.Cp2kOutputHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False DeepmdModelHelper class DeepmdModelHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class DeepmdModelHelper ( DataHelper ): format = 'deepmd/model' Ancestors (in MRO) ai2_kit.domain.data_helper.DataHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False DeepmdNpyHelper class DeepmdNpyHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class DeepmdNpyHelper ( DataHelper ): format = 'deepmd/npy' Ancestors (in MRO) ai2_kit.domain.data_helper.DataHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False LammpsOutputHelper class LammpsOutputHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class LammpsOutputHelper ( DataHelper ) : format = 'lammps/output-dir' def get_model_devi_file ( self , filename : str ) -> Artifact : return self . artifact . join ( filename ) def get_passed_dump_files ( self ) -> List [ Artifact ] : return [ self.artifact.join(LAMMPS_TRAJ_DIR, f'{i}{LAMMPS_TRAJ_SUFFIX}') for i in self.artifact.attrs['passed' ] ] Ancestors (in MRO) ai2_kit.domain.data_helper.DataHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False Methods get_model_devi_file def get_model_devi_file ( self , filename : str ) -> ai2_kit . core . artifact . Artifact View Source def get_model_devi_file ( self , filename : str ) -> Artifact : return self . artifact . join ( filename ) get_passed_dump_files def get_passed_dump_files ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_passed_dump_files ( self ) -> List [ Artifact ] : return [ self.artifact.join(LAMMPS_TRAJ_DIR, f'{i}{LAMMPS_TRAJ_SUFFIX}') for i in self.artifact.attrs['passed' ] ] PoscarHelper class PoscarHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class PoscarHelper ( DataHelper ): format = 'vasp/poscar' pattern = re . compile ( r'POSCAR' ) Ancestors (in MRO) ai2_kit.domain.data_helper.DataHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False XyzHelper class XyzHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class XyzHelper ( DataHelper ): format = 'extxyz' suffix = '.xyz' Ancestors (in MRO) ai2_kit.domain.data_helper.DataHelper Class variables format pattern suffix Static methods is_match def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"Data Helper"},{"location":"reference/ai2_kit/domain/data_helper/#module-ai2_kitdomaindata_helper","text":"View Source from ai2_kit.core.artifact import Artifact , ArtifactDict from typing import List , Tuple , Optional from ase import Atoms import os import re from .constant import MODEL_DEVI_OUT , LAMMPS_TRAJ_DIR , LAMMPS_TRAJ_SUFFIX class DataHelper : format : Optional [ str ] = None suffix : Optional [ str ] = None pattern : Optional [ re . Pattern ] = None @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ): return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ): return True return False def __init__ ( self , artifact : Artifact ) -> None : self . artifact = artifact class LammpsOutputHelper ( DataHelper ): format = 'lammps/output-dir' def get_model_devi_file ( self , filename : str ) -> Artifact : return self . artifact . join ( filename ) def get_passed_dump_files ( self ) -> List [ Artifact ]: return [ self . artifact . join ( LAMMPS_TRAJ_DIR , f ' { i }{ LAMMPS_TRAJ_SUFFIX } ' ) for i in self . artifact . attrs [ 'passed' ]] class PoscarHelper ( DataHelper ): format = 'vasp/poscar' pattern = re . compile ( r 'POSCAR' ) class XyzHelper ( DataHelper ): format = 'extxyz' suffix = '.xyz' class DeepmdNpyHelper ( DataHelper ): format = 'deepmd/npy' class DeepmdModelHelper ( DataHelper ): format = 'deepmd/model' class Cp2kOutputHelper ( DataHelper ): format = 'cp2k-output-dir' def __ase_atoms_to_cp2k_input_data (): \"\"\"workaround for cloudpickle issue\"\"\" def ase_atoms_to_cp2k_input_data ( atoms : Atoms ) -> Tuple [ List [ str ], List [ List [ float ]]]: \"\"\" Convert ASE Atoms to CP2K input format \"\"\" coords = [ atom . symbol + ' ' + ' ' . join ( str ( x ) for x in atom . position ) for atom in atoms ] # type: ignore cell = [ list ( row ) for row in atoms . cell ] # type: ignore return ( coords , cell ) return ase_atoms_to_cp2k_input_data ase_atoms_to_cp2k_input_data = __ase_atoms_to_cp2k_input_data () def __convert_to_deepmd_npy (): \"\"\"workaround for cloudpickle issue\"\"\" def covert_to_deepmd_npy ( cp2k_outputs : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata from itertools import groupby atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] for cp2k_output in cp2k_outputs : dp_system = dpdata . LabeledSystem ( os . path . join ( cp2k_output [ 'url' ], 'output' ), fmt = 'cp2k/output' , type_map = type_map ) atoms_list += [ ( cp2k_output , atoms ) for atoms in dp_system . to_ase_structure () # type: ignore ] output_dirs = [] # group dataset by ancestor key for i , ( key , atoms_group ) in enumerate ( groupby ( atoms_list , key = lambda x : x [ 0 ][ 'attrs' ][ 'ancestor' ])): output_dir = os . path . join ( base_dir , key . replace ( '/' , '_' )) atoms_group = list ( item [ 1 ] for item in atoms_group ) if not atoms_group : continue dp_system = dpdata . LabeledSystem ( atoms_group [ 0 ], fmt = 'ase/structure' ) for atoms in atoms_group [ 1 :]: dp_system += dpdata . LabeledSystem ( atoms , fmt = 'ase/structure' ) dp_system . to_deepmd_npy ( output_dir , set_size = len ( dp_system )) # type: ignore # TODO: return ArtifactDict output_dirs . append ( output_dir ) return output_dirs return covert_to_deepmd_npy convert_to_deepmd_npy = __convert_to_deepmd_npy () def __convert_to_lammps_input_data (): \"\"\"workaround for cloudpickle issue\"\"\" def convert_to_lammps_input_data ( poscar_files : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata import os lammps_data_files = [] for i , poscar_file in enumerate ( poscar_files ): output_file = os . path . join ( base_dir , f ' { i : 06d } .lammps.data' ) dpdata . System ( poscar_file [ 'url' ], fmt = 'vasp/poscar' , type_map = type_map ) . to_lammps_lmp ( output_file ) # type: ignore lammps_data_files . append ({ 'url' : output_file , 'attrs' : poscar_file [ 'attrs' ], }) return lammps_data_files return convert_to_lammps_input_data convert_to_lammps_input_data = __convert_to_lammps_input_data ()","title":"Module ai2_kit.domain.data_helper"},{"location":"reference/ai2_kit/domain/data_helper/#variables","text":"LAMMPS_TRAJ_DIR LAMMPS_TRAJ_SUFFIX MODEL_DEVI_OUT","title":"Variables"},{"location":"reference/ai2_kit/domain/data_helper/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/domain/data_helper/#ase_atoms_to_cp2k_input_data","text":"def ase_atoms_to_cp2k_input_data ( atoms : ase . atoms . Atoms ) -> Tuple [ List [ str ], List [ List [ float ]]] Convert ASE Atoms to CP2K input format View Source def ase_atoms_to_cp2k_input_data ( atoms : Atoms ) -> Tuple [ List[str ] , List [ List[float ] ]]: \"\"\" Convert ASE Atoms to CP2K input format \"\"\" coords = [ atom.symbol + ' ' + ' '.join(str(x) for x in atom.position) for atom in atoms ] # type : ignore cell = [ list(row) for row in atoms.cell ] # type : ignore return ( coords , cell )","title":"ase_atoms_to_cp2k_input_data"},{"location":"reference/ai2_kit/domain/data_helper/#convert_to_deepmd_npy","text":"def convert_to_deepmd_npy ( cp2k_outputs : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], base_dir : str , type_map : List [ str ] ) View Source def covert_to_deepmd_npy ( cp2k_outputs : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata from itertools import groupby atoms_list : List [ Tuple [ ArtifactDict , Atoms ]] = [] for cp2k_output in cp2k_outputs : dp_system = dpdata . LabeledSystem ( os . path . join ( cp2k_output [ 'url' ], 'output' ), fmt = 'cp2k/output' , type_map = type_map ) atoms_list += [ ( cp2k_output , atoms ) for atoms in dp_system . to_ase_structure () # type: ignore ] output_dirs = [] # group dataset by ancestor key for i , ( key , atoms_group ) in enumerate ( groupby ( atoms_list , key = lambda x : x [ 0 ][ 'attrs' ][ 'ancestor' ])): output_dir = os . path . join ( base_dir , key . replace ( '/' , '_' )) atoms_group = list ( item [ 1 ] for item in atoms_group ) if not atoms_group : continue dp_system = dpdata . LabeledSystem ( atoms_group [ 0 ], fmt = 'ase/structure' ) for atoms in atoms_group [ 1 :]: dp_system += dpdata . LabeledSystem ( atoms , fmt = 'ase/structure' ) dp_system . to_deepmd_npy ( output_dir , set_size = len ( dp_system )) # type: ignore # TODO: return ArtifactDict output_dirs . append ( output_dir ) return output_dirs","title":"convert_to_deepmd_npy"},{"location":"reference/ai2_kit/domain/data_helper/#convert_to_lammps_input_data","text":"def convert_to_lammps_input_data ( poscar_files : List [ ai2_kit . core . artifact . __ArtifactDict .< locals >. ArtifactDict ], base_dir : str , type_map : List [ str ] ) View Source def convert_to_lammps_input_data ( poscar_files : List [ ArtifactDict ], base_dir : str , type_map : List [ str ]): import dpdata import os lammps_data_files = [] for i , poscar_file in enumerate ( poscar_files ): output_file = os . path . join ( base_dir , f ' { i : 06d } .lammps.data' ) dpdata . System ( poscar_file [ 'url' ], fmt = 'vasp/poscar' , type_map = type_map ) . to_lammps_lmp ( output_file ) # type: ignore lammps_data_files . append ({ 'url' : output_file , 'attrs' : poscar_file [ 'attrs' ], }) return lammps_data_files","title":"convert_to_lammps_input_data"},{"location":"reference/ai2_kit/domain/data_helper/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/data_helper/#cp2koutputhelper","text":"class Cp2kOutputHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class Cp2kOutputHelper ( DataHelper ): format = 'cp2k-output-dir'","title":"Cp2kOutputHelper"},{"location":"reference/ai2_kit/domain/data_helper/#ancestors-in-mro","text":"ai2_kit.domain.data_helper.DataHelper","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/data_helper/#datahelper","text":"class DataHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class DataHelper : format : Optional [ str ] = None suffix : Optional [ str ] = None pattern : Optional [ re.Pattern ] = None @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False def __init__ ( self , artifact : Artifact ) -> None : self . artifact = artifact","title":"DataHelper"},{"location":"reference/ai2_kit/domain/data_helper/#descendants","text":"ai2_kit.domain.data_helper.LammpsOutputHelper ai2_kit.domain.data_helper.PoscarHelper ai2_kit.domain.data_helper.XyzHelper ai2_kit.domain.data_helper.DeepmdNpyHelper ai2_kit.domain.data_helper.DeepmdModelHelper ai2_kit.domain.data_helper.Cp2kOutputHelper","title":"Descendants"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables_1","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match_1","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/data_helper/#deepmdmodelhelper","text":"class DeepmdModelHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class DeepmdModelHelper ( DataHelper ): format = 'deepmd/model'","title":"DeepmdModelHelper"},{"location":"reference/ai2_kit/domain/data_helper/#ancestors-in-mro_1","text":"ai2_kit.domain.data_helper.DataHelper","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables_2","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match_2","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/data_helper/#deepmdnpyhelper","text":"class DeepmdNpyHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class DeepmdNpyHelper ( DataHelper ): format = 'deepmd/npy'","title":"DeepmdNpyHelper"},{"location":"reference/ai2_kit/domain/data_helper/#ancestors-in-mro_2","text":"ai2_kit.domain.data_helper.DataHelper","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables_3","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match_3","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/data_helper/#lammpsoutputhelper","text":"class LammpsOutputHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class LammpsOutputHelper ( DataHelper ) : format = 'lammps/output-dir' def get_model_devi_file ( self , filename : str ) -> Artifact : return self . artifact . join ( filename ) def get_passed_dump_files ( self ) -> List [ Artifact ] : return [ self.artifact.join(LAMMPS_TRAJ_DIR, f'{i}{LAMMPS_TRAJ_SUFFIX}') for i in self.artifact.attrs['passed' ] ]","title":"LammpsOutputHelper"},{"location":"reference/ai2_kit/domain/data_helper/#ancestors-in-mro_3","text":"ai2_kit.domain.data_helper.DataHelper","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables_4","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match_4","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/data_helper/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/data_helper/#get_model_devi_file","text":"def get_model_devi_file ( self , filename : str ) -> ai2_kit . core . artifact . Artifact View Source def get_model_devi_file ( self , filename : str ) -> Artifact : return self . artifact . join ( filename )","title":"get_model_devi_file"},{"location":"reference/ai2_kit/domain/data_helper/#get_passed_dump_files","text":"def get_passed_dump_files ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_passed_dump_files ( self ) -> List [ Artifact ] : return [ self.artifact.join(LAMMPS_TRAJ_DIR, f'{i}{LAMMPS_TRAJ_SUFFIX}') for i in self.artifact.attrs['passed' ] ]","title":"get_passed_dump_files"},{"location":"reference/ai2_kit/domain/data_helper/#poscarhelper","text":"class PoscarHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class PoscarHelper ( DataHelper ): format = 'vasp/poscar' pattern = re . compile ( r'POSCAR' )","title":"PoscarHelper"},{"location":"reference/ai2_kit/domain/data_helper/#ancestors-in-mro_4","text":"ai2_kit.domain.data_helper.DataHelper","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables_5","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods_5","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match_5","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/data_helper/#xyzhelper","text":"class XyzHelper ( artifact : ai2_kit . core . artifact . Artifact ) View Source class XyzHelper ( DataHelper ): format = 'extxyz' suffix = '.xyz'","title":"XyzHelper"},{"location":"reference/ai2_kit/domain/data_helper/#ancestors-in-mro_5","text":"ai2_kit.domain.data_helper.DataHelper","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/data_helper/#class-variables_6","text":"format pattern suffix","title":"Class variables"},{"location":"reference/ai2_kit/domain/data_helper/#static-methods_6","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/data_helper/#is_match_6","text":"def is_match ( artifact : ai2_kit . core . artifact . Artifact ) -> bool View Source @classmethod def is_match ( cls , artifact : Artifact ) -> bool : if cls . format and artifact . format == cls . format : return True if cls . suffix and artifact . url . endswith ( cls . suffix ) : return True file_name = os . path . basename ( artifact . url ) if cls . pattern and cls . pattern . match ( file_name ) : return True return False","title":"is_match"},{"location":"reference/ai2_kit/domain/deepmd/","text":"Module ai2_kit.domain.deepmd View Source from ai2_kit.core.artifact import Artifact from ai2_kit.core.script import BashTemplate from ai2_kit.core.script import BashScript , BashStep from ai2_kit.core.job import JobFuture , gather_jobs from ai2_kit.core.log import get_logger from pydantic import BaseModel from typing import List from dataclasses import dataclass import os import copy import random import sys import json from .cll import ICllTrainOutput , BaseCllContext from .data_helper import Cp2kOutputHelper , DeepmdNpyHelper , convert_to_deepmd_npy from .constant import ( DP_CHECKPOINT_FILE , DP_DISP_FILE , DP_PROFILING_FILE , DP_INPUT_FILE , DP_FROZEN_MODEL , ) logger = get_logger ( __name__ ) class GenericDeepmdInputConfig ( BaseModel ): model_num : int = 4 init_dataset : List [ str ] input_template : dict class GenericDeepmdContextConfig ( BaseModel ): script_template : BashTemplate dp_cmd : str = 'dp' @dataclass class GenericDeepmdInput : config : GenericDeepmdInputConfig type_map : List [ str ] old_dataset : List [ Artifact ] # training data used by previous iteration new_dataset : List [ Artifact ] # training data used by current iteration initiated : bool = False def update_training_dataset ( self , dataset : List [ Artifact ]): self . old_dataset += self . new_dataset self . new_dataset = dataset @dataclass class GenericDeepmdContext ( BaseCllContext ): config : GenericDeepmdContextConfig @dataclass class GenericDeepmdOutput ( ICllTrainOutput ): outputs : List [ Artifact ] input : GenericDeepmdInput def get_mlp_models ( self ) -> List [ Artifact ]: return [ a . join ( DP_FROZEN_MODEL ) for a in self . outputs ] def get_training_dataset ( self ) -> List [ Artifact ]: return self . input . new_dataset + self . input . old_dataset async def generic_deepmd ( input : GenericDeepmdInput , ctx : GenericDeepmdContext ): executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ converted_data_dir , tasks_dir ] = executor . setup_workspace ( work_dir , [ 'converted_input_data' , 'tasks' ]) # initialization if not input . initiated : input . new_dataset += ctx . resource_manager . resolve_artifacts ( input . config . init_dataset ) # convert data type if necessary, only needed for new data as old data is already converted new_deepmd_npy_data : List [ Artifact ] = [] cp2k_output_data : List [ Artifact ] = [] # TODO: refactor data type conversion for artifact in input . new_dataset : if not artifact . format or DeepmdNpyHelper . is_match ( artifact ): # treated as deepmd npy data if format is not specified new_deepmd_npy_data . append ( artifact ) elif Cp2kOutputHelper . is_match ( artifact ): cp2k_output_data . append ( artifact ) else : raise ValueError ( f 'unsupported data type: { artifact . format } ' ) # convert data to deepmd/npy format # TODO: support more data type converted_data_dirs = executor . run_python_fn ( convert_to_deepmd_npy )( cp2k_outputs = [ a . to_dict () for a in cp2k_output_data ], base_dir = converted_data_dir , type_map = input . type_map , ) new_deepmd_npy_data += [ Artifact . of ( url = url , format = DeepmdNpyHelper . format , ) for url in converted_data_dirs ] input . new_dataset = new_deepmd_npy_data jobs : List [ JobFuture ] = [] output_dirs = [] # train multiple models at the same time for i in range ( input . config . model_num ): # each model should be trained in its own task_dir task_dir = os . path . join ( tasks_dir , str ( i ) . zfill ( 3 )) executor . mkdir ( task_dir ) # create dp train input file # NOTE: dp v1 format is supported currently # TODO: support more params if it is necessary # ref: https://github.com/deepmodeling/dpgen2/blob/master/examples/ch4/param_CH4_deepmd-kit-2.1.1.json # ref: https://github.com/deepmodeling/dpgen2/blob/master/dpgen2/op/prep_dp_train.py # ref: https://github.com/deepmodeling/dpgen2/blob/master/dpgen2/op/run_dp_train.py dp_input = copy . deepcopy ( input . config . input_template ) training : dict = dp_input [ 'training' ] # set output files training [ 'disp_file' ] = DP_DISP_FILE training [ 'save_ckpt' ] = DP_CHECKPOINT_FILE training [ 'profiling_file' ] = DP_PROFILING_FILE # set random seed discriptor = dp_input [ 'model' ][ 'descriptor' ] if discriptor [ 'type' ] == 'hybrid' : for d in discriptor [ 'list' ]: d [ 'seed' ] = _random_seed () else : discriptor [ 'seed' ] = _random_seed () dp_input [ 'model' ][ 'fitting_net' ][ 'seed' ] = _random_seed () dp_input [ 'training' ][ 'seed' ] = _random_seed () # set training data systems = [ a . url for a in input . old_dataset + input . new_dataset ] training [ 'systems' ] = systems set_prefix : str = training . setdefault ( 'set_prefix' , 'set' ) # respect user input auto_prob_str = \"prob_sys_size\" training . setdefault ( 'batch_size' , 'auto' ) training [ 'auto_prob_style' ] = auto_prob_str # v2 training data training_data = { 'systems' : training [ 'systems' ], 'set_prefix' : training [ 'set_prefix' ], 'auto_prob_style' : training [ 'auto_prob_style' ], 'batch_size' : training [ 'batch_size' ], } training [ 'training_data' ] = training_data # other params dp_input [ 'model' ][ 'type_map' ] = input . type_map # write config to executor dp_input_text = json . dumps ( dp_input , indent = 2 ) dp_input_path = os . path . join ( task_dir , DP_INPUT_FILE ) executor . dump_text ( dp_input_text , dp_input_path ) # build script dp_cmd = ctx . config . dp_cmd dp_train_cmd = [ dp_cmd , 'train' , DP_INPUT_FILE ] dp_freeze_cmd = [ dp_cmd , 'freeze' , '-o' , DP_FROZEN_MODEL ] dp_train_script = BashScript ( template = ctx . config . script_template , steps = [ BashStep ( cmd = dp_train_cmd , checkpoint = 'dp-train' ), # type: ignore BashStep ( cmd = dp_freeze_cmd ), # type: ignore ] # type: ignore ) output_dirs . append ( task_dir ) # submit job job = executor . submit ( dp_train_script . render (), cwd = task_dir , checkpoint_key = f 'submit-job/dp-train/ { i } : { task_dir } ' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) return GenericDeepmdOutput ( input = input , outputs = [ Artifact . of ( url = url , ) for url in output_dirs ] ) def _random_seed (): return random . randrange ( sys . maxsize ) % ( 1 << 32 ) Variables DP_CHECKPOINT_FILE DP_DISP_FILE DP_FROZEN_MODEL DP_INPUT_FILE DP_PROFILING_FILE logger Functions generic_deepmd def generic_deepmd ( input : ai2_kit . domain . deepmd . GenericDeepmdInput , ctx : ai2_kit . domain . deepmd . GenericDeepmdContext ) View Source async def generic_deepmd ( input : GenericDeepmdInput , ctx : GenericDeepmdContext ) : executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ converted_data_dir, tasks_dir ] = executor . setup_workspace ( work_dir , [ 'converted_input_data', 'tasks' ] ) # initialization if not input . initiated : input . new_dataset += ctx . resource_manager . resolve_artifacts ( input . config . init_dataset ) # convert data type if necessary , only needed for new data as old data is already converted new_deepmd_npy_data : List [ Artifact ] = [] cp2k_output_data : List [ Artifact ] = [] # TODO : refactor data type conversion for artifact in input . new_dataset : if not artifact . format or DeepmdNpyHelper . is_match ( artifact ) : # treated as deepmd npy data if format is not specified new_deepmd_npy_data . append ( artifact ) elif Cp2kOutputHelper . is_match ( artifact ) : cp2k_output_data . append ( artifact ) else : raise ValueError ( f 'unsupported data type: {artifact.format}' ) # convert data to deepmd / npy format # TODO : support more data type converted_data_dirs = executor . run_python_fn ( convert_to_deepmd_npy )( cp2k_outputs =[ a.to_dict() for a in cp2k_output_data ] , base_dir = converted_data_dir , type_map = input . type_map , ) new_deepmd_npy_data += [ Artifact.of( url=url, format=DeepmdNpyHelper.format, ) for url in converted_data_dirs ] input . new_dataset = new_deepmd_npy_data jobs : List [ JobFuture ] = [] output_dirs = [] # train multiple models at the same time for i in range ( input . config . model_num ) : # each model should be trained in its own task_dir task_dir = os . path . join ( tasks_dir , str ( i ). zfill ( 3 )) executor . mkdir ( task_dir ) # create dp train input file # NOTE : dp v1 format is supported currently # TODO : support more params if it is necessary # ref : https : // github . com / deepmodeling / dpgen2 / blob / master / examples / ch4 / param_CH4_deepmd - kit - 2.1.1 . json # ref : https : // github . com / deepmodeling / dpgen2 / blob / master / dpgen2 / op / prep_dp_train . py # ref : https : // github . com / deepmodeling / dpgen2 / blob / master / dpgen2 / op / run_dp_train . py dp_input = copy . deepcopy ( input . config . input_template ) training : dict = dp_input [ 'training' ] # set output files training [ 'disp_file' ] = DP_DISP_FILE training [ 'save_ckpt' ] = DP_CHECKPOINT_FILE training [ 'profiling_file' ] = DP_PROFILING_FILE # set random seed discriptor = dp_input [ 'model' ][ 'descriptor' ] if discriptor [ 'type' ] == 'hybrid' : for d in discriptor [ 'list' ] : d [ 'seed' ] = _random_seed () else : discriptor [ 'seed' ] = _random_seed () dp_input [ 'model' ][ 'fitting_net' ][ 'seed' ] = _random_seed () dp_input [ 'training' ][ 'seed' ] = _random_seed () # set training data systems = [ a.url for a in input.old_dataset + input.new_dataset ] training [ 'systems' ] = systems set_prefix : str = training . setdefault ( 'set_prefix' , 'set' ) # respect user input auto_prob_str = \"prob_sys_size\" training . setdefault ( 'batch_size' , 'auto' ) training [ 'auto_prob_style' ] = auto_prob_str # v2 training data training_data = { 'systems' : training [ 'systems' ] , 'set_prefix' : training [ 'set_prefix' ] , 'auto_prob_style' : training [ 'auto_prob_style' ] , 'batch_size' : training [ 'batch_size' ] , } training [ 'training_data' ] = training_data # other params dp_input [ 'model' ][ 'type_map' ] = input . type_map # write config to executor dp_input_text = json . dumps ( dp_input , indent = 2 ) dp_input_path = os . path . join ( task_dir , DP_INPUT_FILE ) executor . dump_text ( dp_input_text , dp_input_path ) # build script dp_cmd = ctx . config . dp_cmd dp_train_cmd = [ dp_cmd, 'train', DP_INPUT_FILE ] dp_freeze_cmd = [ dp_cmd, 'freeze', '-o', DP_FROZEN_MODEL ] dp_train_script = BashScript ( template = ctx . config . script_template , steps =[ BashStep(cmd=dp_train_cmd, checkpoint='dp-train'), # type: ignore BashStep(cmd=dp_freeze_cmd), # type: ignore ] # type : ignore ) output_dirs . append ( task_dir ) # submit job job = executor . submit ( dp_train_script . render (), cwd = task_dir , checkpoint_key = f 'submit-job/dp-train/{i}:{task_dir}' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) return GenericDeepmdOutput ( input = input , outputs =[ Artifact.of( url=url, ) for url in output_dirs ] ) Classes GenericDeepmdContext class GenericDeepmdContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager , config : ai2_kit . domain . deepmd . GenericDeepmdContextConfig ) GenericDeepmdContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager, config: ai2_kit.domain.deepmd.GenericDeepmdContextConfig) View Source class GenericDeepmdContext ( BaseCllContext ): config: GenericDeepmdContextConfig Ancestors (in MRO) ai2_kit.domain.cll.BaseCllContext GenericDeepmdContextConfig class GenericDeepmdContextConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericDeepmdContextConfig ( BaseModel ): script_template: BashTemplate dp_cmd: str = 'dp' Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericDeepmdInput class GenericDeepmdInput ( config : ai2_kit . domain . deepmd . GenericDeepmdInputConfig , type_map : List [ str ], old_dataset : List [ ai2_kit . core . artifact . Artifact ], new_dataset : List [ ai2_kit . core . artifact . Artifact ], initiated : bool = False ) GenericDeepmdInput(config: ai2_kit.domain.deepmd.GenericDeepmdInputConfig, type_map: List[str], old_dataset: List[ai2_kit.core.artifact.Artifact], new_dataset: List[ai2_kit.core.artifact.Artifact], initiated: bool = False) View Source class GenericDeepmdInput : config : GenericDeepmdInputConfig type_map : List [ str ] old_dataset : List [ Artifact ] # training data used by previous iteration new_dataset : List [ Artifact ] # training data used by current iteration initiated : bool = False def update_training_dataset ( self , dataset : List [ Artifact ] ) : self . old_dataset += self . new_dataset self . new_dataset = dataset Class variables initiated Methods update_training_dataset def update_training_dataset ( self , dataset : List [ ai2_kit . core . artifact . Artifact ] ) View Source def update_training_dataset ( self , dataset : List [ Artifact ] ) : self . old_dataset += self . new_dataset self . new_dataset = dataset GenericDeepmdInputConfig class GenericDeepmdInputConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericDeepmdInputConfig ( BaseModel ) : model_num : int = 4 init_dataset : List [ str ] input_template : dict Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericDeepmdOutput class GenericDeepmdOutput ( outputs : List [ ai2_kit . core . artifact . Artifact ], input : ai2_kit . domain . deepmd . GenericDeepmdInput ) GenericDeepmdOutput(outputs: List[ai2_kit.core.artifact.Artifact], input: ai2_kit.domain.deepmd.GenericDeepmdInput) View Source class GenericDeepmdOutput ( ICllTrainOutput ) : outputs : List [ Artifact ] input : GenericDeepmdInput def get_mlp_models ( self ) -> List [ Artifact ] : return [ a.join(DP_FROZEN_MODEL) for a in self.outputs ] def get_training_dataset ( self ) -> List [ Artifact ] : return self . input . new_dataset + self . input . old_dataset Ancestors (in MRO) ai2_kit.domain.cll.ICllTrainOutput abc.ABC Methods get_mlp_models def get_mlp_models ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_mlp_models ( self ) -> List [ Artifact ] : return [ a.join(DP_FROZEN_MODEL) for a in self.outputs ] get_training_dataset def get_training_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_training_dataset ( self ) -> List [ Artifact ] : return self . input . new_dataset + self . input . old_dataset","title":"Deepmd"},{"location":"reference/ai2_kit/domain/deepmd/#module-ai2_kitdomaindeepmd","text":"View Source from ai2_kit.core.artifact import Artifact from ai2_kit.core.script import BashTemplate from ai2_kit.core.script import BashScript , BashStep from ai2_kit.core.job import JobFuture , gather_jobs from ai2_kit.core.log import get_logger from pydantic import BaseModel from typing import List from dataclasses import dataclass import os import copy import random import sys import json from .cll import ICllTrainOutput , BaseCllContext from .data_helper import Cp2kOutputHelper , DeepmdNpyHelper , convert_to_deepmd_npy from .constant import ( DP_CHECKPOINT_FILE , DP_DISP_FILE , DP_PROFILING_FILE , DP_INPUT_FILE , DP_FROZEN_MODEL , ) logger = get_logger ( __name__ ) class GenericDeepmdInputConfig ( BaseModel ): model_num : int = 4 init_dataset : List [ str ] input_template : dict class GenericDeepmdContextConfig ( BaseModel ): script_template : BashTemplate dp_cmd : str = 'dp' @dataclass class GenericDeepmdInput : config : GenericDeepmdInputConfig type_map : List [ str ] old_dataset : List [ Artifact ] # training data used by previous iteration new_dataset : List [ Artifact ] # training data used by current iteration initiated : bool = False def update_training_dataset ( self , dataset : List [ Artifact ]): self . old_dataset += self . new_dataset self . new_dataset = dataset @dataclass class GenericDeepmdContext ( BaseCllContext ): config : GenericDeepmdContextConfig @dataclass class GenericDeepmdOutput ( ICllTrainOutput ): outputs : List [ Artifact ] input : GenericDeepmdInput def get_mlp_models ( self ) -> List [ Artifact ]: return [ a . join ( DP_FROZEN_MODEL ) for a in self . outputs ] def get_training_dataset ( self ) -> List [ Artifact ]: return self . input . new_dataset + self . input . old_dataset async def generic_deepmd ( input : GenericDeepmdInput , ctx : GenericDeepmdContext ): executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ converted_data_dir , tasks_dir ] = executor . setup_workspace ( work_dir , [ 'converted_input_data' , 'tasks' ]) # initialization if not input . initiated : input . new_dataset += ctx . resource_manager . resolve_artifacts ( input . config . init_dataset ) # convert data type if necessary, only needed for new data as old data is already converted new_deepmd_npy_data : List [ Artifact ] = [] cp2k_output_data : List [ Artifact ] = [] # TODO: refactor data type conversion for artifact in input . new_dataset : if not artifact . format or DeepmdNpyHelper . is_match ( artifact ): # treated as deepmd npy data if format is not specified new_deepmd_npy_data . append ( artifact ) elif Cp2kOutputHelper . is_match ( artifact ): cp2k_output_data . append ( artifact ) else : raise ValueError ( f 'unsupported data type: { artifact . format } ' ) # convert data to deepmd/npy format # TODO: support more data type converted_data_dirs = executor . run_python_fn ( convert_to_deepmd_npy )( cp2k_outputs = [ a . to_dict () for a in cp2k_output_data ], base_dir = converted_data_dir , type_map = input . type_map , ) new_deepmd_npy_data += [ Artifact . of ( url = url , format = DeepmdNpyHelper . format , ) for url in converted_data_dirs ] input . new_dataset = new_deepmd_npy_data jobs : List [ JobFuture ] = [] output_dirs = [] # train multiple models at the same time for i in range ( input . config . model_num ): # each model should be trained in its own task_dir task_dir = os . path . join ( tasks_dir , str ( i ) . zfill ( 3 )) executor . mkdir ( task_dir ) # create dp train input file # NOTE: dp v1 format is supported currently # TODO: support more params if it is necessary # ref: https://github.com/deepmodeling/dpgen2/blob/master/examples/ch4/param_CH4_deepmd-kit-2.1.1.json # ref: https://github.com/deepmodeling/dpgen2/blob/master/dpgen2/op/prep_dp_train.py # ref: https://github.com/deepmodeling/dpgen2/blob/master/dpgen2/op/run_dp_train.py dp_input = copy . deepcopy ( input . config . input_template ) training : dict = dp_input [ 'training' ] # set output files training [ 'disp_file' ] = DP_DISP_FILE training [ 'save_ckpt' ] = DP_CHECKPOINT_FILE training [ 'profiling_file' ] = DP_PROFILING_FILE # set random seed discriptor = dp_input [ 'model' ][ 'descriptor' ] if discriptor [ 'type' ] == 'hybrid' : for d in discriptor [ 'list' ]: d [ 'seed' ] = _random_seed () else : discriptor [ 'seed' ] = _random_seed () dp_input [ 'model' ][ 'fitting_net' ][ 'seed' ] = _random_seed () dp_input [ 'training' ][ 'seed' ] = _random_seed () # set training data systems = [ a . url for a in input . old_dataset + input . new_dataset ] training [ 'systems' ] = systems set_prefix : str = training . setdefault ( 'set_prefix' , 'set' ) # respect user input auto_prob_str = \"prob_sys_size\" training . setdefault ( 'batch_size' , 'auto' ) training [ 'auto_prob_style' ] = auto_prob_str # v2 training data training_data = { 'systems' : training [ 'systems' ], 'set_prefix' : training [ 'set_prefix' ], 'auto_prob_style' : training [ 'auto_prob_style' ], 'batch_size' : training [ 'batch_size' ], } training [ 'training_data' ] = training_data # other params dp_input [ 'model' ][ 'type_map' ] = input . type_map # write config to executor dp_input_text = json . dumps ( dp_input , indent = 2 ) dp_input_path = os . path . join ( task_dir , DP_INPUT_FILE ) executor . dump_text ( dp_input_text , dp_input_path ) # build script dp_cmd = ctx . config . dp_cmd dp_train_cmd = [ dp_cmd , 'train' , DP_INPUT_FILE ] dp_freeze_cmd = [ dp_cmd , 'freeze' , '-o' , DP_FROZEN_MODEL ] dp_train_script = BashScript ( template = ctx . config . script_template , steps = [ BashStep ( cmd = dp_train_cmd , checkpoint = 'dp-train' ), # type: ignore BashStep ( cmd = dp_freeze_cmd ), # type: ignore ] # type: ignore ) output_dirs . append ( task_dir ) # submit job job = executor . submit ( dp_train_script . render (), cwd = task_dir , checkpoint_key = f 'submit-job/dp-train/ { i } : { task_dir } ' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) return GenericDeepmdOutput ( input = input , outputs = [ Artifact . of ( url = url , ) for url in output_dirs ] ) def _random_seed (): return random . randrange ( sys . maxsize ) % ( 1 << 32 )","title":"Module ai2_kit.domain.deepmd"},{"location":"reference/ai2_kit/domain/deepmd/#variables","text":"DP_CHECKPOINT_FILE DP_DISP_FILE DP_FROZEN_MODEL DP_INPUT_FILE DP_PROFILING_FILE logger","title":"Variables"},{"location":"reference/ai2_kit/domain/deepmd/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/domain/deepmd/#generic_deepmd","text":"def generic_deepmd ( input : ai2_kit . domain . deepmd . GenericDeepmdInput , ctx : ai2_kit . domain . deepmd . GenericDeepmdContext ) View Source async def generic_deepmd ( input : GenericDeepmdInput , ctx : GenericDeepmdContext ) : executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) [ converted_data_dir, tasks_dir ] = executor . setup_workspace ( work_dir , [ 'converted_input_data', 'tasks' ] ) # initialization if not input . initiated : input . new_dataset += ctx . resource_manager . resolve_artifacts ( input . config . init_dataset ) # convert data type if necessary , only needed for new data as old data is already converted new_deepmd_npy_data : List [ Artifact ] = [] cp2k_output_data : List [ Artifact ] = [] # TODO : refactor data type conversion for artifact in input . new_dataset : if not artifact . format or DeepmdNpyHelper . is_match ( artifact ) : # treated as deepmd npy data if format is not specified new_deepmd_npy_data . append ( artifact ) elif Cp2kOutputHelper . is_match ( artifact ) : cp2k_output_data . append ( artifact ) else : raise ValueError ( f 'unsupported data type: {artifact.format}' ) # convert data to deepmd / npy format # TODO : support more data type converted_data_dirs = executor . run_python_fn ( convert_to_deepmd_npy )( cp2k_outputs =[ a.to_dict() for a in cp2k_output_data ] , base_dir = converted_data_dir , type_map = input . type_map , ) new_deepmd_npy_data += [ Artifact.of( url=url, format=DeepmdNpyHelper.format, ) for url in converted_data_dirs ] input . new_dataset = new_deepmd_npy_data jobs : List [ JobFuture ] = [] output_dirs = [] # train multiple models at the same time for i in range ( input . config . model_num ) : # each model should be trained in its own task_dir task_dir = os . path . join ( tasks_dir , str ( i ). zfill ( 3 )) executor . mkdir ( task_dir ) # create dp train input file # NOTE : dp v1 format is supported currently # TODO : support more params if it is necessary # ref : https : // github . com / deepmodeling / dpgen2 / blob / master / examples / ch4 / param_CH4_deepmd - kit - 2.1.1 . json # ref : https : // github . com / deepmodeling / dpgen2 / blob / master / dpgen2 / op / prep_dp_train . py # ref : https : // github . com / deepmodeling / dpgen2 / blob / master / dpgen2 / op / run_dp_train . py dp_input = copy . deepcopy ( input . config . input_template ) training : dict = dp_input [ 'training' ] # set output files training [ 'disp_file' ] = DP_DISP_FILE training [ 'save_ckpt' ] = DP_CHECKPOINT_FILE training [ 'profiling_file' ] = DP_PROFILING_FILE # set random seed discriptor = dp_input [ 'model' ][ 'descriptor' ] if discriptor [ 'type' ] == 'hybrid' : for d in discriptor [ 'list' ] : d [ 'seed' ] = _random_seed () else : discriptor [ 'seed' ] = _random_seed () dp_input [ 'model' ][ 'fitting_net' ][ 'seed' ] = _random_seed () dp_input [ 'training' ][ 'seed' ] = _random_seed () # set training data systems = [ a.url for a in input.old_dataset + input.new_dataset ] training [ 'systems' ] = systems set_prefix : str = training . setdefault ( 'set_prefix' , 'set' ) # respect user input auto_prob_str = \"prob_sys_size\" training . setdefault ( 'batch_size' , 'auto' ) training [ 'auto_prob_style' ] = auto_prob_str # v2 training data training_data = { 'systems' : training [ 'systems' ] , 'set_prefix' : training [ 'set_prefix' ] , 'auto_prob_style' : training [ 'auto_prob_style' ] , 'batch_size' : training [ 'batch_size' ] , } training [ 'training_data' ] = training_data # other params dp_input [ 'model' ][ 'type_map' ] = input . type_map # write config to executor dp_input_text = json . dumps ( dp_input , indent = 2 ) dp_input_path = os . path . join ( task_dir , DP_INPUT_FILE ) executor . dump_text ( dp_input_text , dp_input_path ) # build script dp_cmd = ctx . config . dp_cmd dp_train_cmd = [ dp_cmd, 'train', DP_INPUT_FILE ] dp_freeze_cmd = [ dp_cmd, 'freeze', '-o', DP_FROZEN_MODEL ] dp_train_script = BashScript ( template = ctx . config . script_template , steps =[ BashStep(cmd=dp_train_cmd, checkpoint='dp-train'), # type: ignore BashStep(cmd=dp_freeze_cmd), # type: ignore ] # type : ignore ) output_dirs . append ( task_dir ) # submit job job = executor . submit ( dp_train_script . render (), cwd = task_dir , checkpoint_key = f 'submit-job/dp-train/{i}:{task_dir}' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) return GenericDeepmdOutput ( input = input , outputs =[ Artifact.of( url=url, ) for url in output_dirs ] )","title":"generic_deepmd"},{"location":"reference/ai2_kit/domain/deepmd/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/deepmd/#genericdeepmdcontext","text":"class GenericDeepmdContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager , config : ai2_kit . domain . deepmd . GenericDeepmdContextConfig ) GenericDeepmdContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager, config: ai2_kit.domain.deepmd.GenericDeepmdContextConfig) View Source class GenericDeepmdContext ( BaseCllContext ): config: GenericDeepmdContextConfig","title":"GenericDeepmdContext"},{"location":"reference/ai2_kit/domain/deepmd/#ancestors-in-mro","text":"ai2_kit.domain.cll.BaseCllContext","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/deepmd/#genericdeepmdcontextconfig","text":"class GenericDeepmdContextConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericDeepmdContextConfig ( BaseModel ): script_template: BashTemplate dp_cmd: str = 'dp'","title":"GenericDeepmdContextConfig"},{"location":"reference/ai2_kit/domain/deepmd/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/deepmd/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/deepmd/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/deepmd/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/deepmd/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/deepmd/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/deepmd/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/deepmd/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/deepmd/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/deepmd/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/deepmd/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/deepmd/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/deepmd/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/deepmd/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/deepmd/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/deepmd/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/deepmd/#genericdeepmdinput","text":"class GenericDeepmdInput ( config : ai2_kit . domain . deepmd . GenericDeepmdInputConfig , type_map : List [ str ], old_dataset : List [ ai2_kit . core . artifact . Artifact ], new_dataset : List [ ai2_kit . core . artifact . Artifact ], initiated : bool = False ) GenericDeepmdInput(config: ai2_kit.domain.deepmd.GenericDeepmdInputConfig, type_map: List[str], old_dataset: List[ai2_kit.core.artifact.Artifact], new_dataset: List[ai2_kit.core.artifact.Artifact], initiated: bool = False) View Source class GenericDeepmdInput : config : GenericDeepmdInputConfig type_map : List [ str ] old_dataset : List [ Artifact ] # training data used by previous iteration new_dataset : List [ Artifact ] # training data used by current iteration initiated : bool = False def update_training_dataset ( self , dataset : List [ Artifact ] ) : self . old_dataset += self . new_dataset self . new_dataset = dataset","title":"GenericDeepmdInput"},{"location":"reference/ai2_kit/domain/deepmd/#class-variables_1","text":"initiated","title":"Class variables"},{"location":"reference/ai2_kit/domain/deepmd/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/deepmd/#update_training_dataset","text":"def update_training_dataset ( self , dataset : List [ ai2_kit . core . artifact . Artifact ] ) View Source def update_training_dataset ( self , dataset : List [ Artifact ] ) : self . old_dataset += self . new_dataset self . new_dataset = dataset","title":"update_training_dataset"},{"location":"reference/ai2_kit/domain/deepmd/#genericdeepmdinputconfig","text":"class GenericDeepmdInputConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericDeepmdInputConfig ( BaseModel ) : model_num : int = 4 init_dataset : List [ str ] input_template : dict","title":"GenericDeepmdInputConfig"},{"location":"reference/ai2_kit/domain/deepmd/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/deepmd/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/deepmd/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/deepmd/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/deepmd/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/deepmd/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/deepmd/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/deepmd/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/deepmd/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/deepmd/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/deepmd/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/deepmd/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/deepmd/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/deepmd/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/deepmd/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/deepmd/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/deepmd/#genericdeepmdoutput","text":"class GenericDeepmdOutput ( outputs : List [ ai2_kit . core . artifact . Artifact ], input : ai2_kit . domain . deepmd . GenericDeepmdInput ) GenericDeepmdOutput(outputs: List[ai2_kit.core.artifact.Artifact], input: ai2_kit.domain.deepmd.GenericDeepmdInput) View Source class GenericDeepmdOutput ( ICllTrainOutput ) : outputs : List [ Artifact ] input : GenericDeepmdInput def get_mlp_models ( self ) -> List [ Artifact ] : return [ a.join(DP_FROZEN_MODEL) for a in self.outputs ] def get_training_dataset ( self ) -> List [ Artifact ] : return self . input . new_dataset + self . input . old_dataset","title":"GenericDeepmdOutput"},{"location":"reference/ai2_kit/domain/deepmd/#ancestors-in-mro_3","text":"ai2_kit.domain.cll.ICllTrainOutput abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/deepmd/#methods_3","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/deepmd/#get_mlp_models","text":"def get_mlp_models ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_mlp_models ( self ) -> List [ Artifact ] : return [ a.join(DP_FROZEN_MODEL) for a in self.outputs ]","title":"get_mlp_models"},{"location":"reference/ai2_kit/domain/deepmd/#get_training_dataset","text":"def get_training_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_training_dataset ( self ) -> List [ Artifact ] : return self . input . new_dataset + self . input . old_dataset","title":"get_training_dataset"},{"location":"reference/ai2_kit/domain/lammps/","text":"Module ai2_kit.domain.lammps View Source from ai2_kit.core.script import BashTemplate , BashStep , BashScript from ai2_kit.core.artifact import Artifact , ArtifactDict from ai2_kit.core.log import get_logger from ai2_kit.core.job import gather_jobs from ai2_kit.core.util import split_list , dict_nested_get from typing import List , Literal , Optional , Union , Mapping , Sequence , Any from pydantic import BaseModel from dataclasses import dataclass from string import Template from allpairspy import AllPairs import os import itertools import random from .cll import BaseCllContext , ICllExploreOutput from .constant import ( MODEL_DEVI_OUT , MODEL_DEVI_NEU_OUT , MODEL_DEVI_RED_OUT , LAMMPS_TRAJ_DIR , LAMMPS_TRAJ_SUFFIX , ) from .data_helper import LammpsOutputHelper , PoscarHelper , convert_to_lammps_input_data logger = get_logger ( __name__ ) class ExploreVariants ( BaseModel ): temp : List [ float ] \"\"\"Temperatures variants.\"\"\" pres : List [ float ] \"\"\"Pressures variants.\"\"\" others : Mapping [ str , Sequence [ Any ]] = dict () \"\"\" Other variants to be combined with. The key is the name of the variant, and the value is the list of values. For example, if you want to combine the variant 'LAMBDA' with values [0.0, 0.5, 1.0], you can set the others field to {'LAMBDA': [0.0, 0.5, 1.0]}. And in LAMMPS input template, you can use the variable ${LAMBDA} and v_LAMBDA to access the value. \"\"\" class GenericLammpsInputConfig ( BaseModel ): explore_vars : ExploreVariants \"\"\"Variants to be explored.\"\"\" n_wise : int = 0 \"\"\"The way of combining variants. 0 means cartesian product, 2 means 2-wise, etc. If n_wise is less than 2 or greater than total fields, the full combination will be used. It is strongly recommended to use n_wise when the full combination is too large. \"\"\" system_files : List [ str ] \"\"\"Artifacts of initial system data.\"\"\" plumed_config : Optional [ str ] \"\"\"Plumed config file content.\"\"\" no_pbc : bool = False tau_t : float = 0.1 tau_p : float = 0.5 timestep : float = 0.0005 sample_freq : int nsteps : int ensemble : Literal [ 'nvt' , 'nvt-i' , 'nvt-a' , 'nvt-iso' , 'nvt-aniso' , 'npt' , 'npt-t' , 'npt-tri' , 'nve' ] \"\"\"Ensemble to be used. nvt means constant volume and temperature. nvt-i means constant volume and temperature, with isotropic scaling. nvt-a means constant volume and temperature, with anisotropic scaling. nvt-iso means constant volume and temperature, with isotropic scaling. npt means constant pressure and temperature. npt-t means constant pressure and temperature, with isotropic scaling. npt-tri means constant pressure and temperature, with anisotropic scaling. nve means constant energy. \"\"\" input_template : Optional [ str ] \"\"\"Lammps input template file content.\"\"\" post_variables_section : str = '' post_init_section : str = '' post_read_data_section : str = '' post_force_field_section : str = '' post_md_section : str = '' post_run_section : str = '' class GenericLammpsContextConfig ( BaseModel ): script_template : BashTemplate lammps_cmd : str = 'lmp' concurrency : int = 5 @dataclass class GenericLammpsInput : @dataclass class MdOptions : models : List [ Artifact ] @dataclass class FepOptions : red_models : List [ Artifact ] neu_models : List [ Artifact ] config : GenericLammpsInputConfig type_map : List [ str ] mass_map : List [ float ] # The following options are mutex md_options : Optional [ MdOptions ] = None fep_options : Optional [ FepOptions ] = None @dataclass class GenericLammpsContext ( BaseCllContext ): config : GenericLammpsContextConfig @dataclass class GenericLammpsOutput ( ICllExploreOutput ): model_devi_outputs : List [ Artifact ] def get_model_devi_dataset ( self ) -> List [ Artifact ]: return self . model_devi_outputs async def generic_lammps ( input : GenericLammpsInput , ctx : GenericLammpsContext ): executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) input_data_dir , tasks_dir = executor . setup_workspace ( work_dir , [ 'input_data' , 'tasks' ]) systems = ctx . resource_manager . resolve_artifacts ( input . config . system_files ) # prepare lammps input data # TODO: refactor the way of handling different types of input # TODO: handle more data format, for example, cp2k output poscar_files : List [ Artifact ] = [] for system_file in systems : if PoscarHelper . is_match ( system_file ): poscar_files . append ( system_file ) else : raise ValueError ( f 'unsupported system file type: { system_file } ' ) input_data_files : List [ ArtifactDict ] = executor . run_python_fn ( convert_to_lammps_input_data )( poscar_files = [ a . to_dict () for a in poscar_files ], base_dir = input_data_dir , type_map = input . type_map , ) combination_fields : List [ str ] = [ 'data_file' , 'temp' , 'pres' ] combination_values : Sequence [ Sequence [ Any ]] = [ input_data_files , input . config . explore_vars . temp , input . config . explore_vars . pres , ] for k , v in input . config . explore_vars . others . items (): combination_fields . append ( k ) combination_values . append ( v ) # type: ignore if 1 < input . config . n_wise <= len ( combination_fields ): logger . info ( 'using %d -wise combination' , input . config . n_wise ) combinations = AllPairs ( combination_values , n = input . config . n_wise ) else : logger . info ( 'using full combination' ) combinations = itertools . product ( * combination_values ) lammps_task_dirs : List [ ArtifactDict ] = [] lammps_input_file_name = 'lammps.input' for i , combination in enumerate ( combinations ): data_file : ArtifactDict = combination [ 0 ] temp , pres = combination [ 1 : 3 ] others_dict = dict ( zip ( combination_fields [ 3 :], combination [ 3 :])) lammps_task_dir = os . path . join ( tasks_dir , f ' { i : 06d } ' ) executor . mkdir ( os . path . join ( lammps_task_dir , LAMMPS_TRAJ_DIR )) # create dump directory for lammps or else will get error if input . md_options : force_field_section = make_md_force_field_section ( models = [ a . url for a in input . md_options . models ], ) elif input . fep_options : if 'LAMBDA_f' not in others_dict : raise ValueError ( 'LAMBDA_f must be set when using FEP mode!' ) # inject the following variables for FEP mode others_dict [ 'LAMBDA_i' ] = '1-v_LAMBDA_f' # should not have space, or you must quote others_dict [ 'plus' ] = 1 others_dict [ 'minus' ] = - 1 force_field_section = make_fep_force_field_section ( neu_models = [ a . url for a in input . fep_options . neu_models ], red_models = [ a . url for a in input . fep_options . red_models ], ) else : raise ValueError ( 'one and only one of md_options or fep_options must be set' ) plumed_file = None # plumed_config could be overrided by the attrs of data_file plumed_config = dict_nested_get ( data_file , [ 'attrs' , 'lammps' , 'plumed_config' ], input . config . plumed_config ) if plumed_config and isinstance ( plumed_config , str ): plumed_file = 'plumed.input' plumed_file_path = os . path . join ( lammps_task_dir , plumed_file ) logger . info ( f 'found plumed config, generate { plumed_file_path } ' ) executor . dump_text ( plumed_config , plumed_file_path ) template = input . config . input_template or DEFAULT_LAMMPS_INPUT_TEMPLATE input_text = make_lammps_input ( data_file = data_file [ 'url' ], nsteps = input . config . nsteps , timestep = input . config . timestep , trj_freq = input . config . sample_freq , temp = temp , pres = pres , tau_t = input . config . tau_t , tau_p = input . config . tau_p , ensemble = input . config . ensemble , mass_map = input . mass_map , others_dict = others_dict , force_field_section = force_field_section , template = template , post_variables_section = input . config . post_variables_section , post_init_section = input . config . post_init_section , post_read_data_section = input . config . post_read_data_section , post_force_field_section = input . config . post_force_field_section , post_md_section = input . config . post_md_section , post_run_section = input . config . post_run_section , plumed_file = plumed_file , no_pbc = False , rand_start = 1_000_000 , ) input_file_path = os . path . join ( lammps_task_dir , lammps_input_file_name ) logger . info ( f 'generate lammps config { input_file_path } ' ) executor . dump_text ( input_text , input_file_path ) lammps_task_dirs . append ({ 'url' : lammps_task_dir , 'attrs' : data_file [ 'attrs' ]}) # type: ignore # build scripts and submit lammps_cmd = ctx . config . lammps_cmd base_cmd = f ' { lammps_cmd } -i { lammps_input_file_name } ' cmd = f '''if [ -f md.restart.* ]; then { base_cmd } -v restart 1; else { base_cmd } -v restart 0; fi''' # generate steps steps = [] for lammps_task_dir in lammps_task_dirs : steps . append ( BashStep ( cwd = lammps_task_dir [ 'url' ], cmd = cmd , checkpoint = 'lammps' )) # submit jobs by the number of concurrency jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )): if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/lammps/ { i } : { tasks_dir } ' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) outputs = [ Artifact . of ( url = task_dir [ 'url' ], executor = executor . name , format = LammpsOutputHelper . format , attrs = task_dir [ 'attrs' ], ) for task_dir in lammps_task_dirs ] # type: ignore return GenericLammpsOutput ( model_devi_outputs = outputs ) def make_md_force_field_section ( models : List [ str ]): deepmd_args = \"\" settings = [ 'pair_style deepmd %s out_freq $ {THERMO_FREQ} out_file %s %s ' % ( ' ' . join ( models ), MODEL_DEVI_OUT , deepmd_args ), 'pair_coeff * *' , ] return settings def make_fep_force_field_section ( neu_models : List [ str ], red_models : List [ str ]): deepmd_args = \"\" settings = [ 'pair_style hybrid/overlay &' , ' deepmd %s out_freq $ {THERMO_FREQ} out_file %s %s &' % ( ' ' . join ( neu_models ), MODEL_DEVI_NEU_OUT , deepmd_args ), ' deepmd %s out_freq $ {THERMO_FREQ} out_file %s %s ' % ( ' ' . join ( red_models ), MODEL_DEVI_RED_OUT , deepmd_args ), 'pair_coeff * * deepmd 1 *' , 'pair_coeff * * deepmd 2 *' , '' , 'fix sampling_PES all adapt 0 &' , ' pair deepmd:1 scale * * v_LAMBDA_f &' , ' pair deepmd:2 scale * * v_LAMBDA_i &' , ' scale yes' , ] return settings def make_lammps_input ( data_file : str , nsteps : int , timestep : float , trj_freq : int , temp : float , pres : float , tau_t : float , tau_p : float , ensemble : str , mass_map : List [ float ], others_dict : Mapping [ str , Any ], template : str , post_variables_section : str , post_init_section : str , post_read_data_section : str , post_force_field_section : str , post_md_section : str , post_run_section : str , force_field_section : List [ str ], plumed_file : Optional [ str ] = None , no_pbc = False , rand_start = 1_000_000 , ): # FIXME: I am not sure if it is a good idea to fix it automatically # maybe we should consider raise an error here if not ensemble . startswith ( 'npt' ): pres = - 1 variables = [ '# required variables' , 'variable NSTEPS equal %d ' % nsteps , 'variable THERMO_FREQ equal %d ' % trj_freq , 'variable DUMP_FREQ equal %d ' % trj_freq , 'variable TEMP equal %f ' % temp , 'variable PRES equal %f ' % pres , 'variable TAU_T equal %f ' % tau_t , 'variable TAU_P equal %f ' % tau_p , '' , '# custom variables (if any)' , ] for k , v in others_dict . items (): variables . append ( f 'variable { k } equal { v } ' ) init_section = [ 'boundary ' + ( 'f f f' if no_pbc else 'p p p' ), ] read_data_section = [ '''if \"${restart} > 0\" then \"read_restart md.restart.*\" else \"read_data %s\"''' % data_file , * ( \"mass {id} {mass} \" . format ( id = i + 1 , mass = m ) for i , m in enumerate ( mass_map )) ] md_section = [ '''if \"${restart} == 0\" then \"velocity all create ${TEMP} %d\"''' % ( random . randrange ( rand_start - 1 ) + 1 ) ] if ensemble . startswith ( 'npt' ) and no_pbc : raise ValueError ( 'ensemble npt conflict with no_pcb' ) if ensemble in ( 'npt' , 'npt-i' , 'npt-iso' ,): md_section . append ( 'fix 1 all npt temp $ {TEMP} $ {TEMP} $ {TAU_T} iso $ {PRES} $ {PRES} $ {TAU_P} ' ) elif ensemble in ( 'npt-a' , 'npt-aniso' ,): md_section . append ( 'fix 1 all npt temp $ {TEMP} $ {TEMP} $ {TAU_T} aniso $ {PRES} $ {PRES} $ {TAU_P} ' ) elif ensemble in ( 'npt-t' , 'npt-tri' ,): md_section . append ( 'fix 1 all npt temp $ {TEMP} $ {TEMP} $ {TAU_T} tri $ {PRES} $ {PRES} $ {TAU_P} ' ) elif ensemble in ( 'nvt' ,): md_section . append ( 'fix 1 all nvt temp $ {TEMP} $ {TEMP} $ {TAU_T} ' ) elif ensemble in ( 'nve' ,): md_section . append ( 'fix 1 all nve' ) else : raise ValueError ( 'unknown ensemble: ' + ensemble ) if plumed_file : md_section . append ( f 'fix dpgen_plm all plumed plumedfile { plumed_file } outfile plumed.out' ) if no_pbc : md_section . extend ([ 'velocity all zero linear' , 'fix fm all momentum 1 linear 1 1 1' , ]) md_section . extend ([ 'thermo_style custom step temp pe ke etotal press vol lx ly lz xy xz yz' , 'thermo $ {THERMO_FREQ} ' , 'dump 1 all custom $ {DUMP_FREQ} %s /* %s id type x y z fx fy fz' % ( LAMMPS_TRAJ_DIR , LAMMPS_TRAJ_SUFFIX ), 'restart 10000 md.restart' , ]) run_section = [ 'timestep %f ' % timestep , 'run $ {NSTEPS} upto' , ] return LammpsInputTemplate ( template ) . substitute ( dict ( variables_section = ' \\n ' . join ( variables ), post_variables_section = post_variables_section , init_section = ' \\n ' . join ( init_section ), post_init_section = post_init_section , read_data_section = ' \\n ' . join ( read_data_section ), post_read_data_section = post_read_data_section , md_section = ' \\n ' . join ( md_section ), post_md_section = post_md_section , force_field_section = ' \\n ' . join ( force_field_section ), post_force_field_section = post_force_field_section , run_section = ' \\n ' . join ( run_section ), post_run_section = post_run_section , )) class LammpsInputTemplate ( Template ): \"\"\" change delimiter from $ to $$ as $ is used a lot in lammps input file \"\"\" delimiter = '$$' DEFAULT_LAMMPS_INPUT_TEMPLATE = ''' \\ ## variables_section $$variables_section ## post_variables_section $$post_variables_section ## init_section units metal atom_style atomic $$init_section ## post_init_section $$post_init_section ## read_data_section $$read_data_section ## post_read_data_section $$post_read_data_section ## force_field_section $$force_field_section ## post_force_field_section $$post_force_field_section ## md_section $$md_section ## post_md_section $$post_md_section ## run_section $$run_section ## post_run_section $$post_run_section ''' Variables DEFAULT_LAMMPS_INPUT_TEMPLATE LAMMPS_TRAJ_DIR LAMMPS_TRAJ_SUFFIX MODEL_DEVI_NEU_OUT MODEL_DEVI_OUT MODEL_DEVI_RED_OUT logger Functions generic_lammps def generic_lammps ( input : ai2_kit . domain . lammps . GenericLammpsInput , ctx : ai2_kit . domain . lammps . GenericLammpsContext ) View Source async def generic_lammps ( input : GenericLammpsInput , ctx : GenericLammpsContext ) : executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) input_data_dir , tasks_dir = executor . setup_workspace ( work_dir , [ 'input_data', 'tasks' ] ) systems = ctx . resource_manager . resolve_artifacts ( input . config . system_files ) # prepare lammps input data # TODO : refactor the way of handling different types of input # TODO : handle more data format , for example , cp2k output poscar_files : List [ Artifact ] = [] for system_file in systems : if PoscarHelper . is_match ( system_file ) : poscar_files . append ( system_file ) else : raise ValueError ( f 'unsupported system file type: {system_file}' ) input_data_files : List [ ArtifactDict ] = executor . run_python_fn ( convert_to_lammps_input_data )( poscar_files =[ a.to_dict() for a in poscar_files ] , base_dir = input_data_dir , type_map = input . type_map , ) combination_fields : List [ str ] = [ 'data_file', 'temp', 'pres' ] combination_values : Sequence [ Sequence[Any ] ] = [ input_data_files, input.config.explore_vars.temp, input.config.explore_vars.pres, ] for k , v in input . config . explore_vars . others . items () : combination_fields . append ( k ) combination_values . append ( v ) # type : ignore if 1 < input . config . n_wise <= len ( combination_fields ) : logger . info ( 'using %d-wise combination' , input . config . n_wise ) combinations = AllPairs ( combination_values , n = input . config . n_wise ) else : logger . info ( 'using full combination' ) combinations = itertools . product ( * combination_values ) lammps_task_dirs : List [ ArtifactDict ] = [] lammps_input_file_name = 'lammps.input' for i , combination in enumerate ( combinations ) : data_file : ArtifactDict = combination [ 0 ] temp , pres = combination [ 1:3 ] others_dict = dict ( zip ( combination_fields [ 3: ] , combination [ 3: ] )) lammps_task_dir = os . path . join ( tasks_dir , f '{i:06d}' ) executor . mkdir ( os . path . join ( lammps_task_dir , LAMMPS_TRAJ_DIR )) # create dump directory for lammps or else will get error if input . md_options : force_field_section = make_md_force_field_section ( models =[ a.url for a in input.md_options.models ] , ) elif input . fep_options : if 'LAMBDA_f' not in others_dict : raise ValueError ( 'LAMBDA_f must be set when using FEP mode!' ) # inject the following variables for FEP mode others_dict [ 'LAMBDA_i' ] = '1-v_LAMBDA_f' # should not have space , or you must quote others_dict [ 'plus' ] = 1 others_dict [ 'minus' ] = - 1 force_field_section = make_fep_force_field_section ( neu_models =[ a.url for a in input.fep_options.neu_models ] , red_models =[ a.url for a in input.fep_options.red_models ] , ) else : raise ValueError ( 'one and only one of md_options or fep_options must be set' ) plumed_file = None # plumed_config could be overrided by the attrs of data_file plumed_config = dict_nested_get ( data_file , [ 'attrs', 'lammps', 'plumed_config' ] , input . config . plumed_config ) if plumed_config and isinstance ( plumed_config , str ) : plumed_file = 'plumed.input' plumed_file_path = os . path . join ( lammps_task_dir , plumed_file ) logger . info ( f 'found plumed config, generate {plumed_file_path}' ) executor . dump_text ( plumed_config , plumed_file_path ) template = input . config . input_template or DEFAULT_LAMMPS_INPUT_TEMPLATE input_text = make_lammps_input ( data_file = data_file [ 'url' ] , nsteps = input . config . nsteps , timestep = input . config . timestep , trj_freq = input . config . sample_freq , temp = temp , pres = pres , tau_t = input . config . tau_t , tau_p = input . config . tau_p , ensemble = input . config . ensemble , mass_map = input . mass_map , others_dict = others_dict , force_field_section = force_field_section , template = template , post_variables_section = input . config . post_variables_section , post_init_section = input . config . post_init_section , post_read_data_section = input . config . post_read_data_section , post_force_field_section = input . config . post_force_field_section , post_md_section = input . config . post_md_section , post_run_section = input . config . post_run_section , plumed_file = plumed_file , no_pbc = False , rand_start = 1 _000_000 , ) input_file_path = os . path . join ( lammps_task_dir , lammps_input_file_name ) logger . info ( f 'generate lammps config {input_file_path}' ) executor . dump_text ( input_text , input_file_path ) lammps_task_dirs . append ( { 'url' : lammps_task_dir , 'attrs' : data_file [ 'attrs' ] } ) # type : ignore # build scripts and submit lammps_cmd = ctx . config . lammps_cmd base_cmd = f '{lammps_cmd} -i {lammps_input_file_name}' cmd = f '''if [ -f md.restart.* ]; then {base_cmd} -v restart 1; else {base_cmd} -v restart 0; fi''' # generate steps steps = [] for lammps_task_dir in lammps_task_dirs : steps . append ( BashStep ( cwd = lammps_task_dir [ 'url' ] , cmd = cmd , checkpoint = 'lammps' )) # submit jobs by the number of concurrency jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )) : if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/lammps/{i}:{tasks_dir}' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) outputs = [ Artifact.of( url=task_dir['url' ] , executor = executor . name , format = LammpsOutputHelper . format , attrs = task_dir [ 'attrs' ] , ) for task_dir in lammps_task_dirs ] # type : ignore return GenericLammpsOutput ( model_devi_outputs = outputs ) make_fep_force_field_section def make_fep_force_field_section ( neu_models : List [ str ], red_models : List [ str ] ) View Source def make_fep_force_field_section ( neu_models : List [ str ] , red_models : List [ str ] ) : deepmd_args = \"\" settings = [ 'pair_style hybrid/overlay &', ' deepmd %s out_freq ${THERMO_FREQ} out_file %s %s &' %(' '.join(neu_models), MODEL_DEVI_NEU_OUT, deepmd_args), ' deepmd %s out_freq ${THERMO_FREQ} out_file %s %s' %(' '.join(red_models), MODEL_DEVI_RED_OUT, deepmd_args), 'pair_coeff * * deepmd 1 *', 'pair_coeff * * deepmd 2 *', '', 'fix sampling_PES all adapt 0 &', ' pair deepmd:1 scale * * v_LAMBDA_f &', ' pair deepmd:2 scale * * v_LAMBDA_i &', ' scale yes', ] return settings make_lammps_input def make_lammps_input ( data_file : str , nsteps : int , timestep : float , trj_freq : int , temp : float , pres : float , tau_t : float , tau_p : float , ensemble : str , mass_map : List [ float ], others_dict : Mapping [ str , Any ], template : str , post_variables_section : str , post_init_section : str , post_read_data_section : str , post_force_field_section : str , post_md_section : str , post_run_section : str , force_field_section : List [ str ], plumed_file : Union [ str , NoneType ] = None , no_pbc = False , rand_start = 1000000 ) View Source def make_lammps_input ( data_file : str , nsteps : int , timestep : float , trj_freq : int , temp : float , pres : float , tau_t : float , tau_p : float , ensemble : str , mass_map : List [ float ] , others_dict : Mapping [ str, Any ] , template : str , post_variables_section : str , post_init_section : str , post_read_data_section : str , post_force_field_section : str , post_md_section : str , post_run_section : str , force_field_section : List [ str ] , plumed_file : Optional [ str ] = None , no_pbc = False , rand_start = 1 _000_000 , ) : # FIXME : I am not sure if it is a good idea to fix it automatically # maybe we should consider raise an error here if not ensemble . startswith ( 'npt' ) : pres = - 1 variables = [ '# required variables', 'variable NSTEPS equal %d' % nsteps, 'variable THERMO_FREQ equal %d' % trj_freq, 'variable DUMP_FREQ equal %d' % trj_freq, 'variable TEMP equal %f' % temp, 'variable PRES equal %f' % pres, 'variable TAU_T equal %f' % tau_t, 'variable TAU_P equal %f' % tau_p, '', '# custom variables (if any)', ] for k , v in others_dict . items () : variables . append ( f 'variable {k} equal {v}' ) init_section = [ 'boundary ' + ('f f f' if no_pbc else 'p p p'), ] read_data_section = [ '''if \"${restart} > 0\" then \"read_restart md.restart.*\" else \"read_data %s\"''' % data_file, *(\"mass {id} {mass}\".format(id=i+1, mass=m) for i, m in enumerate(mass_map)) ] md_section = [ '''if \"${restart} == 0\" then \"velocity all create ${TEMP} %d\"''' % (random.randrange(rand_start - 1) + 1) ] if ensemble . startswith ( 'npt' ) and no_pbc : raise ValueError ( 'ensemble npt conflict with no_pcb' ) if ensemble in ( 'npt' , 'npt-i' , 'npt-iso' ,) : md_section . append ( 'fix 1 all npt temp ${TEMP} ${TEMP} ${TAU_T} iso ${PRES} ${PRES} ${TAU_P}' ) elif ensemble in ( 'npt-a' , 'npt-aniso' ,) : md_section . append ( 'fix 1 all npt temp ${TEMP} ${TEMP} ${TAU_T} aniso ${PRES} ${PRES} ${TAU_P}' ) elif ensemble in ( 'npt-t' , 'npt-tri' ,) : md_section . append ( 'fix 1 all npt temp ${TEMP} ${TEMP} ${TAU_T} tri ${PRES} ${PRES} ${TAU_P}' ) elif ensemble in ( 'nvt' ,) : md_section . append ( 'fix 1 all nvt temp ${TEMP} ${TEMP} ${TAU_T}' ) elif ensemble in ( 'nve' ,) : md_section . append ( 'fix 1 all nve' ) else : raise ValueError ( 'unknown ensemble: ' + ensemble ) if plumed_file : md_section . append ( f 'fix dpgen_plm all plumed plumedfile {plumed_file} outfile plumed.out' ) if no_pbc : md_section . extend ( [ 'velocity all zero linear', 'fix fm all momentum 1 linear 1 1 1', ] ) md_section . extend ( [ 'thermo_style custom step temp pe ke etotal press vol lx ly lz xy xz yz', 'thermo ${THERMO_FREQ}', 'dump 1 all custom ${DUMP_FREQ} %s/*%s id type x y z fx fy fz' % (LAMMPS_TRAJ_DIR, LAMMPS_TRAJ_SUFFIX), 'restart 10000 md.restart', ] ) run_section = [ 'timestep %f' % timestep, 'run ${NSTEPS} upto', ] return LammpsInputTemplate ( template ). substitute ( dict ( variables_section = '\\n' . join ( variables ), post_variables_section = post_variables_section , init_section = '\\n' . join ( init_section ), post_init_section = post_init_section , read_data_section = '\\n' . join ( read_data_section ), post_read_data_section = post_read_data_section , md_section = '\\n' . join ( md_section ), post_md_section = post_md_section , force_field_section = '\\n' . join ( force_field_section ), post_force_field_section = post_force_field_section , run_section = '\\n' . join ( run_section ), post_run_section = post_run_section , )) make_md_force_field_section def make_md_force_field_section ( models : List [ str ] ) View Source def make_md_force_field_section ( models : List [ str ] ) : deepmd_args = \"\" settings = [ 'pair_style deepmd %s out_freq ${THERMO_FREQ} out_file %s %s' % (' '.join(models), MODEL_DEVI_OUT, deepmd_args), 'pair_coeff * *', ] return settings Classes ExploreVariants class ExploreVariants ( __pydantic_self__ , ** data : Any ) View Source class ExploreVariants ( BaseModel ) : temp : List [ float ] \"\"\"Temperatures variants.\"\"\" pres : List [ float ] \"\"\"Pressures variants.\"\"\" others : Mapping [ str, Sequence[Any ] ] = dict () \"\"\" Other variants to be combined with. The key is the name of the variant, and the value is the list of values. For example, if you want to combine the variant 'LAMBDA' with values [0.0, 0.5, 1.0], you can set the others field to {'LAMBDA': [0.0, 0.5, 1.0]}. And in LAMMPS input template, you can use the variable ${LAMBDA} and v_LAMBDA to access the value. \"\"\" Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericLammpsContext class GenericLammpsContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager , config : ai2_kit . domain . lammps . GenericLammpsContextConfig ) GenericLammpsContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager, config: ai2_kit.domain.lammps.GenericLammpsContextConfig) View Source class GenericLammpsContext ( BaseCllContext ): config: GenericLammpsContextConfig Ancestors (in MRO) ai2_kit.domain.cll.BaseCllContext GenericLammpsContextConfig class GenericLammpsContextConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericLammpsContextConfig ( BaseModel ): script_template: BashTemplate lammps_cmd: str = 'lmp' concurrency: int = 5 Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericLammpsInput class GenericLammpsInput ( config : ai2_kit . domain . lammps . GenericLammpsInputConfig , type_map : List [ str ], mass_map : List [ float ], md_options : Union [ ai2_kit . domain . lammps . GenericLammpsInput . MdOptions , NoneType ] = None , fep_options : Union [ ai2_kit . domain . lammps . GenericLammpsInput . FepOptions , NoneType ] = None ) GenericLammpsInput(config: ai2_kit.domain.lammps.GenericLammpsInputConfig, type_map: List[str], mass_map: List[float], md_options: Union[ai2_kit.domain.lammps.GenericLammpsInput.MdOptions, NoneType] = None, fep_options: Union[ai2_kit.domain.lammps.GenericLammpsInput.FepOptions, NoneType] = None) View Source class GenericLammpsInput : @dataclass class MdOptions : models : List [ Artifact ] @dataclass class FepOptions : red_models : List [ Artifact ] neu_models : List [ Artifact ] config : GenericLammpsInputConfig type_map : List [ str ] mass_map : List [ float ] # The following options are mutex md_options : Optional [ MdOptions ] = None fep_options : Optional [ FepOptions ] = None Class variables FepOptions MdOptions fep_options md_options GenericLammpsInputConfig class GenericLammpsInputConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericLammpsInputConfig ( BaseModel ) : explore_vars : ExploreVariants \"\"\"Variants to be explored.\"\"\" n_wise : int = 0 \"\"\"The way of combining variants. 0 means cartesian product, 2 means 2-wise, etc. If n_wise is less than 2 or greater than total fields, the full combination will be used. It is strongly recommended to use n_wise when the full combination is too large. \"\"\" system_files : List [ str ] \"\"\"Artifacts of initial system data.\"\"\" plumed_config : Optional [ str ] \"\"\"Plumed config file content.\"\"\" no_pbc : bool = False tau_t : float = 0.1 tau_p : float = 0.5 timestep : float = 0.0005 sample_freq : int nsteps : int ensemble : Literal [ 'nvt', 'nvt-i', 'nvt-a', 'nvt-iso', 'nvt-aniso', 'npt', 'npt-t', 'npt-tri', 'nve' ] \"\"\"Ensemble to be used. nvt means constant volume and temperature. nvt-i means constant volume and temperature, with isotropic scaling. nvt-a means constant volume and temperature, with anisotropic scaling. nvt-iso means constant volume and temperature, with isotropic scaling. npt means constant pressure and temperature. npt-t means constant pressure and temperature, with isotropic scaling. npt-tri means constant pressure and temperature, with anisotropic scaling. nve means constant energy. \"\"\" input_template : Optional [ str ] \"\"\"Lammps input template file content.\"\"\" post_variables_section : str = '' post_init_section : str = '' post_read_data_section : str = '' post_force_field_section : str = '' post_md_section : str = '' post_run_section : str = '' Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . GenericLammpsOutput class GenericLammpsOutput ( model_devi_outputs : List [ ai2_kit . core . artifact . Artifact ] ) GenericLammpsOutput(model_devi_outputs: List[ai2_kit.core.artifact.Artifact]) View Source class GenericLammpsOutput ( ICllExploreOutput ) : model_devi_outputs : List [ Artifact ] def get_model_devi_dataset ( self ) -> List [ Artifact ] : return self . model_devi_outputs Ancestors (in MRO) ai2_kit.domain.cll.ICllExploreOutput abc.ABC Methods get_model_devi_dataset def get_model_devi_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_model_devi_dataset ( self ) -> List [ Artifact ] : return self . model_devi_outputs LammpsInputTemplate class LammpsInputTemplate ( template ) change delimiter from $ to $$ as $ is used a lot in lammps input file View Source class LammpsInputTemplate ( Template ): \"\"\" change delimiter from $ to $$ as $ is used a lot in lammps input file \"\"\" delimiter = '$$' Ancestors (in MRO) string.Template Class variables braceidpattern delimiter flags idpattern pattern Methods safe_substitute def safe_substitute ( self , mapping = {}, / , ** kws ) View Source def safe_substitute ( self , mapping = _sentinel_dict , / , ** kws ) : if mapping is _sentinel_dict : mapping = kws elif kws : mapping = _ChainMap ( kws , mapping ) # Helper function for . sub () def convert ( mo ) : named = mo . group ( 'named' ) or mo . group ( 'braced' ) if named is not None : try : return str ( mapping [ named ] ) except KeyError : return mo . group () if mo . group ( 'escaped' ) is not None : return self . delimiter if mo . group ( 'invalid' ) is not None : return mo . group () raise ValueError ( 'Unrecognized named group in pattern' , self . pattern ) return self . pattern . sub ( convert , self . template ) substitute def substitute ( self , mapping = {}, / , ** kws ) View Source def substitute ( self , mapping = _sentinel_dict , / , ** kws ) : if mapping is _sentinel_dict : mapping = kws elif kws : mapping = _ChainMap ( kws , mapping ) # Helper function for . sub () def convert ( mo ) : # Check the most common path first . named = mo . group ( 'named' ) or mo . group ( 'braced' ) if named is not None : return str ( mapping [ named ] ) if mo . group ( 'escaped' ) is not None : return self . delimiter if mo . group ( 'invalid' ) is not None : self . _invalid ( mo ) raise ValueError ( 'Unrecognized named group in pattern' , self . pattern ) return self . pattern . sub ( convert , self . template )","title":"Lammps"},{"location":"reference/ai2_kit/domain/lammps/#module-ai2_kitdomainlammps","text":"View Source from ai2_kit.core.script import BashTemplate , BashStep , BashScript from ai2_kit.core.artifact import Artifact , ArtifactDict from ai2_kit.core.log import get_logger from ai2_kit.core.job import gather_jobs from ai2_kit.core.util import split_list , dict_nested_get from typing import List , Literal , Optional , Union , Mapping , Sequence , Any from pydantic import BaseModel from dataclasses import dataclass from string import Template from allpairspy import AllPairs import os import itertools import random from .cll import BaseCllContext , ICllExploreOutput from .constant import ( MODEL_DEVI_OUT , MODEL_DEVI_NEU_OUT , MODEL_DEVI_RED_OUT , LAMMPS_TRAJ_DIR , LAMMPS_TRAJ_SUFFIX , ) from .data_helper import LammpsOutputHelper , PoscarHelper , convert_to_lammps_input_data logger = get_logger ( __name__ ) class ExploreVariants ( BaseModel ): temp : List [ float ] \"\"\"Temperatures variants.\"\"\" pres : List [ float ] \"\"\"Pressures variants.\"\"\" others : Mapping [ str , Sequence [ Any ]] = dict () \"\"\" Other variants to be combined with. The key is the name of the variant, and the value is the list of values. For example, if you want to combine the variant 'LAMBDA' with values [0.0, 0.5, 1.0], you can set the others field to {'LAMBDA': [0.0, 0.5, 1.0]}. And in LAMMPS input template, you can use the variable ${LAMBDA} and v_LAMBDA to access the value. \"\"\" class GenericLammpsInputConfig ( BaseModel ): explore_vars : ExploreVariants \"\"\"Variants to be explored.\"\"\" n_wise : int = 0 \"\"\"The way of combining variants. 0 means cartesian product, 2 means 2-wise, etc. If n_wise is less than 2 or greater than total fields, the full combination will be used. It is strongly recommended to use n_wise when the full combination is too large. \"\"\" system_files : List [ str ] \"\"\"Artifacts of initial system data.\"\"\" plumed_config : Optional [ str ] \"\"\"Plumed config file content.\"\"\" no_pbc : bool = False tau_t : float = 0.1 tau_p : float = 0.5 timestep : float = 0.0005 sample_freq : int nsteps : int ensemble : Literal [ 'nvt' , 'nvt-i' , 'nvt-a' , 'nvt-iso' , 'nvt-aniso' , 'npt' , 'npt-t' , 'npt-tri' , 'nve' ] \"\"\"Ensemble to be used. nvt means constant volume and temperature. nvt-i means constant volume and temperature, with isotropic scaling. nvt-a means constant volume and temperature, with anisotropic scaling. nvt-iso means constant volume and temperature, with isotropic scaling. npt means constant pressure and temperature. npt-t means constant pressure and temperature, with isotropic scaling. npt-tri means constant pressure and temperature, with anisotropic scaling. nve means constant energy. \"\"\" input_template : Optional [ str ] \"\"\"Lammps input template file content.\"\"\" post_variables_section : str = '' post_init_section : str = '' post_read_data_section : str = '' post_force_field_section : str = '' post_md_section : str = '' post_run_section : str = '' class GenericLammpsContextConfig ( BaseModel ): script_template : BashTemplate lammps_cmd : str = 'lmp' concurrency : int = 5 @dataclass class GenericLammpsInput : @dataclass class MdOptions : models : List [ Artifact ] @dataclass class FepOptions : red_models : List [ Artifact ] neu_models : List [ Artifact ] config : GenericLammpsInputConfig type_map : List [ str ] mass_map : List [ float ] # The following options are mutex md_options : Optional [ MdOptions ] = None fep_options : Optional [ FepOptions ] = None @dataclass class GenericLammpsContext ( BaseCllContext ): config : GenericLammpsContextConfig @dataclass class GenericLammpsOutput ( ICllExploreOutput ): model_devi_outputs : List [ Artifact ] def get_model_devi_dataset ( self ) -> List [ Artifact ]: return self . model_devi_outputs async def generic_lammps ( input : GenericLammpsInput , ctx : GenericLammpsContext ): executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) input_data_dir , tasks_dir = executor . setup_workspace ( work_dir , [ 'input_data' , 'tasks' ]) systems = ctx . resource_manager . resolve_artifacts ( input . config . system_files ) # prepare lammps input data # TODO: refactor the way of handling different types of input # TODO: handle more data format, for example, cp2k output poscar_files : List [ Artifact ] = [] for system_file in systems : if PoscarHelper . is_match ( system_file ): poscar_files . append ( system_file ) else : raise ValueError ( f 'unsupported system file type: { system_file } ' ) input_data_files : List [ ArtifactDict ] = executor . run_python_fn ( convert_to_lammps_input_data )( poscar_files = [ a . to_dict () for a in poscar_files ], base_dir = input_data_dir , type_map = input . type_map , ) combination_fields : List [ str ] = [ 'data_file' , 'temp' , 'pres' ] combination_values : Sequence [ Sequence [ Any ]] = [ input_data_files , input . config . explore_vars . temp , input . config . explore_vars . pres , ] for k , v in input . config . explore_vars . others . items (): combination_fields . append ( k ) combination_values . append ( v ) # type: ignore if 1 < input . config . n_wise <= len ( combination_fields ): logger . info ( 'using %d -wise combination' , input . config . n_wise ) combinations = AllPairs ( combination_values , n = input . config . n_wise ) else : logger . info ( 'using full combination' ) combinations = itertools . product ( * combination_values ) lammps_task_dirs : List [ ArtifactDict ] = [] lammps_input_file_name = 'lammps.input' for i , combination in enumerate ( combinations ): data_file : ArtifactDict = combination [ 0 ] temp , pres = combination [ 1 : 3 ] others_dict = dict ( zip ( combination_fields [ 3 :], combination [ 3 :])) lammps_task_dir = os . path . join ( tasks_dir , f ' { i : 06d } ' ) executor . mkdir ( os . path . join ( lammps_task_dir , LAMMPS_TRAJ_DIR )) # create dump directory for lammps or else will get error if input . md_options : force_field_section = make_md_force_field_section ( models = [ a . url for a in input . md_options . models ], ) elif input . fep_options : if 'LAMBDA_f' not in others_dict : raise ValueError ( 'LAMBDA_f must be set when using FEP mode!' ) # inject the following variables for FEP mode others_dict [ 'LAMBDA_i' ] = '1-v_LAMBDA_f' # should not have space, or you must quote others_dict [ 'plus' ] = 1 others_dict [ 'minus' ] = - 1 force_field_section = make_fep_force_field_section ( neu_models = [ a . url for a in input . fep_options . neu_models ], red_models = [ a . url for a in input . fep_options . red_models ], ) else : raise ValueError ( 'one and only one of md_options or fep_options must be set' ) plumed_file = None # plumed_config could be overrided by the attrs of data_file plumed_config = dict_nested_get ( data_file , [ 'attrs' , 'lammps' , 'plumed_config' ], input . config . plumed_config ) if plumed_config and isinstance ( plumed_config , str ): plumed_file = 'plumed.input' plumed_file_path = os . path . join ( lammps_task_dir , plumed_file ) logger . info ( f 'found plumed config, generate { plumed_file_path } ' ) executor . dump_text ( plumed_config , plumed_file_path ) template = input . config . input_template or DEFAULT_LAMMPS_INPUT_TEMPLATE input_text = make_lammps_input ( data_file = data_file [ 'url' ], nsteps = input . config . nsteps , timestep = input . config . timestep , trj_freq = input . config . sample_freq , temp = temp , pres = pres , tau_t = input . config . tau_t , tau_p = input . config . tau_p , ensemble = input . config . ensemble , mass_map = input . mass_map , others_dict = others_dict , force_field_section = force_field_section , template = template , post_variables_section = input . config . post_variables_section , post_init_section = input . config . post_init_section , post_read_data_section = input . config . post_read_data_section , post_force_field_section = input . config . post_force_field_section , post_md_section = input . config . post_md_section , post_run_section = input . config . post_run_section , plumed_file = plumed_file , no_pbc = False , rand_start = 1_000_000 , ) input_file_path = os . path . join ( lammps_task_dir , lammps_input_file_name ) logger . info ( f 'generate lammps config { input_file_path } ' ) executor . dump_text ( input_text , input_file_path ) lammps_task_dirs . append ({ 'url' : lammps_task_dir , 'attrs' : data_file [ 'attrs' ]}) # type: ignore # build scripts and submit lammps_cmd = ctx . config . lammps_cmd base_cmd = f ' { lammps_cmd } -i { lammps_input_file_name } ' cmd = f '''if [ -f md.restart.* ]; then { base_cmd } -v restart 1; else { base_cmd } -v restart 0; fi''' # generate steps steps = [] for lammps_task_dir in lammps_task_dirs : steps . append ( BashStep ( cwd = lammps_task_dir [ 'url' ], cmd = cmd , checkpoint = 'lammps' )) # submit jobs by the number of concurrency jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )): if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/lammps/ { i } : { tasks_dir } ' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) outputs = [ Artifact . of ( url = task_dir [ 'url' ], executor = executor . name , format = LammpsOutputHelper . format , attrs = task_dir [ 'attrs' ], ) for task_dir in lammps_task_dirs ] # type: ignore return GenericLammpsOutput ( model_devi_outputs = outputs ) def make_md_force_field_section ( models : List [ str ]): deepmd_args = \"\" settings = [ 'pair_style deepmd %s out_freq $ {THERMO_FREQ} out_file %s %s ' % ( ' ' . join ( models ), MODEL_DEVI_OUT , deepmd_args ), 'pair_coeff * *' , ] return settings def make_fep_force_field_section ( neu_models : List [ str ], red_models : List [ str ]): deepmd_args = \"\" settings = [ 'pair_style hybrid/overlay &' , ' deepmd %s out_freq $ {THERMO_FREQ} out_file %s %s &' % ( ' ' . join ( neu_models ), MODEL_DEVI_NEU_OUT , deepmd_args ), ' deepmd %s out_freq $ {THERMO_FREQ} out_file %s %s ' % ( ' ' . join ( red_models ), MODEL_DEVI_RED_OUT , deepmd_args ), 'pair_coeff * * deepmd 1 *' , 'pair_coeff * * deepmd 2 *' , '' , 'fix sampling_PES all adapt 0 &' , ' pair deepmd:1 scale * * v_LAMBDA_f &' , ' pair deepmd:2 scale * * v_LAMBDA_i &' , ' scale yes' , ] return settings def make_lammps_input ( data_file : str , nsteps : int , timestep : float , trj_freq : int , temp : float , pres : float , tau_t : float , tau_p : float , ensemble : str , mass_map : List [ float ], others_dict : Mapping [ str , Any ], template : str , post_variables_section : str , post_init_section : str , post_read_data_section : str , post_force_field_section : str , post_md_section : str , post_run_section : str , force_field_section : List [ str ], plumed_file : Optional [ str ] = None , no_pbc = False , rand_start = 1_000_000 , ): # FIXME: I am not sure if it is a good idea to fix it automatically # maybe we should consider raise an error here if not ensemble . startswith ( 'npt' ): pres = - 1 variables = [ '# required variables' , 'variable NSTEPS equal %d ' % nsteps , 'variable THERMO_FREQ equal %d ' % trj_freq , 'variable DUMP_FREQ equal %d ' % trj_freq , 'variable TEMP equal %f ' % temp , 'variable PRES equal %f ' % pres , 'variable TAU_T equal %f ' % tau_t , 'variable TAU_P equal %f ' % tau_p , '' , '# custom variables (if any)' , ] for k , v in others_dict . items (): variables . append ( f 'variable { k } equal { v } ' ) init_section = [ 'boundary ' + ( 'f f f' if no_pbc else 'p p p' ), ] read_data_section = [ '''if \"${restart} > 0\" then \"read_restart md.restart.*\" else \"read_data %s\"''' % data_file , * ( \"mass {id} {mass} \" . format ( id = i + 1 , mass = m ) for i , m in enumerate ( mass_map )) ] md_section = [ '''if \"${restart} == 0\" then \"velocity all create ${TEMP} %d\"''' % ( random . randrange ( rand_start - 1 ) + 1 ) ] if ensemble . startswith ( 'npt' ) and no_pbc : raise ValueError ( 'ensemble npt conflict with no_pcb' ) if ensemble in ( 'npt' , 'npt-i' , 'npt-iso' ,): md_section . append ( 'fix 1 all npt temp $ {TEMP} $ {TEMP} $ {TAU_T} iso $ {PRES} $ {PRES} $ {TAU_P} ' ) elif ensemble in ( 'npt-a' , 'npt-aniso' ,): md_section . append ( 'fix 1 all npt temp $ {TEMP} $ {TEMP} $ {TAU_T} aniso $ {PRES} $ {PRES} $ {TAU_P} ' ) elif ensemble in ( 'npt-t' , 'npt-tri' ,): md_section . append ( 'fix 1 all npt temp $ {TEMP} $ {TEMP} $ {TAU_T} tri $ {PRES} $ {PRES} $ {TAU_P} ' ) elif ensemble in ( 'nvt' ,): md_section . append ( 'fix 1 all nvt temp $ {TEMP} $ {TEMP} $ {TAU_T} ' ) elif ensemble in ( 'nve' ,): md_section . append ( 'fix 1 all nve' ) else : raise ValueError ( 'unknown ensemble: ' + ensemble ) if plumed_file : md_section . append ( f 'fix dpgen_plm all plumed plumedfile { plumed_file } outfile plumed.out' ) if no_pbc : md_section . extend ([ 'velocity all zero linear' , 'fix fm all momentum 1 linear 1 1 1' , ]) md_section . extend ([ 'thermo_style custom step temp pe ke etotal press vol lx ly lz xy xz yz' , 'thermo $ {THERMO_FREQ} ' , 'dump 1 all custom $ {DUMP_FREQ} %s /* %s id type x y z fx fy fz' % ( LAMMPS_TRAJ_DIR , LAMMPS_TRAJ_SUFFIX ), 'restart 10000 md.restart' , ]) run_section = [ 'timestep %f ' % timestep , 'run $ {NSTEPS} upto' , ] return LammpsInputTemplate ( template ) . substitute ( dict ( variables_section = ' \\n ' . join ( variables ), post_variables_section = post_variables_section , init_section = ' \\n ' . join ( init_section ), post_init_section = post_init_section , read_data_section = ' \\n ' . join ( read_data_section ), post_read_data_section = post_read_data_section , md_section = ' \\n ' . join ( md_section ), post_md_section = post_md_section , force_field_section = ' \\n ' . join ( force_field_section ), post_force_field_section = post_force_field_section , run_section = ' \\n ' . join ( run_section ), post_run_section = post_run_section , )) class LammpsInputTemplate ( Template ): \"\"\" change delimiter from $ to $$ as $ is used a lot in lammps input file \"\"\" delimiter = '$$' DEFAULT_LAMMPS_INPUT_TEMPLATE = ''' \\ ## variables_section $$variables_section ## post_variables_section $$post_variables_section ## init_section units metal atom_style atomic $$init_section ## post_init_section $$post_init_section ## read_data_section $$read_data_section ## post_read_data_section $$post_read_data_section ## force_field_section $$force_field_section ## post_force_field_section $$post_force_field_section ## md_section $$md_section ## post_md_section $$post_md_section ## run_section $$run_section ## post_run_section $$post_run_section '''","title":"Module ai2_kit.domain.lammps"},{"location":"reference/ai2_kit/domain/lammps/#variables","text":"DEFAULT_LAMMPS_INPUT_TEMPLATE LAMMPS_TRAJ_DIR LAMMPS_TRAJ_SUFFIX MODEL_DEVI_NEU_OUT MODEL_DEVI_OUT MODEL_DEVI_RED_OUT logger","title":"Variables"},{"location":"reference/ai2_kit/domain/lammps/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/domain/lammps/#generic_lammps","text":"def generic_lammps ( input : ai2_kit . domain . lammps . GenericLammpsInput , ctx : ai2_kit . domain . lammps . GenericLammpsContext ) View Source async def generic_lammps ( input : GenericLammpsInput , ctx : GenericLammpsContext ) : executor = ctx . resource_manager . default_executor # setup workspace work_dir = os . path . join ( executor . work_dir , ctx . path_prefix ) input_data_dir , tasks_dir = executor . setup_workspace ( work_dir , [ 'input_data', 'tasks' ] ) systems = ctx . resource_manager . resolve_artifacts ( input . config . system_files ) # prepare lammps input data # TODO : refactor the way of handling different types of input # TODO : handle more data format , for example , cp2k output poscar_files : List [ Artifact ] = [] for system_file in systems : if PoscarHelper . is_match ( system_file ) : poscar_files . append ( system_file ) else : raise ValueError ( f 'unsupported system file type: {system_file}' ) input_data_files : List [ ArtifactDict ] = executor . run_python_fn ( convert_to_lammps_input_data )( poscar_files =[ a.to_dict() for a in poscar_files ] , base_dir = input_data_dir , type_map = input . type_map , ) combination_fields : List [ str ] = [ 'data_file', 'temp', 'pres' ] combination_values : Sequence [ Sequence[Any ] ] = [ input_data_files, input.config.explore_vars.temp, input.config.explore_vars.pres, ] for k , v in input . config . explore_vars . others . items () : combination_fields . append ( k ) combination_values . append ( v ) # type : ignore if 1 < input . config . n_wise <= len ( combination_fields ) : logger . info ( 'using %d-wise combination' , input . config . n_wise ) combinations = AllPairs ( combination_values , n = input . config . n_wise ) else : logger . info ( 'using full combination' ) combinations = itertools . product ( * combination_values ) lammps_task_dirs : List [ ArtifactDict ] = [] lammps_input_file_name = 'lammps.input' for i , combination in enumerate ( combinations ) : data_file : ArtifactDict = combination [ 0 ] temp , pres = combination [ 1:3 ] others_dict = dict ( zip ( combination_fields [ 3: ] , combination [ 3: ] )) lammps_task_dir = os . path . join ( tasks_dir , f '{i:06d}' ) executor . mkdir ( os . path . join ( lammps_task_dir , LAMMPS_TRAJ_DIR )) # create dump directory for lammps or else will get error if input . md_options : force_field_section = make_md_force_field_section ( models =[ a.url for a in input.md_options.models ] , ) elif input . fep_options : if 'LAMBDA_f' not in others_dict : raise ValueError ( 'LAMBDA_f must be set when using FEP mode!' ) # inject the following variables for FEP mode others_dict [ 'LAMBDA_i' ] = '1-v_LAMBDA_f' # should not have space , or you must quote others_dict [ 'plus' ] = 1 others_dict [ 'minus' ] = - 1 force_field_section = make_fep_force_field_section ( neu_models =[ a.url for a in input.fep_options.neu_models ] , red_models =[ a.url for a in input.fep_options.red_models ] , ) else : raise ValueError ( 'one and only one of md_options or fep_options must be set' ) plumed_file = None # plumed_config could be overrided by the attrs of data_file plumed_config = dict_nested_get ( data_file , [ 'attrs', 'lammps', 'plumed_config' ] , input . config . plumed_config ) if plumed_config and isinstance ( plumed_config , str ) : plumed_file = 'plumed.input' plumed_file_path = os . path . join ( lammps_task_dir , plumed_file ) logger . info ( f 'found plumed config, generate {plumed_file_path}' ) executor . dump_text ( plumed_config , plumed_file_path ) template = input . config . input_template or DEFAULT_LAMMPS_INPUT_TEMPLATE input_text = make_lammps_input ( data_file = data_file [ 'url' ] , nsteps = input . config . nsteps , timestep = input . config . timestep , trj_freq = input . config . sample_freq , temp = temp , pres = pres , tau_t = input . config . tau_t , tau_p = input . config . tau_p , ensemble = input . config . ensemble , mass_map = input . mass_map , others_dict = others_dict , force_field_section = force_field_section , template = template , post_variables_section = input . config . post_variables_section , post_init_section = input . config . post_init_section , post_read_data_section = input . config . post_read_data_section , post_force_field_section = input . config . post_force_field_section , post_md_section = input . config . post_md_section , post_run_section = input . config . post_run_section , plumed_file = plumed_file , no_pbc = False , rand_start = 1 _000_000 , ) input_file_path = os . path . join ( lammps_task_dir , lammps_input_file_name ) logger . info ( f 'generate lammps config {input_file_path}' ) executor . dump_text ( input_text , input_file_path ) lammps_task_dirs . append ( { 'url' : lammps_task_dir , 'attrs' : data_file [ 'attrs' ] } ) # type : ignore # build scripts and submit lammps_cmd = ctx . config . lammps_cmd base_cmd = f '{lammps_cmd} -i {lammps_input_file_name}' cmd = f '''if [ -f md.restart.* ]; then {base_cmd} -v restart 1; else {base_cmd} -v restart 0; fi''' # generate steps steps = [] for lammps_task_dir in lammps_task_dirs : steps . append ( BashStep ( cwd = lammps_task_dir [ 'url' ] , cmd = cmd , checkpoint = 'lammps' )) # submit jobs by the number of concurrency jobs = [] for i , steps_group in enumerate ( split_list ( steps , ctx . config . concurrency )) : if not steps_group : continue script = BashScript ( template = ctx . config . script_template , steps = steps_group , ) job = executor . submit ( script . render (), cwd = tasks_dir , checkpoint_key = f 'submit-job/lammps/{i}:{tasks_dir}' ) jobs . append ( job ) await gather_jobs ( jobs , max_tries = 2 ) outputs = [ Artifact.of( url=task_dir['url' ] , executor = executor . name , format = LammpsOutputHelper . format , attrs = task_dir [ 'attrs' ] , ) for task_dir in lammps_task_dirs ] # type : ignore return GenericLammpsOutput ( model_devi_outputs = outputs )","title":"generic_lammps"},{"location":"reference/ai2_kit/domain/lammps/#make_fep_force_field_section","text":"def make_fep_force_field_section ( neu_models : List [ str ], red_models : List [ str ] ) View Source def make_fep_force_field_section ( neu_models : List [ str ] , red_models : List [ str ] ) : deepmd_args = \"\" settings = [ 'pair_style hybrid/overlay &', ' deepmd %s out_freq ${THERMO_FREQ} out_file %s %s &' %(' '.join(neu_models), MODEL_DEVI_NEU_OUT, deepmd_args), ' deepmd %s out_freq ${THERMO_FREQ} out_file %s %s' %(' '.join(red_models), MODEL_DEVI_RED_OUT, deepmd_args), 'pair_coeff * * deepmd 1 *', 'pair_coeff * * deepmd 2 *', '', 'fix sampling_PES all adapt 0 &', ' pair deepmd:1 scale * * v_LAMBDA_f &', ' pair deepmd:2 scale * * v_LAMBDA_i &', ' scale yes', ] return settings","title":"make_fep_force_field_section"},{"location":"reference/ai2_kit/domain/lammps/#make_lammps_input","text":"def make_lammps_input ( data_file : str , nsteps : int , timestep : float , trj_freq : int , temp : float , pres : float , tau_t : float , tau_p : float , ensemble : str , mass_map : List [ float ], others_dict : Mapping [ str , Any ], template : str , post_variables_section : str , post_init_section : str , post_read_data_section : str , post_force_field_section : str , post_md_section : str , post_run_section : str , force_field_section : List [ str ], plumed_file : Union [ str , NoneType ] = None , no_pbc = False , rand_start = 1000000 ) View Source def make_lammps_input ( data_file : str , nsteps : int , timestep : float , trj_freq : int , temp : float , pres : float , tau_t : float , tau_p : float , ensemble : str , mass_map : List [ float ] , others_dict : Mapping [ str, Any ] , template : str , post_variables_section : str , post_init_section : str , post_read_data_section : str , post_force_field_section : str , post_md_section : str , post_run_section : str , force_field_section : List [ str ] , plumed_file : Optional [ str ] = None , no_pbc = False , rand_start = 1 _000_000 , ) : # FIXME : I am not sure if it is a good idea to fix it automatically # maybe we should consider raise an error here if not ensemble . startswith ( 'npt' ) : pres = - 1 variables = [ '# required variables', 'variable NSTEPS equal %d' % nsteps, 'variable THERMO_FREQ equal %d' % trj_freq, 'variable DUMP_FREQ equal %d' % trj_freq, 'variable TEMP equal %f' % temp, 'variable PRES equal %f' % pres, 'variable TAU_T equal %f' % tau_t, 'variable TAU_P equal %f' % tau_p, '', '# custom variables (if any)', ] for k , v in others_dict . items () : variables . append ( f 'variable {k} equal {v}' ) init_section = [ 'boundary ' + ('f f f' if no_pbc else 'p p p'), ] read_data_section = [ '''if \"${restart} > 0\" then \"read_restart md.restart.*\" else \"read_data %s\"''' % data_file, *(\"mass {id} {mass}\".format(id=i+1, mass=m) for i, m in enumerate(mass_map)) ] md_section = [ '''if \"${restart} == 0\" then \"velocity all create ${TEMP} %d\"''' % (random.randrange(rand_start - 1) + 1) ] if ensemble . startswith ( 'npt' ) and no_pbc : raise ValueError ( 'ensemble npt conflict with no_pcb' ) if ensemble in ( 'npt' , 'npt-i' , 'npt-iso' ,) : md_section . append ( 'fix 1 all npt temp ${TEMP} ${TEMP} ${TAU_T} iso ${PRES} ${PRES} ${TAU_P}' ) elif ensemble in ( 'npt-a' , 'npt-aniso' ,) : md_section . append ( 'fix 1 all npt temp ${TEMP} ${TEMP} ${TAU_T} aniso ${PRES} ${PRES} ${TAU_P}' ) elif ensemble in ( 'npt-t' , 'npt-tri' ,) : md_section . append ( 'fix 1 all npt temp ${TEMP} ${TEMP} ${TAU_T} tri ${PRES} ${PRES} ${TAU_P}' ) elif ensemble in ( 'nvt' ,) : md_section . append ( 'fix 1 all nvt temp ${TEMP} ${TEMP} ${TAU_T}' ) elif ensemble in ( 'nve' ,) : md_section . append ( 'fix 1 all nve' ) else : raise ValueError ( 'unknown ensemble: ' + ensemble ) if plumed_file : md_section . append ( f 'fix dpgen_plm all plumed plumedfile {plumed_file} outfile plumed.out' ) if no_pbc : md_section . extend ( [ 'velocity all zero linear', 'fix fm all momentum 1 linear 1 1 1', ] ) md_section . extend ( [ 'thermo_style custom step temp pe ke etotal press vol lx ly lz xy xz yz', 'thermo ${THERMO_FREQ}', 'dump 1 all custom ${DUMP_FREQ} %s/*%s id type x y z fx fy fz' % (LAMMPS_TRAJ_DIR, LAMMPS_TRAJ_SUFFIX), 'restart 10000 md.restart', ] ) run_section = [ 'timestep %f' % timestep, 'run ${NSTEPS} upto', ] return LammpsInputTemplate ( template ). substitute ( dict ( variables_section = '\\n' . join ( variables ), post_variables_section = post_variables_section , init_section = '\\n' . join ( init_section ), post_init_section = post_init_section , read_data_section = '\\n' . join ( read_data_section ), post_read_data_section = post_read_data_section , md_section = '\\n' . join ( md_section ), post_md_section = post_md_section , force_field_section = '\\n' . join ( force_field_section ), post_force_field_section = post_force_field_section , run_section = '\\n' . join ( run_section ), post_run_section = post_run_section , ))","title":"make_lammps_input"},{"location":"reference/ai2_kit/domain/lammps/#make_md_force_field_section","text":"def make_md_force_field_section ( models : List [ str ] ) View Source def make_md_force_field_section ( models : List [ str ] ) : deepmd_args = \"\" settings = [ 'pair_style deepmd %s out_freq ${THERMO_FREQ} out_file %s %s' % (' '.join(models), MODEL_DEVI_OUT, deepmd_args), 'pair_coeff * *', ] return settings","title":"make_md_force_field_section"},{"location":"reference/ai2_kit/domain/lammps/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/lammps/#explorevariants","text":"class ExploreVariants ( __pydantic_self__ , ** data : Any ) View Source class ExploreVariants ( BaseModel ) : temp : List [ float ] \"\"\"Temperatures variants.\"\"\" pres : List [ float ] \"\"\"Pressures variants.\"\"\" others : Mapping [ str, Sequence[Any ] ] = dict () \"\"\" Other variants to be combined with. The key is the name of the variant, and the value is the list of values. For example, if you want to combine the variant 'LAMBDA' with values [0.0, 0.5, 1.0], you can set the others field to {'LAMBDA': [0.0, 0.5, 1.0]}. And in LAMMPS input template, you can use the variable ${LAMBDA} and v_LAMBDA to access the value. \"\"\"","title":"ExploreVariants"},{"location":"reference/ai2_kit/domain/lammps/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/lammps/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/lammps/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/lammps/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/lammps/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/lammps/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/lammps/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/lammps/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/lammps/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/lammps/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/lammps/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/lammps/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/lammps/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/lammps/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/lammps/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/lammps/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/lammps/#genericlammpscontext","text":"class GenericLammpsContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager , config : ai2_kit . domain . lammps . GenericLammpsContextConfig ) GenericLammpsContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager, config: ai2_kit.domain.lammps.GenericLammpsContextConfig) View Source class GenericLammpsContext ( BaseCllContext ): config: GenericLammpsContextConfig","title":"GenericLammpsContext"},{"location":"reference/ai2_kit/domain/lammps/#ancestors-in-mro_1","text":"ai2_kit.domain.cll.BaseCllContext","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/lammps/#genericlammpscontextconfig","text":"class GenericLammpsContextConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericLammpsContextConfig ( BaseModel ): script_template: BashTemplate lammps_cmd: str = 'lmp' concurrency: int = 5","title":"GenericLammpsContextConfig"},{"location":"reference/ai2_kit/domain/lammps/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/lammps/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/lammps/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/lammps/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/lammps/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/lammps/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/lammps/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/lammps/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/lammps/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/lammps/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/lammps/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/lammps/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/lammps/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/lammps/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/lammps/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/lammps/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/lammps/#genericlammpsinput","text":"class GenericLammpsInput ( config : ai2_kit . domain . lammps . GenericLammpsInputConfig , type_map : List [ str ], mass_map : List [ float ], md_options : Union [ ai2_kit . domain . lammps . GenericLammpsInput . MdOptions , NoneType ] = None , fep_options : Union [ ai2_kit . domain . lammps . GenericLammpsInput . FepOptions , NoneType ] = None ) GenericLammpsInput(config: ai2_kit.domain.lammps.GenericLammpsInputConfig, type_map: List[str], mass_map: List[float], md_options: Union[ai2_kit.domain.lammps.GenericLammpsInput.MdOptions, NoneType] = None, fep_options: Union[ai2_kit.domain.lammps.GenericLammpsInput.FepOptions, NoneType] = None) View Source class GenericLammpsInput : @dataclass class MdOptions : models : List [ Artifact ] @dataclass class FepOptions : red_models : List [ Artifact ] neu_models : List [ Artifact ] config : GenericLammpsInputConfig type_map : List [ str ] mass_map : List [ float ] # The following options are mutex md_options : Optional [ MdOptions ] = None fep_options : Optional [ FepOptions ] = None","title":"GenericLammpsInput"},{"location":"reference/ai2_kit/domain/lammps/#class-variables_2","text":"FepOptions MdOptions fep_options md_options","title":"Class variables"},{"location":"reference/ai2_kit/domain/lammps/#genericlammpsinputconfig","text":"class GenericLammpsInputConfig ( __pydantic_self__ , ** data : Any ) View Source class GenericLammpsInputConfig ( BaseModel ) : explore_vars : ExploreVariants \"\"\"Variants to be explored.\"\"\" n_wise : int = 0 \"\"\"The way of combining variants. 0 means cartesian product, 2 means 2-wise, etc. If n_wise is less than 2 or greater than total fields, the full combination will be used. It is strongly recommended to use n_wise when the full combination is too large. \"\"\" system_files : List [ str ] \"\"\"Artifacts of initial system data.\"\"\" plumed_config : Optional [ str ] \"\"\"Plumed config file content.\"\"\" no_pbc : bool = False tau_t : float = 0.1 tau_p : float = 0.5 timestep : float = 0.0005 sample_freq : int nsteps : int ensemble : Literal [ 'nvt', 'nvt-i', 'nvt-a', 'nvt-iso', 'nvt-aniso', 'npt', 'npt-t', 'npt-tri', 'nve' ] \"\"\"Ensemble to be used. nvt means constant volume and temperature. nvt-i means constant volume and temperature, with isotropic scaling. nvt-a means constant volume and temperature, with anisotropic scaling. nvt-iso means constant volume and temperature, with isotropic scaling. npt means constant pressure and temperature. npt-t means constant pressure and temperature, with isotropic scaling. npt-tri means constant pressure and temperature, with anisotropic scaling. nve means constant energy. \"\"\" input_template : Optional [ str ] \"\"\"Lammps input template file content.\"\"\" post_variables_section : str = '' post_init_section : str = '' post_read_data_section : str = '' post_force_field_section : str = '' post_md_section : str = '' post_run_section : str = ''","title":"GenericLammpsInputConfig"},{"location":"reference/ai2_kit/domain/lammps/#ancestors-in-mro_3","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/lammps/#class-variables_3","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/lammps/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/lammps/#construct_2","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/lammps/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/lammps/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/lammps/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/lammps/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/lammps/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/lammps/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/lammps/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/lammps/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/lammps/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/lammps/#copy_2","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/lammps/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/lammps/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/lammps/#genericlammpsoutput","text":"class GenericLammpsOutput ( model_devi_outputs : List [ ai2_kit . core . artifact . Artifact ] ) GenericLammpsOutput(model_devi_outputs: List[ai2_kit.core.artifact.Artifact]) View Source class GenericLammpsOutput ( ICllExploreOutput ) : model_devi_outputs : List [ Artifact ] def get_model_devi_dataset ( self ) -> List [ Artifact ] : return self . model_devi_outputs","title":"GenericLammpsOutput"},{"location":"reference/ai2_kit/domain/lammps/#ancestors-in-mro_4","text":"ai2_kit.domain.cll.ICllExploreOutput abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/lammps/#methods_3","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/lammps/#get_model_devi_dataset","text":"def get_model_devi_dataset ( self ) -> List [ ai2_kit . core . artifact . Artifact ] View Source def get_model_devi_dataset ( self ) -> List [ Artifact ] : return self . model_devi_outputs","title":"get_model_devi_dataset"},{"location":"reference/ai2_kit/domain/lammps/#lammpsinputtemplate","text":"class LammpsInputTemplate ( template ) change delimiter from $ to $$ as $ is used a lot in lammps input file View Source class LammpsInputTemplate ( Template ): \"\"\" change delimiter from $ to $$ as $ is used a lot in lammps input file \"\"\" delimiter = '$$'","title":"LammpsInputTemplate"},{"location":"reference/ai2_kit/domain/lammps/#ancestors-in-mro_5","text":"string.Template","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/lammps/#class-variables_4","text":"braceidpattern delimiter flags idpattern pattern","title":"Class variables"},{"location":"reference/ai2_kit/domain/lammps/#methods_4","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/lammps/#safe_substitute","text":"def safe_substitute ( self , mapping = {}, / , ** kws ) View Source def safe_substitute ( self , mapping = _sentinel_dict , / , ** kws ) : if mapping is _sentinel_dict : mapping = kws elif kws : mapping = _ChainMap ( kws , mapping ) # Helper function for . sub () def convert ( mo ) : named = mo . group ( 'named' ) or mo . group ( 'braced' ) if named is not None : try : return str ( mapping [ named ] ) except KeyError : return mo . group () if mo . group ( 'escaped' ) is not None : return self . delimiter if mo . group ( 'invalid' ) is not None : return mo . group () raise ValueError ( 'Unrecognized named group in pattern' , self . pattern ) return self . pattern . sub ( convert , self . template )","title":"safe_substitute"},{"location":"reference/ai2_kit/domain/lammps/#substitute","text":"def substitute ( self , mapping = {}, / , ** kws ) View Source def substitute ( self , mapping = _sentinel_dict , / , ** kws ) : if mapping is _sentinel_dict : mapping = kws elif kws : mapping = _ChainMap ( kws , mapping ) # Helper function for . sub () def convert ( mo ) : # Check the most common path first . named = mo . group ( 'named' ) or mo . group ( 'braced' ) if named is not None : return str ( mapping [ named ] ) if mo . group ( 'escaped' ) is not None : return self . delimiter if mo . group ( 'invalid' ) is not None : self . _invalid ( mo ) raise ValueError ( 'Unrecognized named group in pattern' , self . pattern ) return self . pattern . sub ( convert , self . template )","title":"substitute"},{"location":"reference/ai2_kit/domain/selector/","text":"Module ai2_kit.domain.selector View Source from ai2_kit.core.artifact import Artifact from ai2_kit.core.log import get_logger from typing import List from io import StringIO from pydantic import BaseModel from dataclasses import dataclass import pandas as pd from .data_helper import LammpsOutputHelper from .cll import ICllSelectorOutput , BaseCllContext logger = get_logger ( __name__ ) class ThresholdSelectorInputConfig ( BaseModel ): f_trust_lo : float f_trust_hi : float @dataclass class ThresholdSelectorContext ( BaseCllContext ): ... @dataclass class ThresholdSelectorOutput ( ICllSelectorOutput ): model_devi_data : List [ Artifact ] passing_rate : float def get_model_devi_dataset ( self ): return self . model_devi_data def get_passing_rate ( self ) -> float : return self . passing_rate @dataclass class ThresholdSelectorInput : config : ThresholdSelectorInputConfig model_devi_data : List [ Artifact ] model_devi_out_filename : str def set_model_devi_dataset ( self , data : List [ Artifact ]): self . model_devi_data = data async def threshold_selector ( input : ThresholdSelectorInput , ctx : ThresholdSelectorContext ): executor = ctx . resource_manager . default_executor f_trust_lo = input . config . f_trust_lo f_trust_hi = input . config . f_trust_hi col_force = 'avg_devi_f' logger . info ( 'criteria: %f <= %s < %f ' , f_trust_lo , col_force , f_trust_hi ) total_count = 0 passed_count = 0 # TODO: support output of different software for candidate in input . model_devi_data : if LammpsOutputHelper . is_match ( candidate ): model_devi_out_file = LammpsOutputHelper ( candidate ) . get_model_devi_file ( input . model_devi_out_filename ) . url else : raise ValueError ( 'unknown model_devi_data types' ) logger . info ( 'start to analysis file: %s ' , model_devi_out_file ) text = executor . load_text ( model_devi_out_file ) df = pd . read_csv ( StringIO ( text . lstrip ( '#' )), delim_whitespace = True ) # layout: # step max_devi_v min_devi_v avg_devi_v max_devi_f min_devi_f avg_devi_f # 0 0 0.006793 0.000672 0.003490 0.143317 0.005612 0.026106 # 1 100 0.006987 0.000550 0.003952 0.128178 0.006042 0.022608 passed_df = df [ df [ col_force ] < f_trust_lo ] selected_df = df [( df [ col_force ] >= f_trust_lo ) & ( df [ col_force ] < f_trust_hi )] rejected_df = df [ df [ col_force ] >= f_trust_hi ] logger . info ( 'result: total: %d , passed: %d , selected: %d , rejected: %d ' , len ( df ), len ( passed_df ), len ( selected_df ), len ( rejected_df )) candidate . attrs [ 'all' ] = df . step . tolist () candidate . attrs [ 'passed' ] = passed_df . step . tolist () candidate . attrs [ 'selected' ] = selected_df . step . tolist () candidate . attrs [ 'rejected' ] = rejected_df . step . tolist () total_count += len ( df ) passed_count += len ( passed_df ) return ThresholdSelectorOutput ( model_devi_data = input . model_devi_data , passing_rate = passed_count / total_count , ) Variables logger Functions threshold_selector def threshold_selector ( input : ai2_kit . domain . selector . ThresholdSelectorInput , ctx : ai2_kit . domain . selector . ThresholdSelectorContext ) View Source async def threshold_selector ( input : ThresholdSelectorInput , ctx : ThresholdSelectorContext ) : executor = ctx . resource_manager . default_executor f_trust_lo = input . config . f_trust_lo f_trust_hi = input . config . f_trust_hi col_force = 'avg_devi_f' logger . info ( 'criteria: %f <= %s < %f ' , f_trust_lo , col_force , f_trust_hi ) total_count = 0 passed_count = 0 # TODO : support output of different software for candidate in input . model_devi_data : if LammpsOutputHelper . is_match ( candidate ) : model_devi_out_file = LammpsOutputHelper ( candidate ). get_model_devi_file ( input . model_devi_out_filename ). url else : raise ValueError ( 'unknown model_devi_data types' ) logger . info ( 'start to analysis file: %s' , model_devi_out_file ) text = executor . load_text ( model_devi_out_file ) df = pd . read_csv ( StringIO ( text . lstrip ( '#' )), delim_whitespace = True ) # layout : # step max_devi_v min_devi_v avg_devi_v max_devi_f min_devi_f avg_devi_f # 0 0 0.006793 0.000672 0.003490 0.143317 0.005612 0.026106 # 1 100 0.006987 0.000550 0.003952 0.128178 0.006042 0.022608 passed_df = df [ df[col_force ] < f_trust_lo ] selected_df = df [ (df[col_force ] >= f_trust_lo ) & ( df [ col_force ] < f_trust_hi ) ] rejected_df = df [ df[col_force ] >= f_trust_hi ] logger . info ( 'result: total: %d, passed: %d, selected: %d, rejected: %d' , len ( df ), len ( passed_df ), len ( selected_df ), len ( rejected_df )) candidate . attrs [ 'all' ] = df . step . tolist () candidate . attrs [ 'passed' ] = passed_df . step . tolist () candidate . attrs [ 'selected' ] = selected_df . step . tolist () candidate . attrs [ 'rejected' ] = rejected_df . step . tolist () total_count += len ( df ) passed_count += len ( passed_df ) return ThresholdSelectorOutput ( model_devi_data = input . model_devi_data , passing_rate = passed_count / total_count , ) Classes ThresholdSelectorContext class ThresholdSelectorContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager ) ThresholdSelectorContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager) View Source class ThresholdSelectorContext ( BaseCllContext ): ... Ancestors (in MRO) ai2_kit.domain.cll.BaseCllContext ThresholdSelectorInput class ThresholdSelectorInput ( config : ai2_kit . domain . selector . ThresholdSelectorInputConfig , model_devi_data : List [ ai2_kit . core . artifact . Artifact ], model_devi_out_filename : str ) ThresholdSelectorInput(config: ai2_kit.domain.selector.ThresholdSelectorInputConfig, model_devi_data: List[ai2_kit.core.artifact.Artifact], model_devi_out_filename: str) View Source class ThresholdSelectorInput : config : ThresholdSelectorInputConfig model_devi_data : List [ Artifact ] model_devi_out_filename : str def set_model_devi_dataset ( self , data : List [ Artifact ] ) : self . model_devi_data = data Methods set_model_devi_dataset def set_model_devi_dataset ( self , data : List [ ai2_kit . core . artifact . Artifact ] ) View Source def set_model_devi_dataset ( self , data : List [ Artifact ] ) : self . model_devi_data = data ThresholdSelectorInputConfig class ThresholdSelectorInputConfig ( __pydantic_self__ , ** data : Any ) View Source class ThresholdSelectorInputConfig ( BaseModel ): f_trust_lo: float f_trust_hi: float Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . ThresholdSelectorOutput class ThresholdSelectorOutput ( model_devi_data : List [ ai2_kit . core . artifact . Artifact ], passing_rate : float ) ThresholdSelectorOutput(model_devi_data: List[ai2_kit.core.artifact.Artifact], passing_rate: float) View Source class ThresholdSelectorOutput ( ICllSelectorOutput ) : model_devi_data : List [ Artifact ] passing_rate : float def get_model_devi_dataset ( self ) : return self . model_devi_data def get_passing_rate ( self ) -> float : return self . passing_rate Ancestors (in MRO) ai2_kit.domain.cll.ICllSelectorOutput abc.ABC Methods get_model_devi_dataset def get_model_devi_dataset ( self ) View Source def get_model_devi_dataset ( self ) : return self . model_devi_data get_passing_rate def get_passing_rate ( self ) -> float View Source def get_passing_rate ( self ) -> float : return self . passing_rate","title":"Selector"},{"location":"reference/ai2_kit/domain/selector/#module-ai2_kitdomainselector","text":"View Source from ai2_kit.core.artifact import Artifact from ai2_kit.core.log import get_logger from typing import List from io import StringIO from pydantic import BaseModel from dataclasses import dataclass import pandas as pd from .data_helper import LammpsOutputHelper from .cll import ICllSelectorOutput , BaseCllContext logger = get_logger ( __name__ ) class ThresholdSelectorInputConfig ( BaseModel ): f_trust_lo : float f_trust_hi : float @dataclass class ThresholdSelectorContext ( BaseCllContext ): ... @dataclass class ThresholdSelectorOutput ( ICllSelectorOutput ): model_devi_data : List [ Artifact ] passing_rate : float def get_model_devi_dataset ( self ): return self . model_devi_data def get_passing_rate ( self ) -> float : return self . passing_rate @dataclass class ThresholdSelectorInput : config : ThresholdSelectorInputConfig model_devi_data : List [ Artifact ] model_devi_out_filename : str def set_model_devi_dataset ( self , data : List [ Artifact ]): self . model_devi_data = data async def threshold_selector ( input : ThresholdSelectorInput , ctx : ThresholdSelectorContext ): executor = ctx . resource_manager . default_executor f_trust_lo = input . config . f_trust_lo f_trust_hi = input . config . f_trust_hi col_force = 'avg_devi_f' logger . info ( 'criteria: %f <= %s < %f ' , f_trust_lo , col_force , f_trust_hi ) total_count = 0 passed_count = 0 # TODO: support output of different software for candidate in input . model_devi_data : if LammpsOutputHelper . is_match ( candidate ): model_devi_out_file = LammpsOutputHelper ( candidate ) . get_model_devi_file ( input . model_devi_out_filename ) . url else : raise ValueError ( 'unknown model_devi_data types' ) logger . info ( 'start to analysis file: %s ' , model_devi_out_file ) text = executor . load_text ( model_devi_out_file ) df = pd . read_csv ( StringIO ( text . lstrip ( '#' )), delim_whitespace = True ) # layout: # step max_devi_v min_devi_v avg_devi_v max_devi_f min_devi_f avg_devi_f # 0 0 0.006793 0.000672 0.003490 0.143317 0.005612 0.026106 # 1 100 0.006987 0.000550 0.003952 0.128178 0.006042 0.022608 passed_df = df [ df [ col_force ] < f_trust_lo ] selected_df = df [( df [ col_force ] >= f_trust_lo ) & ( df [ col_force ] < f_trust_hi )] rejected_df = df [ df [ col_force ] >= f_trust_hi ] logger . info ( 'result: total: %d , passed: %d , selected: %d , rejected: %d ' , len ( df ), len ( passed_df ), len ( selected_df ), len ( rejected_df )) candidate . attrs [ 'all' ] = df . step . tolist () candidate . attrs [ 'passed' ] = passed_df . step . tolist () candidate . attrs [ 'selected' ] = selected_df . step . tolist () candidate . attrs [ 'rejected' ] = rejected_df . step . tolist () total_count += len ( df ) passed_count += len ( passed_df ) return ThresholdSelectorOutput ( model_devi_data = input . model_devi_data , passing_rate = passed_count / total_count , )","title":"Module ai2_kit.domain.selector"},{"location":"reference/ai2_kit/domain/selector/#variables","text":"logger","title":"Variables"},{"location":"reference/ai2_kit/domain/selector/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/domain/selector/#threshold_selector","text":"def threshold_selector ( input : ai2_kit . domain . selector . ThresholdSelectorInput , ctx : ai2_kit . domain . selector . ThresholdSelectorContext ) View Source async def threshold_selector ( input : ThresholdSelectorInput , ctx : ThresholdSelectorContext ) : executor = ctx . resource_manager . default_executor f_trust_lo = input . config . f_trust_lo f_trust_hi = input . config . f_trust_hi col_force = 'avg_devi_f' logger . info ( 'criteria: %f <= %s < %f ' , f_trust_lo , col_force , f_trust_hi ) total_count = 0 passed_count = 0 # TODO : support output of different software for candidate in input . model_devi_data : if LammpsOutputHelper . is_match ( candidate ) : model_devi_out_file = LammpsOutputHelper ( candidate ). get_model_devi_file ( input . model_devi_out_filename ). url else : raise ValueError ( 'unknown model_devi_data types' ) logger . info ( 'start to analysis file: %s' , model_devi_out_file ) text = executor . load_text ( model_devi_out_file ) df = pd . read_csv ( StringIO ( text . lstrip ( '#' )), delim_whitespace = True ) # layout : # step max_devi_v min_devi_v avg_devi_v max_devi_f min_devi_f avg_devi_f # 0 0 0.006793 0.000672 0.003490 0.143317 0.005612 0.026106 # 1 100 0.006987 0.000550 0.003952 0.128178 0.006042 0.022608 passed_df = df [ df[col_force ] < f_trust_lo ] selected_df = df [ (df[col_force ] >= f_trust_lo ) & ( df [ col_force ] < f_trust_hi ) ] rejected_df = df [ df[col_force ] >= f_trust_hi ] logger . info ( 'result: total: %d, passed: %d, selected: %d, rejected: %d' , len ( df ), len ( passed_df ), len ( selected_df ), len ( rejected_df )) candidate . attrs [ 'all' ] = df . step . tolist () candidate . attrs [ 'passed' ] = passed_df . step . tolist () candidate . attrs [ 'selected' ] = selected_df . step . tolist () candidate . attrs [ 'rejected' ] = rejected_df . step . tolist () total_count += len ( df ) passed_count += len ( passed_df ) return ThresholdSelectorOutput ( model_devi_data = input . model_devi_data , passing_rate = passed_count / total_count , )","title":"threshold_selector"},{"location":"reference/ai2_kit/domain/selector/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/selector/#thresholdselectorcontext","text":"class ThresholdSelectorContext ( path_prefix : str , resource_manager : ai2_kit . core . resource_manager . ResourceManager ) ThresholdSelectorContext(path_prefix: str, resource_manager: ai2_kit.core.resource_manager.ResourceManager) View Source class ThresholdSelectorContext ( BaseCllContext ): ...","title":"ThresholdSelectorContext"},{"location":"reference/ai2_kit/domain/selector/#ancestors-in-mro","text":"ai2_kit.domain.cll.BaseCllContext","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/selector/#thresholdselectorinput","text":"class ThresholdSelectorInput ( config : ai2_kit . domain . selector . ThresholdSelectorInputConfig , model_devi_data : List [ ai2_kit . core . artifact . Artifact ], model_devi_out_filename : str ) ThresholdSelectorInput(config: ai2_kit.domain.selector.ThresholdSelectorInputConfig, model_devi_data: List[ai2_kit.core.artifact.Artifact], model_devi_out_filename: str) View Source class ThresholdSelectorInput : config : ThresholdSelectorInputConfig model_devi_data : List [ Artifact ] model_devi_out_filename : str def set_model_devi_dataset ( self , data : List [ Artifact ] ) : self . model_devi_data = data","title":"ThresholdSelectorInput"},{"location":"reference/ai2_kit/domain/selector/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/selector/#set_model_devi_dataset","text":"def set_model_devi_dataset ( self , data : List [ ai2_kit . core . artifact . Artifact ] ) View Source def set_model_devi_dataset ( self , data : List [ Artifact ] ) : self . model_devi_data = data","title":"set_model_devi_dataset"},{"location":"reference/ai2_kit/domain/selector/#thresholdselectorinputconfig","text":"class ThresholdSelectorInputConfig ( __pydantic_self__ , ** data : Any ) View Source class ThresholdSelectorInputConfig ( BaseModel ): f_trust_lo: float f_trust_hi: float","title":"ThresholdSelectorInputConfig"},{"location":"reference/ai2_kit/domain/selector/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/selector/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/selector/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/selector/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/selector/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/selector/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/selector/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/selector/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/selector/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/selector/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/selector/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/selector/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/selector/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/selector/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/selector/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/selector/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/domain/selector/#thresholdselectoroutput","text":"class ThresholdSelectorOutput ( model_devi_data : List [ ai2_kit . core . artifact . Artifact ], passing_rate : float ) ThresholdSelectorOutput(model_devi_data: List[ai2_kit.core.artifact.Artifact], passing_rate: float) View Source class ThresholdSelectorOutput ( ICllSelectorOutput ) : model_devi_data : List [ Artifact ] passing_rate : float def get_model_devi_dataset ( self ) : return self . model_devi_data def get_passing_rate ( self ) -> float : return self . passing_rate","title":"ThresholdSelectorOutput"},{"location":"reference/ai2_kit/domain/selector/#ancestors-in-mro_2","text":"ai2_kit.domain.cll.ICllSelectorOutput abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/selector/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/selector/#get_model_devi_dataset","text":"def get_model_devi_dataset ( self ) View Source def get_model_devi_dataset ( self ) : return self . model_devi_data","title":"get_model_devi_dataset"},{"location":"reference/ai2_kit/domain/selector/#get_passing_rate","text":"def get_passing_rate ( self ) -> float View Source def get_passing_rate ( self ) -> float : return self . passing_rate","title":"get_passing_rate"},{"location":"reference/ai2_kit/domain/updater/","text":"Module ai2_kit.domain.updater View Source from pydantic import BaseModel from typing import Optional , List , Any class WalkthroughUpdaterInputConfig ( BaseModel ): passing_rate_threshold : float = - 1.0 table : List [ Any ] Classes WalkthroughUpdaterInputConfig class WalkthroughUpdaterInputConfig ( __pydantic_self__ , ** data : Any ) View Source class WalkthroughUpdaterInputConfig ( BaseModel ) : passing_rate_threshold : float = - 1.0 table : List [ Any ] Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Updater"},{"location":"reference/ai2_kit/domain/updater/#module-ai2_kitdomainupdater","text":"View Source from pydantic import BaseModel from typing import Optional , List , Any class WalkthroughUpdaterInputConfig ( BaseModel ): passing_rate_threshold : float = - 1.0 table : List [ Any ]","title":"Module ai2_kit.domain.updater"},{"location":"reference/ai2_kit/domain/updater/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/domain/updater/#walkthroughupdaterinputconfig","text":"class WalkthroughUpdaterInputConfig ( __pydantic_self__ , ** data : Any ) View Source class WalkthroughUpdaterInputConfig ( BaseModel ) : passing_rate_threshold : float = - 1.0 table : List [ Any ]","title":"WalkthroughUpdaterInputConfig"},{"location":"reference/ai2_kit/domain/updater/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/domain/updater/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/domain/updater/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/domain/updater/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/domain/updater/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/domain/updater/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/domain/updater/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/domain/updater/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/domain/updater/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/domain/updater/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/domain/updater/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/domain/updater/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/domain/updater/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/domain/updater/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/domain/updater/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/domain/updater/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/tool/","text":"Module ai2_kit.tool Sub-modules ai2_kit.tool.ase","title":"Index"},{"location":"reference/ai2_kit/tool/#module-ai2_kittool","text":"","title":"Module ai2_kit.tool"},{"location":"reference/ai2_kit/tool/#sub-modules","text":"ai2_kit.tool.ase","title":"Sub-modules"},{"location":"reference/ai2_kit/tool/ase/","text":"Module ai2_kit.tool.ase View Source import ase.io from ase import Atoms from typing import List class AseHelper : def __init__ ( self ): self . _atoms_list : List [ Atoms ] = [] def read ( self , filename : str , ** kwargs ): kwargs . setdefault ( 'index' , ':' ) data = ase . io . read ( filename , ** kwargs ) if not isinstance ( data , list ): data = [ data ] self . _atoms_list = data return self def write ( self , filename : str , ** kwargs ): ase . io . write ( filename , self . _atoms_list , ** kwargs ) def write_each_frame ( self , filename : str , ** kwargs ): for i , atoms in enumerate ( self . _atoms_list ): ase . io . write ( filename . format ( i = i ), atoms , ** kwargs ) Classes AseHelper class AseHelper ( ) View Source class AseHelper : def __init__ ( self ) : self . _atoms_list : List [ Atoms ] = [] def read ( self , filename : str , ** kwargs ) : kwargs . setdefault ( 'index' , ':' ) data = ase . io . read ( filename , ** kwargs ) if not isinstance ( data , list ) : data = [ data ] self . _atoms_list = data return self def write ( self , filename : str , ** kwargs ) : ase . io . write ( filename , self . _atoms_list , ** kwargs ) def write_each_frame ( self , filename : str , ** kwargs ) : for i , atoms in enumerate ( self . _atoms_list ) : ase . io . write ( filename . format ( i = i ), atoms , ** kwargs ) Methods read def read ( self , filename : str , ** kwargs ) View Source def read ( self , filename : str , ** kwargs ) : kwargs . setdefault ( 'index' , ':' ) data = ase . io . read ( filename , ** kwargs ) if not isinstance ( data , list ) : data = [ data ] self . _atoms_list = data return self write def write ( self , filename : str , ** kwargs ) View Source def write(self, filename: str, **kwargs): ase.io.write(filename, self._atoms_list, **kwargs) write_each_frame def write_each_frame ( self , filename : str , ** kwargs ) View Source def write_each_frame ( self , filename : str , ** kwargs ): for i , atoms in enumerate ( self . _atoms_list ): ase . io . write ( filename . format ( i = i ), atoms , ** kwargs )","title":"Ase"},{"location":"reference/ai2_kit/tool/ase/#module-ai2_kittoolase","text":"View Source import ase.io from ase import Atoms from typing import List class AseHelper : def __init__ ( self ): self . _atoms_list : List [ Atoms ] = [] def read ( self , filename : str , ** kwargs ): kwargs . setdefault ( 'index' , ':' ) data = ase . io . read ( filename , ** kwargs ) if not isinstance ( data , list ): data = [ data ] self . _atoms_list = data return self def write ( self , filename : str , ** kwargs ): ase . io . write ( filename , self . _atoms_list , ** kwargs ) def write_each_frame ( self , filename : str , ** kwargs ): for i , atoms in enumerate ( self . _atoms_list ): ase . io . write ( filename . format ( i = i ), atoms , ** kwargs )","title":"Module ai2_kit.tool.ase"},{"location":"reference/ai2_kit/tool/ase/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/tool/ase/#asehelper","text":"class AseHelper ( ) View Source class AseHelper : def __init__ ( self ) : self . _atoms_list : List [ Atoms ] = [] def read ( self , filename : str , ** kwargs ) : kwargs . setdefault ( 'index' , ':' ) data = ase . io . read ( filename , ** kwargs ) if not isinstance ( data , list ) : data = [ data ] self . _atoms_list = data return self def write ( self , filename : str , ** kwargs ) : ase . io . write ( filename , self . _atoms_list , ** kwargs ) def write_each_frame ( self , filename : str , ** kwargs ) : for i , atoms in enumerate ( self . _atoms_list ) : ase . io . write ( filename . format ( i = i ), atoms , ** kwargs )","title":"AseHelper"},{"location":"reference/ai2_kit/tool/ase/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/tool/ase/#read","text":"def read ( self , filename : str , ** kwargs ) View Source def read ( self , filename : str , ** kwargs ) : kwargs . setdefault ( 'index' , ':' ) data = ase . io . read ( filename , ** kwargs ) if not isinstance ( data , list ) : data = [ data ] self . _atoms_list = data return self","title":"read"},{"location":"reference/ai2_kit/tool/ase/#write","text":"def write ( self , filename : str , ** kwargs ) View Source def write(self, filename: str, **kwargs): ase.io.write(filename, self._atoms_list, **kwargs)","title":"write"},{"location":"reference/ai2_kit/tool/ase/#write_each_frame","text":"def write_each_frame ( self , filename : str , ** kwargs ) View Source def write_each_frame ( self , filename : str , ** kwargs ): for i , atoms in enumerate ( self . _atoms_list ): ase . io . write ( filename . format ( i = i ), atoms , ** kwargs )","title":"write_each_frame"},{"location":"reference/ai2_kit/workflow/","text":"Module ai2_kit.workflow Sub-modules ai2_kit.workflow.cll_mlp ai2_kit.workflow.fep_mlp","title":"Index"},{"location":"reference/ai2_kit/workflow/#module-ai2_kitworkflow","text":"","title":"Module ai2_kit.workflow"},{"location":"reference/ai2_kit/workflow/#sub-modules","text":"ai2_kit.workflow.cll_mlp ai2_kit.workflow.fep_mlp","title":"Sub-modules"},{"location":"reference/ai2_kit/workflow/cll_mlp/","text":"Module ai2_kit.workflow.cll_mlp View Source from ai2_kit.core.executor import BaseExecutorConfig from ai2_kit.core.artifact import ArtifactMap from ai2_kit.core.log import get_logger from ai2_kit.core.util import load_yaml_files , merge_dict from ai2_kit.core.resource_manager import ResourceManager from ai2_kit.core.checkpoint import set_checkpoint_file , apply_checkpoint from ai2_kit.domain import ( deepmd , lammps , selector , cp2k , constant as const , updater , cll , ) from pydantic import BaseModel from typing import Dict , List , Optional , Any from fire import Fire import asyncio import itertools import copy import os logger = get_logger ( __name__ ) class CllWorkflowExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): class Train ( BaseModel ): deepmd : deepmd . GenericDeepmdContextConfig class Explore ( BaseModel ): lammps : lammps . GenericLammpsContextConfig class Label ( BaseModel ): cp2k : cp2k . GenericCp2kContextConfig train : Train explore : Explore label : Label context : Context class WorkflowConfig ( BaseModel ): class General ( BaseModel ): type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Label ( BaseModel ): cp2k : cp2k . GenericCp2kInputConfig class Train ( BaseModel ): deepmd : deepmd . GenericDeepmdInputConfig class Explore ( BaseModel ): lammps : lammps . GenericLammpsInputConfig class Select ( BaseModel ): by_threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ): walkthrough : updater . WalkthroughUpdaterInputConfig general : General train : Train explore : Explore select : Select label : Label update : Update class CllWorkflowConfig ( BaseModel ): executors : Dict [ str , CllWorkflowExecutorConfig ] artifacts : ArtifactMap workflow : Any # Keep it raw here, it should be parsed later in iteration def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint_file : Optional [ str ] = None ): \"\"\" Run Closed-Loop Learning (CLL) workflow to train Machine Learning Potential (MLP) models. \"\"\" if checkpoint_file is not None : set_checkpoint_file ( checkpoint_file ) config_data = load_yaml_files ( * config_files ) config = CllWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor { executor } is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix )) async def cll_mlp_training_workflow ( config : CllWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ): context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step label_output : Optional [ cll . ICllLabelOutput ] = None selector_output : Optional [ cll . ICllSelectorOutput ] = None train_output : Optional [ cll . ICllTrainOutput ] = None explore_output : Optional [ cll . ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ): # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration { i } exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters- { i : 03d } ' ) # prefix of checkpoint cp_prefix = f 'iters- { i : 03d } ' # label if workflow_config . label . cp2k : cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . label . cp2k , type_map = type_map , system_files = [] if selector_output is None else selector_output . get_model_devi_dataset (), initiated = i > 0 , ) cpk2_context = cp2k . GenericCp2kContext ( config = context_config . label . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'label-cp2k' ), resource_manager = resource_manager , ) label_output = await apply_checkpoint ( f ' { cp_prefix } /label-cp2k' )( cp2k . generic_cp2k )( cp2k_input , cpk2_context ) else : raise ValueError ( 'No label method is specified' ) # train if workflow_config . train . deepmd : deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . train . deepmd , type_map = type_map , old_dataset = [] if train_output is None else train_output . get_training_dataset (), new_dataset = label_output . get_labeled_system_dataset (), initiated = i > 0 , ) deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'train-deepmd' ), config = context_config . train . deepmd , resource_manager = resource_manager , ) train_output = await apply_checkpoint ( f ' { cp_prefix } /train-deepmd' )( deepmd . generic_deepmd )( deepmd_input , deepmd_context ) else : raise ValueError ( 'No train method is specified' ) # explore if workflow_config . explore . lammps : md_options = lammps . GenericLammpsInput . MdOptions ( models = train_output . get_mlp_models (), ) lammps_input = lammps . GenericLammpsInput ( config = workflow_config . explore . lammps , type_map = type_map , mass_map = mass_map , md_options = md_options , ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . explore . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f ' { cp_prefix } /explore-lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) else : raise ValueError ( 'No explore method is specified' ) # select if workflow_config . select . by_threshold : selector_input = selector . ThresholdSelectorInput ( config = workflow_config . select . by_threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_OUT , ) selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'selector-threshold' ), resource_manager = resource_manager , ) selector_output = await apply_checkpoint ( f ' { cp_prefix } /selector-threshold' )( selector . threshold_selector )( selector_input , selector_context ) else : raise ValueError ( 'No select method is specified' ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ): continue # move cursor to next row if passing rate pass threshold if selector_output . get_passing_rate () > update_config . passing_rate_threshold : raw_workflow_config = merge_dict ( copy . deepcopy ( config . workflow ), update_config . table [ update_cursor ]) update_cursor += 1 if __name__ == '__main__' : # use python-fire to parse command line arguments Fire ( run_workflow ) Variables logger Functions cll_mlp_training_workflow def cll_mlp_training_workflow ( config : ai2_kit . workflow . cll_mlp . CllWorkflowConfig , resource_manager : ai2_kit . core . resource_manager . ResourceManager , executor : str , path_prefix : str ) View Source async def cll_mlp_training_workflow ( config : CllWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ) : context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step label_output : Optional [ cll.ICllLabelOutput ] = None selector_output : Optional [ cll.ICllSelectorOutput ] = None train_output : Optional [ cll.ICllTrainOutput ] = None explore_output : Optional [ cll.ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ) : # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration {i} exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters-{i:03d}' ) # prefix of checkpoint cp_prefix = f 'iters-{i:03d}' # label if workflow_config . label . cp2k : cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . label . cp2k , type_map = type_map , system_files = [] if selector_output is None else selector_output . get_model_devi_dataset (), initiated = i > 0 , ) cpk2_context = cp2k . GenericCp2kContext ( config = context_config . label . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'label-cp2k' ), resource_manager = resource_manager , ) label_output = await apply_checkpoint ( f '{cp_prefix}/label-cp2k' )( cp2k . generic_cp2k )( cp2k_input , cpk2_context ) else : raise ValueError ( 'No label method is specified' ) # train if workflow_config . train . deepmd : deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . train . deepmd , type_map = type_map , old_dataset = [] if train_output is None else train_output . get_training_dataset (), new_dataset = label_output . get_labeled_system_dataset (), initiated = i > 0 , ) deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'train-deepmd' ), config = context_config . train . deepmd , resource_manager = resource_manager , ) train_output = await apply_checkpoint ( f '{cp_prefix}/train-deepmd' )( deepmd . generic_deepmd )( deepmd_input , deepmd_context ) else : raise ValueError ( 'No train method is specified' ) # explore if workflow_config . explore . lammps : md_options = lammps . GenericLammpsInput . MdOptions ( models = train_output . get_mlp_models (), ) lammps_input = lammps . GenericLammpsInput ( config = workflow_config . explore . lammps , type_map = type_map , mass_map = mass_map , md_options = md_options , ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . explore . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f '{cp_prefix}/explore-lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) else : raise ValueError ( 'No explore method is specified' ) # select if workflow_config . select . by_threshold : selector_input = selector . ThresholdSelectorInput ( config = workflow_config . select . by_threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_OUT , ) selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'selector-threshold' ), resource_manager = resource_manager , ) selector_output = await apply_checkpoint ( f '{cp_prefix}/selector-threshold' )( selector . threshold_selector )( selector_input , selector_context ) else : raise ValueError ( 'No select method is specified' ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ) : continue # move cursor to next row if passing rate pass threshold if selector_output . get_passing_rate () > update_config . passing_rate_threshold : raw_workflow_config = merge_dict ( copy . deepcopy ( config . workflow ), update_config . table [ update_cursor ] ) update_cursor += 1 run_workflow def run_workflow ( * config_files , executor : Union [ str , NoneType ] = None , path_prefix : Union [ str , NoneType ] = None , checkpoint_file : Union [ str , NoneType ] = None ) Run Closed-Loop Learning (CLL) workflow to train Machine Learning Potential (MLP) models. View Source def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint_file : Optional [ str ] = None ) : \"\"\" Run Closed-Loop Learning (CLL) workflow to train Machine Learning Potential (MLP) models. \"\"\" if checkpoint_file is not None : set_checkpoint_file ( checkpoint_file ) config_data = load_yaml_files ( * config_files ) config = CllWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor {executor} is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix )) Classes CllWorkflowConfig class CllWorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class CllWorkflowConfig ( BaseModel ): executors: Dict [ str , CllWorkflowExecutorConfig ] artifacts: ArtifactMap workflow: Any # Keep it raw here, it should be parsed later in iteration Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . CllWorkflowExecutorConfig class CllWorkflowExecutorConfig ( __pydantic_self__ , ** data : Any ) View Source class CllWorkflowExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): class Train ( BaseModel ): deepmd: deepmd . GenericDeepmdContextConfig class Explore ( BaseModel ): lammps: lammps . GenericLammpsContextConfig class Label ( BaseModel ): cp2k: cp2k . GenericCp2kContextConfig train: Train explore: Explore label: Label context: Context Ancestors (in MRO) ai2_kit.core.executor.BaseExecutorConfig pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Context Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . WorkflowConfig class WorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class WorkflowConfig ( BaseModel ) : class General ( BaseModel ) : type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Label ( BaseModel ) : cp2k : cp2k . GenericCp2kInputConfig class Train ( BaseModel ) : deepmd : deepmd . GenericDeepmdInputConfig class Explore ( BaseModel ) : lammps : lammps . GenericLammpsInputConfig class Select ( BaseModel ) : by_threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ) : walkthrough : updater . WalkthroughUpdaterInputConfig general : General train : Train explore : Explore select : Select label : Label update : Update Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Explore General Label Select Train Update Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Cll Mlp"},{"location":"reference/ai2_kit/workflow/cll_mlp/#module-ai2_kitworkflowcll_mlp","text":"View Source from ai2_kit.core.executor import BaseExecutorConfig from ai2_kit.core.artifact import ArtifactMap from ai2_kit.core.log import get_logger from ai2_kit.core.util import load_yaml_files , merge_dict from ai2_kit.core.resource_manager import ResourceManager from ai2_kit.core.checkpoint import set_checkpoint_file , apply_checkpoint from ai2_kit.domain import ( deepmd , lammps , selector , cp2k , constant as const , updater , cll , ) from pydantic import BaseModel from typing import Dict , List , Optional , Any from fire import Fire import asyncio import itertools import copy import os logger = get_logger ( __name__ ) class CllWorkflowExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): class Train ( BaseModel ): deepmd : deepmd . GenericDeepmdContextConfig class Explore ( BaseModel ): lammps : lammps . GenericLammpsContextConfig class Label ( BaseModel ): cp2k : cp2k . GenericCp2kContextConfig train : Train explore : Explore label : Label context : Context class WorkflowConfig ( BaseModel ): class General ( BaseModel ): type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Label ( BaseModel ): cp2k : cp2k . GenericCp2kInputConfig class Train ( BaseModel ): deepmd : deepmd . GenericDeepmdInputConfig class Explore ( BaseModel ): lammps : lammps . GenericLammpsInputConfig class Select ( BaseModel ): by_threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ): walkthrough : updater . WalkthroughUpdaterInputConfig general : General train : Train explore : Explore select : Select label : Label update : Update class CllWorkflowConfig ( BaseModel ): executors : Dict [ str , CllWorkflowExecutorConfig ] artifacts : ArtifactMap workflow : Any # Keep it raw here, it should be parsed later in iteration def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint_file : Optional [ str ] = None ): \"\"\" Run Closed-Loop Learning (CLL) workflow to train Machine Learning Potential (MLP) models. \"\"\" if checkpoint_file is not None : set_checkpoint_file ( checkpoint_file ) config_data = load_yaml_files ( * config_files ) config = CllWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor { executor } is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix )) async def cll_mlp_training_workflow ( config : CllWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ): context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step label_output : Optional [ cll . ICllLabelOutput ] = None selector_output : Optional [ cll . ICllSelectorOutput ] = None train_output : Optional [ cll . ICllTrainOutput ] = None explore_output : Optional [ cll . ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ): # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration { i } exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters- { i : 03d } ' ) # prefix of checkpoint cp_prefix = f 'iters- { i : 03d } ' # label if workflow_config . label . cp2k : cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . label . cp2k , type_map = type_map , system_files = [] if selector_output is None else selector_output . get_model_devi_dataset (), initiated = i > 0 , ) cpk2_context = cp2k . GenericCp2kContext ( config = context_config . label . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'label-cp2k' ), resource_manager = resource_manager , ) label_output = await apply_checkpoint ( f ' { cp_prefix } /label-cp2k' )( cp2k . generic_cp2k )( cp2k_input , cpk2_context ) else : raise ValueError ( 'No label method is specified' ) # train if workflow_config . train . deepmd : deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . train . deepmd , type_map = type_map , old_dataset = [] if train_output is None else train_output . get_training_dataset (), new_dataset = label_output . get_labeled_system_dataset (), initiated = i > 0 , ) deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'train-deepmd' ), config = context_config . train . deepmd , resource_manager = resource_manager , ) train_output = await apply_checkpoint ( f ' { cp_prefix } /train-deepmd' )( deepmd . generic_deepmd )( deepmd_input , deepmd_context ) else : raise ValueError ( 'No train method is specified' ) # explore if workflow_config . explore . lammps : md_options = lammps . GenericLammpsInput . MdOptions ( models = train_output . get_mlp_models (), ) lammps_input = lammps . GenericLammpsInput ( config = workflow_config . explore . lammps , type_map = type_map , mass_map = mass_map , md_options = md_options , ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . explore . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f ' { cp_prefix } /explore-lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) else : raise ValueError ( 'No explore method is specified' ) # select if workflow_config . select . by_threshold : selector_input = selector . ThresholdSelectorInput ( config = workflow_config . select . by_threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_OUT , ) selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'selector-threshold' ), resource_manager = resource_manager , ) selector_output = await apply_checkpoint ( f ' { cp_prefix } /selector-threshold' )( selector . threshold_selector )( selector_input , selector_context ) else : raise ValueError ( 'No select method is specified' ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ): continue # move cursor to next row if passing rate pass threshold if selector_output . get_passing_rate () > update_config . passing_rate_threshold : raw_workflow_config = merge_dict ( copy . deepcopy ( config . workflow ), update_config . table [ update_cursor ]) update_cursor += 1 if __name__ == '__main__' : # use python-fire to parse command line arguments Fire ( run_workflow )","title":"Module ai2_kit.workflow.cll_mlp"},{"location":"reference/ai2_kit/workflow/cll_mlp/#variables","text":"logger","title":"Variables"},{"location":"reference/ai2_kit/workflow/cll_mlp/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/workflow/cll_mlp/#cll_mlp_training_workflow","text":"def cll_mlp_training_workflow ( config : ai2_kit . workflow . cll_mlp . CllWorkflowConfig , resource_manager : ai2_kit . core . resource_manager . ResourceManager , executor : str , path_prefix : str ) View Source async def cll_mlp_training_workflow ( config : CllWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ) : context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step label_output : Optional [ cll.ICllLabelOutput ] = None selector_output : Optional [ cll.ICllSelectorOutput ] = None train_output : Optional [ cll.ICllTrainOutput ] = None explore_output : Optional [ cll.ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ) : # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration {i} exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters-{i:03d}' ) # prefix of checkpoint cp_prefix = f 'iters-{i:03d}' # label if workflow_config . label . cp2k : cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . label . cp2k , type_map = type_map , system_files = [] if selector_output is None else selector_output . get_model_devi_dataset (), initiated = i > 0 , ) cpk2_context = cp2k . GenericCp2kContext ( config = context_config . label . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'label-cp2k' ), resource_manager = resource_manager , ) label_output = await apply_checkpoint ( f '{cp_prefix}/label-cp2k' )( cp2k . generic_cp2k )( cp2k_input , cpk2_context ) else : raise ValueError ( 'No label method is specified' ) # train if workflow_config . train . deepmd : deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . train . deepmd , type_map = type_map , old_dataset = [] if train_output is None else train_output . get_training_dataset (), new_dataset = label_output . get_labeled_system_dataset (), initiated = i > 0 , ) deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'train-deepmd' ), config = context_config . train . deepmd , resource_manager = resource_manager , ) train_output = await apply_checkpoint ( f '{cp_prefix}/train-deepmd' )( deepmd . generic_deepmd )( deepmd_input , deepmd_context ) else : raise ValueError ( 'No train method is specified' ) # explore if workflow_config . explore . lammps : md_options = lammps . GenericLammpsInput . MdOptions ( models = train_output . get_mlp_models (), ) lammps_input = lammps . GenericLammpsInput ( config = workflow_config . explore . lammps , type_map = type_map , mass_map = mass_map , md_options = md_options , ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . explore . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f '{cp_prefix}/explore-lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) else : raise ValueError ( 'No explore method is specified' ) # select if workflow_config . select . by_threshold : selector_input = selector . ThresholdSelectorInput ( config = workflow_config . select . by_threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_OUT , ) selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'selector-threshold' ), resource_manager = resource_manager , ) selector_output = await apply_checkpoint ( f '{cp_prefix}/selector-threshold' )( selector . threshold_selector )( selector_input , selector_context ) else : raise ValueError ( 'No select method is specified' ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ) : continue # move cursor to next row if passing rate pass threshold if selector_output . get_passing_rate () > update_config . passing_rate_threshold : raw_workflow_config = merge_dict ( copy . deepcopy ( config . workflow ), update_config . table [ update_cursor ] ) update_cursor += 1","title":"cll_mlp_training_workflow"},{"location":"reference/ai2_kit/workflow/cll_mlp/#run_workflow","text":"def run_workflow ( * config_files , executor : Union [ str , NoneType ] = None , path_prefix : Union [ str , NoneType ] = None , checkpoint_file : Union [ str , NoneType ] = None ) Run Closed-Loop Learning (CLL) workflow to train Machine Learning Potential (MLP) models. View Source def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint_file : Optional [ str ] = None ) : \"\"\" Run Closed-Loop Learning (CLL) workflow to train Machine Learning Potential (MLP) models. \"\"\" if checkpoint_file is not None : set_checkpoint_file ( checkpoint_file ) config_data = load_yaml_files ( * config_files ) config = CllWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor {executor} is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix ))","title":"run_workflow"},{"location":"reference/ai2_kit/workflow/cll_mlp/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/workflow/cll_mlp/#cllworkflowconfig","text":"class CllWorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class CllWorkflowConfig ( BaseModel ): executors: Dict [ str , CllWorkflowExecutorConfig ] artifacts: ArtifactMap workflow: Any # Keep it raw here, it should be parsed later in iteration","title":"CllWorkflowConfig"},{"location":"reference/ai2_kit/workflow/cll_mlp/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/workflow/cll_mlp/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/workflow/cll_mlp/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/workflow/cll_mlp/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/workflow/cll_mlp/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/workflow/cll_mlp/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/workflow/cll_mlp/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/workflow/cll_mlp/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/workflow/cll_mlp/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/workflow/cll_mlp/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/workflow/cll_mlp/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/workflow/cll_mlp/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/workflow/cll_mlp/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/workflow/cll_mlp/#cllworkflowexecutorconfig","text":"class CllWorkflowExecutorConfig ( __pydantic_self__ , ** data : Any ) View Source class CllWorkflowExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): class Train ( BaseModel ): deepmd: deepmd . GenericDeepmdContextConfig class Explore ( BaseModel ): lammps: lammps . GenericLammpsContextConfig class Label ( BaseModel ): cp2k: cp2k . GenericCp2kContextConfig train: Train explore: Explore label: Label context: Context","title":"CllWorkflowExecutorConfig"},{"location":"reference/ai2_kit/workflow/cll_mlp/#ancestors-in-mro_1","text":"ai2_kit.core.executor.BaseExecutorConfig pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/workflow/cll_mlp/#class-variables_1","text":"Config Context","title":"Class variables"},{"location":"reference/ai2_kit/workflow/cll_mlp/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/workflow/cll_mlp/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/workflow/cll_mlp/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/workflow/cll_mlp/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/workflow/cll_mlp/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/workflow/cll_mlp/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/workflow/cll_mlp/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/workflow/cll_mlp/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/workflow/cll_mlp/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/workflow/cll_mlp/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/workflow/cll_mlp/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/workflow/cll_mlp/#workflowconfig","text":"class WorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class WorkflowConfig ( BaseModel ) : class General ( BaseModel ) : type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Label ( BaseModel ) : cp2k : cp2k . GenericCp2kInputConfig class Train ( BaseModel ) : deepmd : deepmd . GenericDeepmdInputConfig class Explore ( BaseModel ) : lammps : lammps . GenericLammpsInputConfig class Select ( BaseModel ) : by_threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ) : walkthrough : updater . WalkthroughUpdaterInputConfig general : General train : Train explore : Explore select : Select label : Label update : Update","title":"WorkflowConfig"},{"location":"reference/ai2_kit/workflow/cll_mlp/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/workflow/cll_mlp/#class-variables_2","text":"Config Explore General Label Select Train Update","title":"Class variables"},{"location":"reference/ai2_kit/workflow/cll_mlp/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ai2_kit/workflow/cll_mlp/#construct_2","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/workflow/cll_mlp/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/workflow/cll_mlp/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/workflow/cll_mlp/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/workflow/cll_mlp/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/workflow/cll_mlp/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/workflow/cll_mlp/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/workflow/cll_mlp/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/workflow/cll_mlp/#copy_2","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/workflow/cll_mlp/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/workflow/cll_mlp/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/workflow/fep_mlp/","text":"Module ai2_kit.workflow.fep_mlp View Source from ai2_kit.core.executor import BaseExecutorConfig from ai2_kit.core.artifact import ArtifactMap from ai2_kit.core.log import get_logger from ai2_kit.core.util import load_yaml_files from ai2_kit.core.resource_manager import ResourceManager from ai2_kit.domain import ( deepmd , lammps , selector , cp2k , constant as const , updater , cll , ) from ai2_kit.core.checkpoint import set_checkpoint_file , apply_checkpoint from pydantic import BaseModel from typing import Dict , List , Optional , Any from fire import Fire import asyncio import copy import itertools import os logger = get_logger ( __name__ ) class FepExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): deepmd : deepmd . GenericDeepmdContextConfig lammps : lammps . GenericLammpsContextConfig cp2k : cp2k . GenericCp2kContextConfig context : Context class WorkflowConfig ( BaseModel ): class General ( BaseModel ): type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Branch ( BaseModel ): deepmd : deepmd . GenericDeepmdInputConfig cp2k : cp2k . GenericCp2kInputConfig threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ): walkthrough : updater . WalkthroughUpdaterInputConfig general : General neu : Branch red : Branch lammps : lammps . GenericLammpsInputConfig update : Update class FepWorkflowConfig ( BaseModel ): executors : Dict [ str , FepExecutorConfig ] artifacts : ArtifactMap workflow : Any def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint : Optional [ str ] = None ): \"\"\" Training ML potential for FEP \"\"\" if checkpoint is not None : set_checkpoint_file ( checkpoint ) config_data = load_yaml_files ( * config_files ) config = FepWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor { executor } is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix )) async def cll_mlp_training_workflow ( config : FepWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ): context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step neu_label_output : Optional [ cll . ICllLabelOutput ] = None red_label_output : Optional [ cll . ICllLabelOutput ] = None neu_selector_output : Optional [ cll . ICllSelectorOutput ] = None red_selector_output : Optional [ cll . ICllSelectorOutput ] = None neu_train_output : Optional [ cll . ICllTrainOutput ] = None red_train_output : Optional [ cll . ICllTrainOutput ] = None explore_output : Optional [ cll . ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ): # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration { i } exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters- { i : 03d } ' ) # prefix of checkpoint cp_prefix = f 'iters- { i : 03d } ' # label: cp2k red_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . red . cp2k , type_map = type_map , system_files = [] if red_selector_output is None else red_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) red_cpk2_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'red-label-cp2k' ), resource_manager = resource_manager , ) neu_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . neu . cp2k , type_map = type_map , system_files = [] if neu_selector_output is None else neu_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) neu_cp2k_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'neu-label-cp2k' ), resource_manager = resource_manager , ) red_label_output , neu_label_output = await asyncio . gather ( apply_checkpoint ( f ' { cp_prefix } /cp2k/red' )( cp2k . generic_cp2k )( red_cp2k_input , red_cpk2_context ), apply_checkpoint ( f ' { cp_prefix } /cp2k/neu' )( cp2k . generic_cp2k )( neu_cp2k_input , neu_cp2k_context ), ) # Train red_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . red . deepmd , type_map = type_map , old_dataset = [] if red_train_output is None else red_train_output . get_training_dataset (), new_dataset = red_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) red_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) neu_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . neu . deepmd , type_map = type_map , old_dataset = [] if neu_train_output is None else neu_train_output . get_training_dataset (), new_dataset = neu_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) neu_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) red_train_output , neu_train_output = await asyncio . gather ( apply_checkpoint ( f ' { cp_prefix } /deepmd/red' )( deepmd . generic_deepmd )( red_deepmd_input , red_deepmd_context ), apply_checkpoint ( f ' { cp_prefix } /deepmd/neu' )( deepmd . generic_deepmd )( neu_deepmd_input , neu_deepmd_context ), ) # explore lammps_input = lammps . GenericLammpsInput ( config = workflow_config . lammps , type_map = type_map , mass_map = mass_map , fep_options = lammps . GenericLammpsInput . FepOptions ( neu_models = neu_train_output . get_mlp_models (), red_models = red_train_output . get_mlp_models (), ), ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f ' { cp_prefix } /lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) # select red_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . red . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_RED_OUT , ) red_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-selector-threshold' ), resource_manager = resource_manager , ) neu_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . neu . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_NEU_OUT , ) neu_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-selector-threshold' ), resource_manager = resource_manager , ) red_selector_output , neu_selector_output = await asyncio . gather ( apply_checkpoint ( f ' { cp_prefix } /selector/red' )( selector . threshold_selector )( red_selector_input , red_selector_context ), apply_checkpoint ( f ' { cp_prefix } /selector/neu' )( selector . threshold_selector )( neu_selector_input , neu_selector_context ), ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ): continue # update config update_cursor += 1 if __name__ == '__main__' : # use python-fire to parse command line arguments Fire ( run_workflow ) Variables logger Functions cll_mlp_training_workflow def cll_mlp_training_workflow ( config : ai2_kit . workflow . fep_mlp . FepWorkflowConfig , resource_manager : ai2_kit . core . resource_manager . ResourceManager , executor : str , path_prefix : str ) View Source async def cll_mlp_training_workflow ( config : FepWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ) : context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step neu_label_output : Optional [ cll.ICllLabelOutput ] = None red_label_output : Optional [ cll.ICllLabelOutput ] = None neu_selector_output : Optional [ cll.ICllSelectorOutput ] = None red_selector_output : Optional [ cll.ICllSelectorOutput ] = None neu_train_output : Optional [ cll.ICllTrainOutput ] = None red_train_output : Optional [ cll.ICllTrainOutput ] = None explore_output : Optional [ cll.ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ) : # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration {i} exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters-{i:03d}' ) # prefix of checkpoint cp_prefix = f 'iters-{i:03d}' # label : cp2k red_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . red . cp2k , type_map = type_map , system_files = [] if red_selector_output is None else red_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) red_cpk2_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'red-label-cp2k' ), resource_manager = resource_manager , ) neu_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . neu . cp2k , type_map = type_map , system_files = [] if neu_selector_output is None else neu_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) neu_cp2k_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'neu-label-cp2k' ), resource_manager = resource_manager , ) red_label_output , neu_label_output = await asyncio . gather ( apply_checkpoint ( f '{cp_prefix}/cp2k/red' )( cp2k . generic_cp2k )( red_cp2k_input , red_cpk2_context ), apply_checkpoint ( f '{cp_prefix}/cp2k/neu' )( cp2k . generic_cp2k )( neu_cp2k_input , neu_cp2k_context ), ) # Train red_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . red . deepmd , type_map = type_map , old_dataset = [] if red_train_output is None else red_train_output . get_training_dataset (), new_dataset = red_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) red_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) neu_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . neu . deepmd , type_map = type_map , old_dataset = [] if neu_train_output is None else neu_train_output . get_training_dataset (), new_dataset = neu_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) neu_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) red_train_output , neu_train_output = await asyncio . gather ( apply_checkpoint ( f '{cp_prefix}/deepmd/red' )( deepmd . generic_deepmd )( red_deepmd_input , red_deepmd_context ), apply_checkpoint ( f '{cp_prefix}/deepmd/neu' )( deepmd . generic_deepmd )( neu_deepmd_input , neu_deepmd_context ), ) # explore lammps_input = lammps . GenericLammpsInput ( config = workflow_config . lammps , type_map = type_map , mass_map = mass_map , fep_options = lammps . GenericLammpsInput . FepOptions ( neu_models = neu_train_output . get_mlp_models (), red_models = red_train_output . get_mlp_models (), ), ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f '{cp_prefix}/lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) # select red_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . red . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_RED_OUT , ) red_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-selector-threshold' ), resource_manager = resource_manager , ) neu_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . neu . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_NEU_OUT , ) neu_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-selector-threshold' ), resource_manager = resource_manager , ) red_selector_output , neu_selector_output = await asyncio . gather ( apply_checkpoint ( f '{cp_prefix}/selector/red' )( selector . threshold_selector )( red_selector_input , red_selector_context ), apply_checkpoint ( f '{cp_prefix}/selector/neu' )( selector . threshold_selector )( neu_selector_input , neu_selector_context ), ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ) : continue # update config update_cursor += 1 run_workflow def run_workflow ( * config_files , executor : Union [ str , NoneType ] = None , path_prefix : Union [ str , NoneType ] = None , checkpoint : Union [ str , NoneType ] = None ) Training ML potential for FEP View Source def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint : Optional [ str ] = None ) : \"\"\" Training ML potential for FEP \"\"\" if checkpoint is not None : set_checkpoint_file ( checkpoint ) config_data = load_yaml_files ( * config_files ) config = FepWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor {executor} is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix )) Classes FepExecutorConfig class FepExecutorConfig ( __pydantic_self__ , ** data : Any ) View Source class FepExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): deepmd: deepmd . GenericDeepmdContextConfig lammps: lammps . GenericLammpsContextConfig cp2k: cp2k . GenericCp2kContextConfig context: Context Ancestors (in MRO) ai2_kit.core.executor.BaseExecutorConfig pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Context Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . FepWorkflowConfig class FepWorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class FepWorkflowConfig ( BaseModel ): executors: Dict [ str , FepExecutorConfig ] artifacts: ArtifactMap workflow: Any Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . WorkflowConfig class WorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class WorkflowConfig ( BaseModel ) : class General ( BaseModel ) : type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Branch ( BaseModel ) : deepmd : deepmd . GenericDeepmdInputConfig cp2k : cp2k . GenericCp2kInputConfig threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ) : walkthrough : updater . WalkthroughUpdaterInputConfig general : General neu : Branch red : Branch lammps : lammps . GenericLammpsInputConfig update : Update Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Branch Config General Update Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Fep Mlp"},{"location":"reference/ai2_kit/workflow/fep_mlp/#module-ai2_kitworkflowfep_mlp","text":"View Source from ai2_kit.core.executor import BaseExecutorConfig from ai2_kit.core.artifact import ArtifactMap from ai2_kit.core.log import get_logger from ai2_kit.core.util import load_yaml_files from ai2_kit.core.resource_manager import ResourceManager from ai2_kit.domain import ( deepmd , lammps , selector , cp2k , constant as const , updater , cll , ) from ai2_kit.core.checkpoint import set_checkpoint_file , apply_checkpoint from pydantic import BaseModel from typing import Dict , List , Optional , Any from fire import Fire import asyncio import copy import itertools import os logger = get_logger ( __name__ ) class FepExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): deepmd : deepmd . GenericDeepmdContextConfig lammps : lammps . GenericLammpsContextConfig cp2k : cp2k . GenericCp2kContextConfig context : Context class WorkflowConfig ( BaseModel ): class General ( BaseModel ): type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Branch ( BaseModel ): deepmd : deepmd . GenericDeepmdInputConfig cp2k : cp2k . GenericCp2kInputConfig threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ): walkthrough : updater . WalkthroughUpdaterInputConfig general : General neu : Branch red : Branch lammps : lammps . GenericLammpsInputConfig update : Update class FepWorkflowConfig ( BaseModel ): executors : Dict [ str , FepExecutorConfig ] artifacts : ArtifactMap workflow : Any def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint : Optional [ str ] = None ): \"\"\" Training ML potential for FEP \"\"\" if checkpoint is not None : set_checkpoint_file ( checkpoint ) config_data = load_yaml_files ( * config_files ) config = FepWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor { executor } is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix )) async def cll_mlp_training_workflow ( config : FepWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ): context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step neu_label_output : Optional [ cll . ICllLabelOutput ] = None red_label_output : Optional [ cll . ICllLabelOutput ] = None neu_selector_output : Optional [ cll . ICllSelectorOutput ] = None red_selector_output : Optional [ cll . ICllSelectorOutput ] = None neu_train_output : Optional [ cll . ICllTrainOutput ] = None red_train_output : Optional [ cll . ICllTrainOutput ] = None explore_output : Optional [ cll . ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ): # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration { i } exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters- { i : 03d } ' ) # prefix of checkpoint cp_prefix = f 'iters- { i : 03d } ' # label: cp2k red_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . red . cp2k , type_map = type_map , system_files = [] if red_selector_output is None else red_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) red_cpk2_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'red-label-cp2k' ), resource_manager = resource_manager , ) neu_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . neu . cp2k , type_map = type_map , system_files = [] if neu_selector_output is None else neu_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) neu_cp2k_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'neu-label-cp2k' ), resource_manager = resource_manager , ) red_label_output , neu_label_output = await asyncio . gather ( apply_checkpoint ( f ' { cp_prefix } /cp2k/red' )( cp2k . generic_cp2k )( red_cp2k_input , red_cpk2_context ), apply_checkpoint ( f ' { cp_prefix } /cp2k/neu' )( cp2k . generic_cp2k )( neu_cp2k_input , neu_cp2k_context ), ) # Train red_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . red . deepmd , type_map = type_map , old_dataset = [] if red_train_output is None else red_train_output . get_training_dataset (), new_dataset = red_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) red_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) neu_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . neu . deepmd , type_map = type_map , old_dataset = [] if neu_train_output is None else neu_train_output . get_training_dataset (), new_dataset = neu_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) neu_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) red_train_output , neu_train_output = await asyncio . gather ( apply_checkpoint ( f ' { cp_prefix } /deepmd/red' )( deepmd . generic_deepmd )( red_deepmd_input , red_deepmd_context ), apply_checkpoint ( f ' { cp_prefix } /deepmd/neu' )( deepmd . generic_deepmd )( neu_deepmd_input , neu_deepmd_context ), ) # explore lammps_input = lammps . GenericLammpsInput ( config = workflow_config . lammps , type_map = type_map , mass_map = mass_map , fep_options = lammps . GenericLammpsInput . FepOptions ( neu_models = neu_train_output . get_mlp_models (), red_models = red_train_output . get_mlp_models (), ), ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f ' { cp_prefix } /lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) # select red_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . red . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_RED_OUT , ) red_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-selector-threshold' ), resource_manager = resource_manager , ) neu_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . neu . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_NEU_OUT , ) neu_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-selector-threshold' ), resource_manager = resource_manager , ) red_selector_output , neu_selector_output = await asyncio . gather ( apply_checkpoint ( f ' { cp_prefix } /selector/red' )( selector . threshold_selector )( red_selector_input , red_selector_context ), apply_checkpoint ( f ' { cp_prefix } /selector/neu' )( selector . threshold_selector )( neu_selector_input , neu_selector_context ), ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ): continue # update config update_cursor += 1 if __name__ == '__main__' : # use python-fire to parse command line arguments Fire ( run_workflow )","title":"Module ai2_kit.workflow.fep_mlp"},{"location":"reference/ai2_kit/workflow/fep_mlp/#variables","text":"logger","title":"Variables"},{"location":"reference/ai2_kit/workflow/fep_mlp/#functions","text":"","title":"Functions"},{"location":"reference/ai2_kit/workflow/fep_mlp/#cll_mlp_training_workflow","text":"def cll_mlp_training_workflow ( config : ai2_kit . workflow . fep_mlp . FepWorkflowConfig , resource_manager : ai2_kit . core . resource_manager . ResourceManager , executor : str , path_prefix : str ) View Source async def cll_mlp_training_workflow ( config : FepWorkflowConfig , resource_manager : ResourceManager , executor : str , path_prefix : str ) : context_config = config . executors [ executor ] . context raw_workflow_config = copy . deepcopy ( config . workflow ) # output of each step neu_label_output : Optional [ cll.ICllLabelOutput ] = None red_label_output : Optional [ cll.ICllLabelOutput ] = None neu_selector_output : Optional [ cll.ICllSelectorOutput ] = None red_selector_output : Optional [ cll.ICllSelectorOutput ] = None neu_train_output : Optional [ cll.ICllTrainOutput ] = None red_train_output : Optional [ cll.ICllTrainOutput ] = None explore_output : Optional [ cll.ICllExploreOutput ] = None # cursor of update table update_cursor = 0 # Start iteration for i in itertools . count ( 0 ) : # parse workflow config workflow_config = WorkflowConfig . parse_obj ( raw_workflow_config ) if i >= workflow_config . general . max_iters : logger . info ( f 'Iteration {i} exceeds max_iters, stop iteration.' ) break # shortcut for type_map and mass_map type_map = workflow_config . general . type_map mass_map = workflow_config . general . mass_map # decide path prefix for each iteration iter_path_prefix = os . path . join ( path_prefix , f 'iters-{i:03d}' ) # prefix of checkpoint cp_prefix = f 'iters-{i:03d}' # label : cp2k red_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . red . cp2k , type_map = type_map , system_files = [] if red_selector_output is None else red_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) red_cpk2_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'red-label-cp2k' ), resource_manager = resource_manager , ) neu_cp2k_input = cp2k . GenericCp2kInput ( config = workflow_config . neu . cp2k , type_map = type_map , system_files = [] if neu_selector_output is None else neu_selector_output . get_model_devi_dataset (), initiated = i > 0 , ) neu_cp2k_context = cp2k . GenericCp2kContext ( config = context_config . cp2k , path_prefix = os . path . join ( iter_path_prefix , 'neu-label-cp2k' ), resource_manager = resource_manager , ) red_label_output , neu_label_output = await asyncio . gather ( apply_checkpoint ( f '{cp_prefix}/cp2k/red' )( cp2k . generic_cp2k )( red_cp2k_input , red_cpk2_context ), apply_checkpoint ( f '{cp_prefix}/cp2k/neu' )( cp2k . generic_cp2k )( neu_cp2k_input , neu_cp2k_context ), ) # Train red_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . red . deepmd , type_map = type_map , old_dataset = [] if red_train_output is None else red_train_output . get_training_dataset (), new_dataset = red_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) red_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) neu_deepmd_input = deepmd . GenericDeepmdInput ( config = workflow_config . neu . deepmd , type_map = type_map , old_dataset = [] if neu_train_output is None else neu_train_output . get_training_dataset (), new_dataset = neu_label_output . get_labeled_system_dataset (), initiated = i > 0 , ) neu_deepmd_context = deepmd . GenericDeepmdContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-train-deepmd' ), config = context_config . deepmd , resource_manager = resource_manager , ) red_train_output , neu_train_output = await asyncio . gather ( apply_checkpoint ( f '{cp_prefix}/deepmd/red' )( deepmd . generic_deepmd )( red_deepmd_input , red_deepmd_context ), apply_checkpoint ( f '{cp_prefix}/deepmd/neu' )( deepmd . generic_deepmd )( neu_deepmd_input , neu_deepmd_context ), ) # explore lammps_input = lammps . GenericLammpsInput ( config = workflow_config . lammps , type_map = type_map , mass_map = mass_map , fep_options = lammps . GenericLammpsInput . FepOptions ( neu_models = neu_train_output . get_mlp_models (), red_models = red_train_output . get_mlp_models (), ), ) lammps_context = lammps . GenericLammpsContext ( path_prefix = os . path . join ( iter_path_prefix , 'explore-lammps' ), config = context_config . lammps , resource_manager = resource_manager , ) explore_output = await apply_checkpoint ( f '{cp_prefix}/lammps' )( lammps . generic_lammps )( lammps_input , lammps_context ) # select red_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . red . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_RED_OUT , ) red_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'red-selector-threshold' ), resource_manager = resource_manager , ) neu_selector_input = selector . ThresholdSelectorInput ( config = workflow_config . neu . threshold , model_devi_data = explore_output . get_model_devi_dataset (), model_devi_out_filename = const . MODEL_DEVI_NEU_OUT , ) neu_selector_context = selector . ThresholdSelectorContext ( path_prefix = os . path . join ( iter_path_prefix , 'neu-selector-threshold' ), resource_manager = resource_manager , ) red_selector_output , neu_selector_output = await asyncio . gather ( apply_checkpoint ( f '{cp_prefix}/selector/red' )( selector . threshold_selector )( red_selector_input , red_selector_context ), apply_checkpoint ( f '{cp_prefix}/selector/neu' )( selector . threshold_selector )( neu_selector_input , neu_selector_context ), ) # Update update_config = workflow_config . update . walkthrough # nothing to update because the table is empty if not update_config . table : continue # keep using the latest config when it reach the end of table if update_cursor >= len ( update_config . table ) : continue # update config update_cursor += 1","title":"cll_mlp_training_workflow"},{"location":"reference/ai2_kit/workflow/fep_mlp/#run_workflow","text":"def run_workflow ( * config_files , executor : Union [ str , NoneType ] = None , path_prefix : Union [ str , NoneType ] = None , checkpoint : Union [ str , NoneType ] = None ) Training ML potential for FEP View Source def run_workflow ( * config_files , executor : Optional [ str ] = None , path_prefix : Optional [ str ] = None , checkpoint : Optional [ str ] = None ) : \"\"\" Training ML potential for FEP \"\"\" if checkpoint is not None : set_checkpoint_file ( checkpoint ) config_data = load_yaml_files ( * config_files ) config = FepWorkflowConfig . parse_obj ( config_data ) if executor not in config . executors : raise ValueError ( f 'executor {executor} is not found' ) if path_prefix is None : raise ValueError ( 'path_prefix should not be empty' ) cll . init_artifacts ( config . artifacts ) resource_manager = ResourceManager ( executor_configs = config . executors , artifacts = config . artifacts , default_executor = executor , ) return asyncio . run ( cll_mlp_training_workflow ( config , resource_manager , executor , path_prefix ))","title":"run_workflow"},{"location":"reference/ai2_kit/workflow/fep_mlp/#classes","text":"","title":"Classes"},{"location":"reference/ai2_kit/workflow/fep_mlp/#fepexecutorconfig","text":"class FepExecutorConfig ( __pydantic_self__ , ** data : Any ) View Source class FepExecutorConfig ( BaseExecutorConfig ): class Context ( BaseModel ): deepmd: deepmd . GenericDeepmdContextConfig lammps: lammps . GenericLammpsContextConfig cp2k: cp2k . GenericCp2kContextConfig context: Context","title":"FepExecutorConfig"},{"location":"reference/ai2_kit/workflow/fep_mlp/#ancestors-in-mro","text":"ai2_kit.core.executor.BaseExecutorConfig pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/workflow/fep_mlp/#class-variables","text":"Config Context","title":"Class variables"},{"location":"reference/ai2_kit/workflow/fep_mlp/#static-methods","text":"","title":"Static methods"},{"location":"reference/ai2_kit/workflow/fep_mlp/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/workflow/fep_mlp/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/workflow/fep_mlp/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/workflow/fep_mlp/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/workflow/fep_mlp/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/workflow/fep_mlp/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/workflow/fep_mlp/#methods","text":"","title":"Methods"},{"location":"reference/ai2_kit/workflow/fep_mlp/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/workflow/fep_mlp/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/workflow/fep_mlp/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/workflow/fep_mlp/#fepworkflowconfig","text":"class FepWorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class FepWorkflowConfig ( BaseModel ): executors: Dict [ str , FepExecutorConfig ] artifacts: ArtifactMap workflow: Any","title":"FepWorkflowConfig"},{"location":"reference/ai2_kit/workflow/fep_mlp/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/workflow/fep_mlp/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/ai2_kit/workflow/fep_mlp/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ai2_kit/workflow/fep_mlp/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/workflow/fep_mlp/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/workflow/fep_mlp/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/workflow/fep_mlp/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/workflow/fep_mlp/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/workflow/fep_mlp/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/workflow/fep_mlp/#methods_1","text":"","title":"Methods"},{"location":"reference/ai2_kit/workflow/fep_mlp/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/workflow/fep_mlp/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/workflow/fep_mlp/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/ai2_kit/workflow/fep_mlp/#workflowconfig","text":"class WorkflowConfig ( __pydantic_self__ , ** data : Any ) View Source class WorkflowConfig ( BaseModel ) : class General ( BaseModel ) : type_map : List [ str ] mass_map : List [ float ] max_iters : int = 10 class Branch ( BaseModel ) : deepmd : deepmd . GenericDeepmdInputConfig cp2k : cp2k . GenericCp2kInputConfig threshold : selector . ThresholdSelectorInputConfig class Update ( BaseModel ) : walkthrough : updater . WalkthroughUpdaterInputConfig general : General neu : Branch red : Branch lammps : lammps . GenericLammpsInputConfig update : Update","title":"WorkflowConfig"},{"location":"reference/ai2_kit/workflow/fep_mlp/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/ai2_kit/workflow/fep_mlp/#class-variables_2","text":"Branch Config General Update","title":"Class variables"},{"location":"reference/ai2_kit/workflow/fep_mlp/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ai2_kit/workflow/fep_mlp/#construct_2","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/ai2_kit/workflow/fep_mlp/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/ai2_kit/workflow/fep_mlp/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/ai2_kit/workflow/fep_mlp/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/ai2_kit/workflow/fep_mlp/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/ai2_kit/workflow/fep_mlp/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/ai2_kit/workflow/fep_mlp/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/ai2_kit/workflow/fep_mlp/#methods_2","text":"","title":"Methods"},{"location":"reference/ai2_kit/workflow/fep_mlp/#copy_2","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , update : Union [ ForwardRef ( 'DictStrAny' ), NoneType ] = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/ai2_kit/workflow/fep_mlp/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/ai2_kit/workflow/fep_mlp/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' ), NoneType ] = None , by_alias : bool = False , skip_defaults : Union [ bool , NoneType ] = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , models_as_dict : bool = True , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"}]}